{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PytorchRNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUunloA54F4g"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obhTp5fb4LnL"
      },
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/xtzx/proj/data/train.csv', sep='\\t')\r\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/xtzx/proj/data/test.csv', sep = '\\t')\r\n",
        "dev_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/xtzx/proj/data/dev.csv', sep = '\\t')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "0tjRazN43zEY",
        "outputId": "9463ff2b-d4b3-4fee-c162-d2ba175aea45"
      },
      "source": [
        "train_feature_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/xtzx/proj/data/train_feature_df.csv', sep='\\t')\r\n",
        "test_feature_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/xtzx/proj/data/test_feature_df.csv', sep = '\\t')\r\n",
        "dev_feature_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/xtzx/proj/data/dev_feature_df.csv', sep = '\\t')\r\n",
        "train_feature_df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>user_id</th>\n",
              "      <th>label</th>\n",
              "      <th>last_day</th>\n",
              "      <th>rounds</th>\n",
              "      <th>num_days</th>\n",
              "      <th>max_level</th>\n",
              "      <th>1.0last_level</th>\n",
              "      <th>1.0last_trytimes</th>\n",
              "      <th>1.0max_retrytimes</th>\n",
              "      <th>1.0num_levels</th>\n",
              "      <th>1.0passrate_ratio</th>\n",
              "      <th>1.0retrytimes_ratio</th>\n",
              "      <th>1.0num_rounds</th>\n",
              "      <th>1.0total_duration</th>\n",
              "      <th>1.0num_help</th>\n",
              "      <th>1.0win_duration_ratio</th>\n",
              "      <th>2.0last_level</th>\n",
              "      <th>2.0last_trytimes</th>\n",
              "      <th>2.0max_retrytimes</th>\n",
              "      <th>2.0num_levels</th>\n",
              "      <th>2.0passrate_ratio</th>\n",
              "      <th>2.0retrytimes_ratio</th>\n",
              "      <th>2.0num_rounds</th>\n",
              "      <th>2.0total_duration</th>\n",
              "      <th>2.0num_help</th>\n",
              "      <th>2.0win_duration_ratio</th>\n",
              "      <th>2.0last_time</th>\n",
              "      <th>3.0last_level</th>\n",
              "      <th>3.0last_trytimes</th>\n",
              "      <th>3.0max_retrytimes</th>\n",
              "      <th>3.0num_levels</th>\n",
              "      <th>3.0passrate_ratio</th>\n",
              "      <th>3.0retrytimes_ratio</th>\n",
              "      <th>3.0num_rounds</th>\n",
              "      <th>3.0total_duration</th>\n",
              "      <th>3.0num_help</th>\n",
              "      <th>3.0win_duration_ratio</th>\n",
              "      <th>3.0last_time</th>\n",
              "      <th>4.0last_level</th>\n",
              "      <th>4.0last_trytimes</th>\n",
              "      <th>4.0max_retrytimes</th>\n",
              "      <th>4.0num_levels</th>\n",
              "      <th>4.0passrate_ratio</th>\n",
              "      <th>4.0retrytimes_ratio</th>\n",
              "      <th>4.0num_rounds</th>\n",
              "      <th>4.0total_duration</th>\n",
              "      <th>4.0num_help</th>\n",
              "      <th>4.0win_duration_ratio</th>\n",
              "      <th>4.0last_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2774</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>215.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>1.004697</td>\n",
              "      <td>0.279470</td>\n",
              "      <td>73.0</td>\n",
              "      <td>8743.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.128784</td>\n",
              "      <td>116.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1.125680</td>\n",
              "      <td>0.810917</td>\n",
              "      <td>46.0</td>\n",
              "      <td>5330.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.942069</td>\n",
              "      <td>1.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>1.126512</td>\n",
              "      <td>0.657605</td>\n",
              "      <td>65.0</td>\n",
              "      <td>7096.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.941808</td>\n",
              "      <td>1.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.894427</td>\n",
              "      <td>2.095213</td>\n",
              "      <td>31.0</td>\n",
              "      <td>4229.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.930846</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2775</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>1.023738</td>\n",
              "      <td>0.204502</td>\n",
              "      <td>55.0</td>\n",
              "      <td>6773.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.232778</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>1.048408</td>\n",
              "      <td>0.300045</td>\n",
              "      <td>21.0</td>\n",
              "      <td>4276.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.642346</td>\n",
              "      <td>0.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1.024516</td>\n",
              "      <td>0.769187</td>\n",
              "      <td>35.0</td>\n",
              "      <td>7790.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.250542</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2776</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.097110</td>\n",
              "      <td>0.248792</td>\n",
              "      <td>49.0</td>\n",
              "      <td>4300.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.837336</td>\n",
              "      <td>68.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.768579</td>\n",
              "      <td>2.568146</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.918683</td>\n",
              "      <td>1.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.403156</td>\n",
              "      <td>1.762801</td>\n",
              "      <td>7.0</td>\n",
              "      <td>707.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.916467</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2777</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>286.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>0.991583</td>\n",
              "      <td>0.518244</td>\n",
              "      <td>93.0</td>\n",
              "      <td>11203.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.064818</td>\n",
              "      <td>141.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>0.896006</td>\n",
              "      <td>0.958794</td>\n",
              "      <td>96.0</td>\n",
              "      <td>14112.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.080538</td>\n",
              "      <td>1.0</td>\n",
              "      <td>153.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1.063837</td>\n",
              "      <td>1.864522</td>\n",
              "      <td>48.0</td>\n",
              "      <td>8663.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.631832</td>\n",
              "      <td>1.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.738995</td>\n",
              "      <td>3.354036</td>\n",
              "      <td>49.0</td>\n",
              "      <td>6830.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.097669</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2778</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>1.127967</td>\n",
              "      <td>0.507783</td>\n",
              "      <td>136.0</td>\n",
              "      <td>26400.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.445506</td>\n",
              "      <td>122.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.803819</td>\n",
              "      <td>3.137302</td>\n",
              "      <td>13.0</td>\n",
              "      <td>2806.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.389755</td>\n",
              "      <td>1.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.099679</td>\n",
              "      <td>1.367873</td>\n",
              "      <td>13.0</td>\n",
              "      <td>2839.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.868698</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8153</th>\n",
              "      <td>8153</td>\n",
              "      <td>10927</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>350.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>207.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1.071706</td>\n",
              "      <td>0.158491</td>\n",
              "      <td>60.0</td>\n",
              "      <td>4278.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.761726</td>\n",
              "      <td>179.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>1.136235</td>\n",
              "      <td>2.217590</td>\n",
              "      <td>199.0</td>\n",
              "      <td>19627.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.725830</td>\n",
              "      <td>1.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.173134</td>\n",
              "      <td>1.851064</td>\n",
              "      <td>37.0</td>\n",
              "      <td>3485.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.718428</td>\n",
              "      <td>1.0</td>\n",
              "      <td>207.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>1.267583</td>\n",
              "      <td>3.122715</td>\n",
              "      <td>54.0</td>\n",
              "      <td>7307.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.164982</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8154</th>\n",
              "      <td>8154</td>\n",
              "      <td>10928</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1.084387</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>40.0</td>\n",
              "      <td>3096.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.851944</td>\n",
              "      <td>48.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.249918</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.0</td>\n",
              "      <td>977.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.905527</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8155</th>\n",
              "      <td>8155</td>\n",
              "      <td>10929</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>243.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>0.900079</td>\n",
              "      <td>0.684813</td>\n",
              "      <td>114.0</td>\n",
              "      <td>13268.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.969805</td>\n",
              "      <td>0.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.754247</td>\n",
              "      <td>1.952475</td>\n",
              "      <td>90.0</td>\n",
              "      <td>10720.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.996916</td>\n",
              "      <td>1.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.782289</td>\n",
              "      <td>1.754519</td>\n",
              "      <td>39.0</td>\n",
              "      <td>4870.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.919773</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8156</th>\n",
              "      <td>8156</td>\n",
              "      <td>10930</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.062608</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.0</td>\n",
              "      <td>2702.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.303275</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1.056998</td>\n",
              "      <td>0.166811</td>\n",
              "      <td>14.0</td>\n",
              "      <td>3418.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.556270</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8157</th>\n",
              "      <td>8157</td>\n",
              "      <td>10931</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.999548</td>\n",
              "      <td>0.142157</td>\n",
              "      <td>24.0</td>\n",
              "      <td>2283.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.053104</td>\n",
              "      <td>0.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.985063</td>\n",
              "      <td>0.459295</td>\n",
              "      <td>32.0</td>\n",
              "      <td>3910.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.212597</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8158 rows × 50 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  user_id  ...  4.0win_duration_ratio  4.0last_time\n",
              "0              0     2774  ...               0.930846           1.0\n",
              "1              1     2775  ...               2.250542           1.0\n",
              "2              2     2776  ...               0.000000           0.0\n",
              "3              3     2777  ...               1.097669           1.0\n",
              "4              4     2778  ...               0.000000           0.0\n",
              "...          ...      ...  ...                    ...           ...\n",
              "8153        8153    10927  ...               1.164982           1.0\n",
              "8154        8154    10928  ...               0.000000           0.0\n",
              "8155        8155    10929  ...               0.919773           1.0\n",
              "8156        8156    10930  ...               0.000000           0.0\n",
              "8157        8157    10931  ...               0.000000           0.0\n",
              "\n",
              "[8158 rows x 50 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2RurJtC59Bb",
        "outputId": "3236abda-d677-4e99-8251-a2a7f5dbd7a6"
      },
      "source": [
        "print(list(train_feature_df[:0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Unnamed: 0', 'user_id', 'label', 'last_day', 'rounds', 'num_days', 'max_level', '1.0last_level', '1.0last_trytimes', '1.0max_retrytimes', '1.0num_levels', '1.0passrate_ratio', '1.0retrytimes_ratio', '1.0num_rounds', '1.0total_duration', '1.0num_help', '1.0win_duration_ratio', '2.0last_level', '2.0last_trytimes', '2.0max_retrytimes', '2.0num_levels', '2.0passrate_ratio', '2.0retrytimes_ratio', '2.0num_rounds', '2.0total_duration', '2.0num_help', '2.0win_duration_ratio', '2.0last_time', '3.0last_level', '3.0last_trytimes', '3.0max_retrytimes', '3.0num_levels', '3.0passrate_ratio', '3.0retrytimes_ratio', '3.0num_rounds', '3.0total_duration', '3.0num_help', '3.0win_duration_ratio', '3.0last_time', '4.0last_level', '4.0last_trytimes', '4.0max_retrytimes', '4.0num_levels', '4.0passrate_ratio', '4.0retrytimes_ratio', '4.0num_rounds', '4.0total_duration', '4.0num_help', '4.0win_duration_ratio', '4.0last_time']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbGizvQhp-ng"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQt02mcG6Y4a"
      },
      "source": [
        "\r\n",
        "# Day1_feature_names = ['1sep_time', '1num_succ', '1last_level', '1last_trytimes', '1max_retrytimes', '1num_levels', '1passrate_ratio', '1retrytimes_ratio', '1num_rounds', '1total_duration', '1num_help', '1win_duration_ratio',]\r\n",
        "# Day2_feature_names = [ '2sep_time', '2num_succ', '2last_level', '2last_trytimes', '2max_retrytimes', '2num_levels', '2passrate_ratio', '2retrytimes_ratio', '2num_rounds', '2total_duration', '2num_help', '2win_duration_ratio', '2last_time']\r\n",
        "# Day3_feature_names = [ '3sep_time', '3num_succ', '3last_level', '3last_trytimes', '3max_retrytimes', '3num_levels', '3passrate_ratio', '3retrytimes_ratio', '3num_rounds', '3total_duration', '3num_help', '3win_duration_ratio', '3last_time']\r\n",
        "# Day4_feature_names = [ '4sep_time', '4num_succ', '4last_level', '4last_trytimes', '4max_retrytimes', '4num_levels', '4passrate_ratio', '4retrytimes_ratio', '4num_rounds', '4total_duration', '4num_help', '4win_duration_ratio', '4last_time']\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okXUNhMJxAv7"
      },
      "source": [
        "Day1_feature_names = ['1.0last_level', '1.0last_trytimes', '1.0max_retrytimes', '1.0num_levels', '1.0passrate_ratio', '1.0retrytimes_ratio', '1.0num_rounds', '1.0total_duration', '1.0num_help', '1.0win_duration_ratio',]\r\n",
        "Day2_feature_names = ['2.0last_level', '2.0last_trytimes', '2.0max_retrytimes', '2.0num_levels', '2.0passrate_ratio', '2.0retrytimes_ratio', '2.0num_rounds', '2.0total_duration', '2.0num_help', '2.0win_duration_ratio', '2.0last_time',]\r\n",
        "Day3_feature_names = ['3.0last_level', '3.0last_trytimes', '3.0max_retrytimes', '3.0num_levels', '3.0passrate_ratio', '3.0retrytimes_ratio', '3.0num_rounds', '3.0total_duration', '3.0num_help', '3.0win_duration_ratio', '3.0last_time']\r\n",
        "Day4_feature_names = [ '4.0last_level', '4.0last_trytimes', '4.0max_retrytimes', '4.0num_levels', '4.0passrate_ratio', '4.0retrytimes_ratio', '4.0num_rounds', '4.0total_duration', '4.0num_help', '4.0win_duration_ratio', '4.0last_time']\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiktwyZE69Ty",
        "outputId": "667a4587-9820-4d92-e64e-b3833efb08fe"
      },
      "source": [
        "#构建训练集数据\r\n",
        "Day1_X = np.hstack((train_feature_df[Day1_feature_names].values, np.zeros((8158, 1)))) #补上一列\r\n",
        "Day2_X = train_feature_df[Day2_feature_names].values\r\n",
        "Day3_X = train_feature_df[Day3_feature_names].values\r\n",
        "Day4_X = train_feature_df[Day4_feature_names].values\r\n",
        "X_train = np.array([Day1_X, Day2_X, Day3_X, Day4_X])\r\n",
        "y_train = np.array(train_feature_df['label'])\r\n",
        "Day1_X.shape, X_train.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8158, 11), (4, 8158, 11))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIWJkuFVWcQQ",
        "outputId": "bab79ec2-69d1-4886-83e3-0bd9da04bee4"
      },
      "source": [
        "#构建测试集数据\r\n",
        "Day1_X_dev = np.hstack((dev_feature_df[Day1_feature_names].values, np.zeros((2658, 1)))) #补上一列\r\n",
        "Day2_X_dev = dev_feature_df[Day2_feature_names].values\r\n",
        "Day3_X_dev = dev_feature_df[Day3_feature_names].values\r\n",
        "Day4_X_dev = dev_feature_df[Day4_feature_names].values\r\n",
        "X_dev = np.array([Day1_X_dev, Day2_X_dev, Day3_X_dev, Day4_X_dev])\r\n",
        "y_dev = np.array(dev_feature_df['label'])\r\n",
        "Day1_X_dev.shape, X_dev.shape, y_dev.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2658, 11), (4, 2658, 11), (2658,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOaftP8L9Mp4",
        "outputId": "3d87d436-16f9-461c-c0ec-3e772408d8e1"
      },
      "source": [
        "#构建测试集数据\r\n",
        "Day1_X_test = np.hstack((test_feature_df[Day1_feature_names].values, np.zeros((2773, 1)))) #补上一列\r\n",
        "Day2_X_test = test_feature_df[Day2_feature_names].values\r\n",
        "Day3_X_test = test_feature_df[Day3_feature_names].values\r\n",
        "Day4_X_test = test_feature_df[Day4_feature_names].values\r\n",
        "X_test = np.array([Day1_X_test, Day2_X_test, Day3_X_test, Day4_X_test])\r\n",
        "Day1_X_test.shape, X_test.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2773, 11), (4, 2773, 11))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSx7uMI24XFD"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "scaler = MinMaxScaler()\r\n",
        "for i in range(4):\r\n",
        "  X_train[i] = scaler.fit_transform(X_train[i])\r\n",
        "  X_dev[i] = scaler.transform(X_dev[i])\r\n",
        "  X_test[i] = scaler.transform(X_test[i])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ4nQyzUB4M3",
        "outputId": "f9945820-eb57-4018-a08c-3e327c3bdde8"
      },
      "source": [
        "X_train.shape[1]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8158"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmkgiDwoGM3t"
      },
      "source": [
        "import math\r\n",
        "def create_tensor(tensor):\r\n",
        "  if USE_GPU:\r\n",
        "    device = torch.device(\"cuda:0\")\r\n",
        "    tensor = tensor.to(device)\r\n",
        "  return tensor \r\n",
        "\r\n",
        "def make_tensors(features, labels):\r\n",
        "  seq_lengths = torch.LongTensor(features.size(0))\r\n",
        "  seq_lengths = seq_lengths.long()\r\n",
        "  labels = labels.long()\r\n",
        "  return create_tensor(features), create_tensor(seq_lengths), create_tensor(labels)\r\n",
        "\r\n",
        "def time_since(since):\r\n",
        "  s = time.time() - since\r\n",
        "  m = math.floor(s / 60)\r\n",
        "  s -= m * 60\r\n",
        "  return '%dm %ds' % (m, s)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epa315s2DANy"
      },
      "source": [
        "import torch\r\n",
        "from torch.utils.data import DataLoader,Dataset\r\n",
        "\r\n",
        "HIDDEN_SIZE = 64\r\n",
        "BATCH_SIZE = 256\r\n",
        "N_LAYER = 3\r\n",
        "N_EPOCHS = 100\r\n",
        "USE_GPU = True\r\n",
        "N_CLASS = 2\r\n",
        "\r\n",
        "\r\n",
        "class DayFeatureDataset(Dataset): #train and dev sets\r\n",
        "  def __init__(self, is_train_set = True):\r\n",
        "    self.X = X_train if is_train_set else X_dev\r\n",
        "    self.X = torch.tensor(self.X).type(torch.float32)\r\n",
        "    self.y = y_train if is_train_set else y_dev\r\n",
        "    self.y = torch.LongTensor(self.y)\r\n",
        "    # self.one_hot = torch.nn.functional.one_hot(self.y, 2)\r\n",
        "    self.len = self.X.shape[1]\r\n",
        "  \r\n",
        "  def __getitem__(self, index):\r\n",
        "    return self.X[:, index, :], self.y[index]\r\n",
        "  \r\n",
        "  def __len__(self):\r\n",
        "    return self.len\r\n",
        "\r\n",
        "class DayFeatureDataset_test(Dataset): #test\r\n",
        "  def __init__(self):\r\n",
        "    self.X = X_test\r\n",
        "    self.X = torch.tensor(self.X).type(torch.float32)\r\n",
        "    self.len = self.X.shape[1]\r\n",
        "  \r\n",
        "  def __getitem__(self, index):\r\n",
        "    return self.X[:, index, :]\r\n",
        "  \r\n",
        "  def __len__(self):\r\n",
        "    return self.len\r\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEtPhBs9cZMC"
      },
      "source": [
        "trainset = DayFeatureDataset(is_train_set = True)\r\n",
        "trainloader = DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True)\r\n",
        "devset = DayFeatureDataset(is_train_set = False)\r\n",
        "devloader = DataLoader(devset, batch_size = BATCH_SIZE, shuffle = False)\r\n",
        "testset = DayFeatureDataset_test()\r\n",
        "testloader = DataLoader(testset, batch_size = BATCH_SIZE, shuffle = False)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5abZPF98G6z"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\r\n",
        "def proba1(output):\r\n",
        "  output0 = output[:, 0].cpu()\r\n",
        "  output1 = output[:, 1].cpu()\r\n",
        "  proba1 = np.exp(output1) / (np.exp(output0) + np.exp(output1))\r\n",
        "  return proba1"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsw3Neng1srY"
      },
      "source": [
        "class RNNClassifier(torch.nn.Module):\r\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional= True):\r\n",
        "    super(RNNClassifier, self).__init__()\r\n",
        "    self.hidden_size = hidden_size\r\n",
        "    self.n_layers = n_layers\r\n",
        "    self.n_directions = 2 if bidirectional else 1\r\n",
        "    self.gru = torch.nn.GRU(input_size, hidden_size, n_layers,\r\n",
        "                            bidirectional=bidirectional)\r\n",
        "    self.fc = torch.nn.Linear(hidden_size * self.n_directions, output_size)\r\n",
        "\r\n",
        "  def _init_hidden(self, batch_size):\r\n",
        "    hidden = torch.zeros(self.n_layers * self.n_directions,\r\n",
        "                         batch_size, self.hidden_size)\r\n",
        "    return create_tensor(hidden)\r\n",
        "\r\n",
        "  def forward(self, input, seq_lengths):\r\n",
        "    # input shape : B x S -> S x B\r\n",
        "    input = input.transpose(0, 1)\r\n",
        "    batch_size = input.shape[1]\r\n",
        "    # print(batch_size)\r\n",
        "\r\n",
        "    hidden = self._init_hidden(batch_size)\r\n",
        "\r\n",
        "    output, hidden = self.gru(input, hidden)\r\n",
        "\r\n",
        "    if self.n_directions == 2:\r\n",
        "      hidden_cat = torch.cat([hidden[-1], hidden[-2]], dim=1)\r\n",
        "    else:\r\n",
        "      hidden_cat = hidden[-1]\r\n",
        "    fc_output = self.fc(hidden_cat)\r\n",
        "    return fc_output\r\n",
        "\r\n",
        "def trainModel():\r\n",
        "  total_loss = 0\r\n",
        "  for i, (features, labels) in enumerate(trainloader, 1):\r\n",
        "    inputs, seq_lengths, target = make_tensors(features, labels)\r\n",
        "    output = classifier(inputs, seq_lengths)\r\n",
        "    loss = criterion(output, target)\r\n",
        "    optimizer.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    total_loss += loss.item()\r\n",
        "    if i % 10 == 0:\r\n",
        "      print(f'[{time_since(start)}] Epoch {epoch} ', end='')\r\n",
        "      print(f'[{i * len(inputs)}/{len(trainset)}] ', end='')\r\n",
        "      print(f'loss={total_loss / (i * len(inputs))}')\r\n",
        "  return total_loss\r\n",
        "\r\n",
        "def devModel():\r\n",
        "  correct = 0\r\n",
        "  total = len(devset)\r\n",
        "  print(\"evaluating trained model ...\")\r\n",
        "  with torch.no_grad():\r\n",
        "    for i, (features, labels) in enumerate(devloader, 1):\r\n",
        "      inputs, seq_lengths, target = make_tensors(features, labels)\r\n",
        "      output = classifier(inputs, seq_lengths)\r\n",
        "      pred = output.max(dim=1, keepdim=True)[1]\r\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\r\n",
        "\r\n",
        "  percent = '%.2f' % (100 * correct / total)\r\n",
        "  print(f'Dev set: Accuracy {correct}/{total} {percent}%')\r\n",
        "\r\n",
        "  return correct / total\r\n",
        "\r\n",
        "def devModel_auc():\r\n",
        "  correct = 0\r\n",
        "  proba = []\r\n",
        "  total = len(devset)\r\n",
        "  print(\"evaluating trained model ...\")\r\n",
        "  with torch.no_grad():\r\n",
        "    for i, (features, labels) in enumerate(devloader, 1):\r\n",
        "      inputs, seq_lengths, target = make_tensors(features, labels)\r\n",
        "      output = classifier(inputs, seq_lengths)\r\n",
        "      pred = output.max(dim=1, keepdim=True)[1]\r\n",
        "      proba_i = proba1(output)\r\n",
        "      proba.append(proba_i)\r\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\r\n",
        "\r\n",
        "  percent = '%.2f' % (100 * correct / total)\r\n",
        "  print(f'Dev set: Accuracy {correct}/{total} {percent}%')\r\n",
        "\r\n",
        "  probas = proba[0]\r\n",
        "  for i in range(1, len(proba)):\r\n",
        "    probas = torch.cat((probas, proba[i]))\r\n",
        "  probas = np.array(probas).reshape(-1, 1)\r\n",
        "  AUC = roc_auc_score(y_dev, probas)\r\n",
        "  print(\"Dev set: AUC \", AUC )\r\n",
        "\r\n",
        "  return correct / total, AUC"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hwya8I_3lyR",
        "outputId": "8033f06f-da4e-461c-e757-af56294dfd39"
      },
      "source": [
        "import time\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "  classifier = RNNClassifier(11, HIDDEN_SIZE, N_CLASS, N_LAYER, bidirectional = True)\r\n",
        "  if USE_GPU:\r\n",
        "    device = torch.device(\"cuda:0\")\r\n",
        "    classifier.to(device)\r\n",
        "\r\n",
        "  criterion = torch.nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "  optimizer = torch.optim.Adam(classifier.parameters(), lr=0.0005)\r\n",
        "\r\n",
        "  start = time.time()\r\n",
        "  print(\"Training for %d epochs...\" % N_EPOCHS)\r\n",
        "  acc_list = []\r\n",
        "  auc_list = []\r\n",
        "  for epoch in range(1, N_EPOCHS + 1):\r\n",
        "    # Train cycle\r\n",
        "    trainModel()\r\n",
        "    acc, auc = devModel_auc()\r\n",
        "    acc_list.append(acc)\r\n",
        "    auc_list.append(auc)\r\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 100 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.0026024861726909876\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.002514053462073207\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.0024603173059100907\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.7659875797270723\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.002246245159767568\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.002173806045902893\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.0021304379721793034\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1917/2658 72.12%\n",
            "Dev set: AUC  0.7704612025972533\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.0020689645200036464\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.00206280360580422\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.002047382714226842\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1942/2658 73.06%\n",
            "Dev set: AUC  0.7808998665240734\n",
            "[0m 1s] Epoch 4 [2560/8158] loss=0.0020311726024374367\n",
            "[0m 1s] Epoch 4 [5120/8158] loss=0.0020051392493769526\n",
            "[0m 1s] Epoch 4 [7680/8158] loss=0.0020151102178109187\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dev set: AUC  0.7863917723745891\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.0019934521056711673\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.001993321004556492\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.0020099127859187623\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1975/2658 74.30%\n",
            "Dev set: AUC  0.7883354800237767\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.0020080924383364617\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.001991979003651068\n",
            "[0m 2s] Epoch 6 [7680/8158] loss=0.001992379784739266\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.7885085628628661\n",
            "[0m 2s] Epoch 7 [2560/8158] loss=0.0019448299426585436\n",
            "[0m 2s] Epoch 7 [5120/8158] loss=0.001967808761401102\n",
            "[0m 2s] Epoch 7 [7680/8158] loss=0.0019860431862374146\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dev set: AUC  0.7886563781342049\n",
            "[0m 2s] Epoch 8 [2560/8158] loss=0.001950803422369063\n",
            "[0m 2s] Epoch 8 [5120/8158] loss=0.0019997556286398322\n",
            "[0m 2s] Epoch 8 [7680/8158] loss=0.001991735550109297\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1974/2658 74.27%\n",
            "Dev set: AUC  0.7890631859749838\n",
            "[0m 2s] Epoch 9 [2560/8158] loss=0.0019688809523358943\n",
            "[0m 3s] Epoch 9 [5120/8158] loss=0.0019564151880331337\n",
            "[0m 3s] Epoch 9 [7680/8158] loss=0.001979266060516238\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dev set: AUC  0.7891989991516414\n",
            "[0m 3s] Epoch 10 [2560/8158] loss=0.00199659310746938\n",
            "[0m 3s] Epoch 10 [5120/8158] loss=0.001991924230242148\n",
            "[0m 3s] Epoch 10 [7680/8158] loss=0.001972604217007756\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7892896465509454\n",
            "[0m 3s] Epoch 11 [2560/8158] loss=0.0019740826566703616\n",
            "[0m 3s] Epoch 11 [5120/8158] loss=0.0019835816870909183\n",
            "[0m 3s] Epoch 11 [7680/8158] loss=0.0019811075724040467\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dev set: AUC  0.789617177397908\n",
            "[0m 4s] Epoch 12 [2560/8158] loss=0.0019909728434868157\n",
            "[0m 4s] Epoch 12 [5120/8158] loss=0.0020026075828354804\n",
            "[0m 4s] Epoch 12 [7680/8158] loss=0.0019872213170553247\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7897931028383691\n",
            "[0m 4s] Epoch 13 [2560/8158] loss=0.0019727554405108093\n",
            "[0m 4s] Epoch 13 [5120/8158] loss=0.001953854790190235\n",
            "[0m 4s] Epoch 13 [7680/8158] loss=0.0019834707103048763\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7899709233464114\n",
            "[0m 4s] Epoch 14 [2560/8158] loss=0.002025314315687865\n",
            "[0m 4s] Epoch 14 [5120/8158] loss=0.0019946331914979963\n",
            "[0m 4s] Epoch 14 [7680/8158] loss=0.0019738301906424265\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7901414794287256\n",
            "[0m 5s] Epoch 15 [2560/8158] loss=0.00197201743721962\n",
            "[0m 5s] Epoch 15 [5120/8158] loss=0.0019487520388793201\n",
            "[0m 5s] Epoch 15 [7680/8158] loss=0.001976096079063912\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dev set: AUC  0.7902873996324833\n",
            "[0m 5s] Epoch 16 [2560/8158] loss=0.0019512677798047661\n",
            "[0m 5s] Epoch 16 [5120/8158] loss=0.0019701059558428825\n",
            "[0m 5s] Epoch 16 [7680/8158] loss=0.00198193221197774\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dev set: AUC  0.7905609210533796\n",
            "[0m 5s] Epoch 17 [2560/8158] loss=0.002013804914895445\n",
            "[0m 5s] Epoch 17 [5120/8158] loss=0.0019954418414272366\n",
            "[0m 6s] Epoch 17 [7680/8158] loss=0.0019745567619490127\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dev set: AUC  0.7908369692310512\n",
            "[0m 6s] Epoch 18 [2560/8158] loss=0.0019989327643997966\n",
            "[0m 6s] Epoch 18 [5120/8158] loss=0.0019888403941877185\n",
            "[0m 6s] Epoch 18 [7680/8158] loss=0.0019705626531504095\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7909197205154331\n",
            "[0m 6s] Epoch 19 [2560/8158] loss=0.001959852909203619\n",
            "[0m 6s] Epoch 19 [5120/8158] loss=0.001951929839560762\n",
            "[0m 6s] Epoch 19 [7680/8158] loss=0.001977983734104782\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dev set: AUC  0.7910277393675653\n",
            "[0m 6s] Epoch 20 [2560/8158] loss=0.0019563274807296695\n",
            "[0m 6s] Epoch 20 [5120/8158] loss=0.0019493655767291785\n",
            "[0m 7s] Epoch 20 [7680/8158] loss=0.0019575330778025092\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.791347374099606\n",
            "[0m 7s] Epoch 21 [2560/8158] loss=0.001975082326680422\n",
            "[0m 7s] Epoch 21 [5120/8158] loss=0.0019669266184791923\n",
            "[0m 7s] Epoch 21 [7680/8158] loss=0.00197249756505092\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dev set: AUC  0.791494557681751\n",
            "[0m 7s] Epoch 22 [2560/8158] loss=0.0019360762438736857\n",
            "[0m 7s] Epoch 22 [5120/8158] loss=0.001972683653002605\n",
            "[0m 7s] Epoch 22 [7680/8158] loss=0.001969838262690852\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7916186846083243\n",
            "[0m 7s] Epoch 23 [2560/8158] loss=0.0019698051386512817\n",
            "[0m 8s] Epoch 23 [5120/8158] loss=0.0019685743609443307\n",
            "[0m 8s] Epoch 23 [7680/8158] loss=0.001973242483412226\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.791836933224767\n",
            "[0m 8s] Epoch 24 [2560/8158] loss=0.0019198235473595559\n",
            "[0m 8s] Epoch 24 [5120/8158] loss=0.0019710993045009674\n",
            "[0m 8s] Epoch 24 [7680/8158] loss=0.001960937026888132\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dev set: AUC  0.7920731849832316\n",
            "[0m 8s] Epoch 25 [2560/8158] loss=0.0019796186476014554\n",
            "[0m 8s] Epoch 25 [5120/8158] loss=0.0019991657871287318\n",
            "[0m 8s] Epoch 25 [7680/8158] loss=0.001970648610343536\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7922427935317552\n",
            "[0m 8s] Epoch 26 [2560/8158] loss=0.00201931627234444\n",
            "[0m 9s] Epoch 26 [5120/8158] loss=0.00200083072995767\n",
            "[0m 9s] Epoch 26 [7680/8158] loss=0.001969778696851184\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dev set: AUC  0.7924799928240106\n",
            "[0m 9s] Epoch 27 [2560/8158] loss=0.0019708960317075253\n",
            "[0m 9s] Epoch 27 [5120/8158] loss=0.001972083532018587\n",
            "[0m 9s] Epoch 27 [7680/8158] loss=0.0019680449933124084\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7925393716082239\n",
            "[0m 9s] Epoch 28 [2560/8158] loss=0.0020165334455668925\n",
            "[0m 9s] Epoch 28 [5120/8158] loss=0.0019810289551969616\n",
            "[0m 9s] Epoch 28 [7680/8158] loss=0.0019743736328867575\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.7927655163395885\n",
            "[0m 9s] Epoch 29 [2560/8158] loss=0.0019805410760454834\n",
            "[0m 10s] Epoch 29 [5120/8158] loss=0.0019794021267443896\n",
            "[0m 10s] Epoch 29 [7680/8158] loss=0.0019678366060058277\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7928830105296272\n",
            "[0m 10s] Epoch 30 [2560/8158] loss=0.0019118875381536782\n",
            "[0m 10s] Epoch 30 [5120/8158] loss=0.0019583981309551747\n",
            "[0m 10s] Epoch 30 [7680/8158] loss=0.0019644816522486506\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7931009433014731\n",
            "[0m 10s] Epoch 31 [2560/8158] loss=0.0019630007445812225\n",
            "[0m 10s] Epoch 31 [5120/8158] loss=0.001976819790434092\n",
            "[0m 10s] Epoch 31 [7680/8158] loss=0.001970637096868207\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7932986620191187\n",
            "[0m 11s] Epoch 32 [2560/8158] loss=0.0019814811181277036\n",
            "[0m 11s] Epoch 32 [5120/8158] loss=0.0019809746416285636\n",
            "[0m 11s] Epoch 32 [7680/8158] loss=0.0019602286241327724\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7934906955340205\n",
            "[0m 11s] Epoch 33 [2560/8158] loss=0.001955398393329233\n",
            "[0m 11s] Epoch 33 [5120/8158] loss=0.001945461251307279\n",
            "[0m 11s] Epoch 33 [7680/8158] loss=0.001968399443042775\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7937869577658923\n",
            "[0m 11s] Epoch 34 [2560/8158] loss=0.0019146965350955725\n",
            "[0m 12s] Epoch 34 [5120/8158] loss=0.001957953424425796\n",
            "[0m 12s] Epoch 34 [7680/8158] loss=0.001960020116530359\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7938722358070494\n",
            "[0m 12s] Epoch 35 [2560/8158] loss=0.002000340330414474\n",
            "[0m 12s] Epoch 35 [5120/8158] loss=0.0019748695835005493\n",
            "[0m 12s] Epoch 35 [7680/8158] loss=0.0019596938470688957\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7940737446598575\n",
            "[0m 12s] Epoch 36 [2560/8158] loss=0.001977539842482656\n",
            "[0m 12s] Epoch 36 [5120/8158] loss=0.0019464829470962285\n",
            "[0m 12s] Epoch 36 [7680/8158] loss=0.0019589400151744487\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7942057677013524\n",
            "[0m 12s] Epoch 37 [2560/8158] loss=0.0019542661379091442\n",
            "[0m 13s] Epoch 37 [5120/8158] loss=0.0019519077497534454\n",
            "[0m 13s] Epoch 37 [7680/8158] loss=0.0019566343864426015\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7943289470941349\n",
            "[0m 13s] Epoch 38 [2560/8158] loss=0.0019241837901063264\n",
            "[0m 13s] Epoch 38 [5120/8158] loss=0.001993251161184162\n",
            "[0m 13s] Epoch 38 [7680/8158] loss=0.0019792693240257603\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7946801662858635\n",
            "[0m 13s] Epoch 39 [2560/8158] loss=0.0019899404142051935\n",
            "[0m 13s] Epoch 39 [5120/8158] loss=0.00198256517178379\n",
            "[0m 13s] Epoch 39 [7680/8158] loss=0.0019629978846448163\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7945367728388807\n",
            "[0m 14s] Epoch 40 [2560/8158] loss=0.001962860394269228\n",
            "[0m 14s] Epoch 40 [5120/8158] loss=0.0019578852865379305\n",
            "[0m 14s] Epoch 40 [7680/8158] loss=0.0019595992479783794\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7946842722656227\n",
            "[0m 14s] Epoch 41 [2560/8158] loss=0.0019542911322787404\n",
            "[0m 14s] Epoch 41 [5120/8158] loss=0.0019303366483654828\n",
            "[0m 14s] Epoch 41 [7680/8158] loss=0.0019589062469700974\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.79484945898979\n",
            "[0m 14s] Epoch 42 [2560/8158] loss=0.0019506942830048502\n",
            "[0m 14s] Epoch 42 [5120/8158] loss=0.0019430713320616633\n",
            "[0m 14s] Epoch 42 [7680/8158] loss=0.0019560656549098593\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7949713750041849\n",
            "[0m 15s] Epoch 43 [2560/8158] loss=0.0019649008521810175\n",
            "[0m 15s] Epoch 43 [5120/8158] loss=0.0019377261982299388\n",
            "[0m 15s] Epoch 43 [7680/8158] loss=0.0019548791538303097\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7950358073019481\n",
            "[0m 15s] Epoch 44 [2560/8158] loss=0.001972494868095964\n",
            "[0m 15s] Epoch 44 [5120/8158] loss=0.0019813753257039934\n",
            "[0m 15s] Epoch 44 [7680/8158] loss=0.0019519130194870133\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7950471777074357\n",
            "[0m 15s] Epoch 45 [2560/8158] loss=0.0019192075938917696\n",
            "[0m 15s] Epoch 45 [5120/8158] loss=0.0019576012913603336\n",
            "[0m 15s] Epoch 45 [7680/8158] loss=0.001959849257643024\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7952846928442878\n",
            "[0m 16s] Epoch 46 [2560/8158] loss=0.001973984111100435\n",
            "[0m 16s] Epoch 46 [5120/8158] loss=0.0019664050720166414\n",
            "[0m 16s] Epoch 46 [7680/8158] loss=0.0019575210520997644\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7952840611550943\n",
            "[0m 16s] Epoch 47 [2560/8158] loss=0.0019472771440632642\n",
            "[0m 16s] Epoch 47 [5120/8158] loss=0.001953302149195224\n",
            "[0m 16s] Epoch 47 [7680/8158] loss=0.001957128441426903\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7953358596689822\n",
            "[0m 16s] Epoch 48 [2560/8158] loss=0.0019804129959084095\n",
            "[0m 16s] Epoch 48 [5120/8158] loss=0.0019396850548218936\n",
            "[0m 16s] Epoch 48 [7680/8158] loss=0.0019541369401849806\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7954862016970962\n",
            "[0m 17s] Epoch 49 [2560/8158] loss=0.001919595361687243\n",
            "[0m 17s] Epoch 49 [5120/8158] loss=0.0019310943956952543\n",
            "[0m 17s] Epoch 49 [7680/8158] loss=0.0019464223296381533\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7954666193320898\n",
            "[0m 17s] Epoch 50 [2560/8158] loss=0.0019706386141479016\n",
            "[0m 17s] Epoch 50 [5120/8158] loss=0.0019622755120508374\n",
            "[0m 17s] Epoch 50 [7680/8158] loss=0.0019370308184685806\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7957603548071863\n",
            "[0m 18s] Epoch 51 [2560/8158] loss=0.0019284559297375382\n",
            "[0m 18s] Epoch 51 [5120/8158] loss=0.001944709860254079\n",
            "[0m 18s] Epoch 51 [7680/8158] loss=0.0019504767221709093\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7954552489266021\n",
            "[0m 18s] Epoch 52 [2560/8158] loss=0.0019149470957927407\n",
            "[0m 18s] Epoch 52 [5120/8158] loss=0.001936687302077189\n",
            "[0m 18s] Epoch 52 [7680/8158] loss=0.0019496139449377856\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7955474755488905\n",
            "[0m 18s] Epoch 53 [2560/8158] loss=0.0019376054173335434\n",
            "[0m 18s] Epoch 53 [5120/8158] loss=0.0019420965225435793\n",
            "[0m 18s] Epoch 53 [7680/8158] loss=0.001952332021513333\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7959119602136878\n",
            "[0m 19s] Epoch 54 [2560/8158] loss=0.0019248159369453788\n",
            "[0m 19s] Epoch 54 [5120/8158] loss=0.0019383133680094034\n",
            "[0m 19s] Epoch 54 [7680/8158] loss=0.0019526764401234687\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7959378594706318\n",
            "[0m 19s] Epoch 55 [2560/8158] loss=0.0019310426665470003\n",
            "[0m 19s] Epoch 55 [5120/8158] loss=0.0019384685496333987\n",
            "[0m 19s] Epoch 55 [7680/8158] loss=0.001957379359131058\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7959258573759505\n",
            "[0m 19s] Epoch 56 [2560/8158] loss=0.0019779303460381926\n",
            "[0m 19s] Epoch 56 [5120/8158] loss=0.0019537198182661085\n",
            "[0m 19s] Epoch 56 [7680/8158] loss=0.0019560161240709324\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7958721637944812\n",
            "[0m 20s] Epoch 57 [2560/8158] loss=0.0019295053440146147\n",
            "[0m 20s] Epoch 57 [5120/8158] loss=0.0019565114809665827\n",
            "[0m 20s] Epoch 57 [7680/8158] loss=0.0019503148699489732\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dev set: AUC  0.7953983968991641\n",
            "[0m 20s] Epoch 58 [2560/8158] loss=0.0019414303358644247\n",
            "[0m 20s] Epoch 58 [5120/8158] loss=0.0019519331690389663\n",
            "[0m 20s] Epoch 58 [7680/8158] loss=0.001955100564130892\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7961039937285896\n",
            "[0m 20s] Epoch 59 [2560/8158] loss=0.001970785262528807\n",
            "[0m 20s] Epoch 59 [5120/8158] loss=0.0019700738252140582\n",
            "[0m 21s] Epoch 59 [7680/8158] loss=0.0019526784773916007\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7961141007556898\n",
            "[0m 21s] Epoch 60 [2560/8158] loss=0.0019604906439781187\n",
            "[0m 21s] Epoch 60 [5120/8158] loss=0.0019512481638230383\n",
            "[0m 21s] Epoch 60 [7680/8158] loss=0.0019442863878794015\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7957944660236491\n",
            "[0m 21s] Epoch 61 [2560/8158] loss=0.0019133890513330699\n",
            "[0m 21s] Epoch 61 [5120/8158] loss=0.0019319432438351213\n",
            "[0m 21s] Epoch 61 [7680/8158] loss=0.0019458261551335454\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7960837796743894\n",
            "[0m 21s] Epoch 62 [2560/8158] loss=0.0019154222565703094\n",
            "[0m 21s] Epoch 62 [5120/8158] loss=0.0019535875006113203\n",
            "[0m 22s] Epoch 62 [7680/8158] loss=0.0019360441210058829\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7956766559890137\n",
            "[0m 22s] Epoch 63 [2560/8158] loss=0.0020235447795130313\n",
            "[0m 22s] Epoch 63 [5120/8158] loss=0.001973479427397251\n",
            "[0m 22s] Epoch 63 [7680/8158] loss=0.0019495930483875176\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7961103106205273\n",
            "[0m 22s] Epoch 64 [2560/8158] loss=0.0019255482242442668\n",
            "[0m 22s] Epoch 64 [5120/8158] loss=0.001961919781751931\n",
            "[0m 22s] Epoch 64 [7680/8158] loss=0.0019596101017668845\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7959416496057944\n",
            "[0m 22s] Epoch 65 [2560/8158] loss=0.001945498783607036\n",
            "[0m 22s] Epoch 65 [5120/8158] loss=0.0019151294720359147\n",
            "[0m 23s] Epoch 65 [7680/8158] loss=0.0019500702619552612\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7960427198767953\n",
            "[0m 23s] Epoch 66 [2560/8158] loss=0.0020158366416580973\n",
            "[0m 23s] Epoch 66 [5120/8158] loss=0.001973773428471759\n",
            "[0m 23s] Epoch 66 [7680/8158] loss=0.0019547374026539425\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7960313494713077\n",
            "[0m 23s] Epoch 67 [2560/8158] loss=0.0020381682785227894\n",
            "[0m 23s] Epoch 67 [5120/8158] loss=0.001973520830506459\n",
            "[0m 23s] Epoch 67 [7680/8158] loss=0.001951140013989061\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7959056433217503\n",
            "[0m 23s] Epoch 68 [2560/8158] loss=0.0019530042307451367\n",
            "[0m 24s] Epoch 68 [5120/8158] loss=0.0019451135536655783\n",
            "[0m 24s] Epoch 68 [7680/8158] loss=0.001953554382392516\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7955790600085784\n",
            "[0m 24s] Epoch 69 [2560/8158] loss=0.0019532854203134775\n",
            "[0m 24s] Epoch 69 [5120/8158] loss=0.0019392216636333615\n",
            "[0m 24s] Epoch 69 [7680/8158] loss=0.001953081355895847\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7955891670356785\n",
            "[0m 24s] Epoch 70 [2560/8158] loss=0.001961695740465075\n",
            "[0m 24s] Epoch 70 [5120/8158] loss=0.001957725785905495\n",
            "[0m 24s] Epoch 70 [7680/8158] loss=0.0019421185250394046\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7957654083207365\n",
            "[0m 24s] Epoch 71 [2560/8158] loss=0.001963631366379559\n",
            "[0m 25s] Epoch 71 [5120/8158] loss=0.0019504762662108988\n",
            "[0m 25s] Epoch 71 [7680/8158] loss=0.0019439540260160963\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7960313494713077\n",
            "[0m 25s] Epoch 72 [2560/8158] loss=0.0019578943494707344\n",
            "[0m 25s] Epoch 72 [5120/8158] loss=0.0019497496774420141\n",
            "[0m 25s] Epoch 72 [7680/8158] loss=0.001948267415476342\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7955803233869657\n",
            "[0m 25s] Epoch 73 [2560/8158] loss=0.0019568057847209274\n",
            "[0m 25s] Epoch 73 [5120/8158] loss=0.001932944729924202\n",
            "[0m 25s] Epoch 73 [7680/8158] loss=0.0019452706483813623\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.7954824115619337\n",
            "[0m 25s] Epoch 74 [2560/8158] loss=0.0019334097974933683\n",
            "[0m 26s] Epoch 74 [5120/8158] loss=0.0019382730650249868\n",
            "[0m 26s] Epoch 74 [7680/8158] loss=0.0019407122201907138\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7958494229835058\n",
            "[0m 26s] Epoch 75 [2560/8158] loss=0.001932102278806269\n",
            "[0m 26s] Epoch 75 [5120/8158] loss=0.0019265956536401064\n",
            "[0m 26s] Epoch 75 [7680/8158] loss=0.0019406971366455158\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7956441239955352\n",
            "[0m 26s] Epoch 76 [2560/8158] loss=0.001944904774427414\n",
            "[0m 26s] Epoch 76 [5120/8158] loss=0.0019405361963436007\n",
            "[0m 27s] Epoch 76 [7680/8158] loss=0.0019431769498623908\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7957142414960421\n",
            "[0m 27s] Epoch 77 [2560/8158] loss=0.001963474415242672\n",
            "[0m 27s] Epoch 77 [5120/8158] loss=0.00194035402382724\n",
            "[0m 27s] Epoch 77 [7680/8158] loss=0.0019387674750760199\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.79603008609292\n",
            "[0m 27s] Epoch 78 [2560/8158] loss=0.001900648872833699\n",
            "[0m 27s] Epoch 78 [5120/8158] loss=0.0019348149187862873\n",
            "[0m 27s] Epoch 78 [7680/8158] loss=0.001945262150062869\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dev set: AUC  0.7959296475111131\n",
            "[0m 28s] Epoch 79 [2560/8158] loss=0.001964200520887971\n",
            "[0m 28s] Epoch 79 [5120/8158] loss=0.0019352622446604073\n",
            "[0m 28s] Epoch 79 [7680/8158] loss=0.0019650458513448634\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7954950453458088\n",
            "[0m 28s] Epoch 80 [2560/8158] loss=0.0019829496392048894\n",
            "[0m 28s] Epoch 80 [5120/8158] loss=0.0019327368238009513\n",
            "[0m 28s] Epoch 80 [7680/8158] loss=0.00193656135428076\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7952701639928316\n",
            "[0m 28s] Epoch 81 [2560/8158] loss=0.001924163824878633\n",
            "[0m 28s] Epoch 81 [5120/8158] loss=0.0019306092581246048\n",
            "[0m 28s] Epoch 81 [7680/8158] loss=0.001942235161550343\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dev set: AUC  0.7956479141306977\n",
            "[0m 29s] Epoch 82 [2560/8158] loss=0.001965641730930656\n",
            "[0m 29s] Epoch 82 [5120/8158] loss=0.0019439338881056755\n",
            "[0m 29s] Epoch 82 [7680/8158] loss=0.0019523025529148677\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7957439308881487\n",
            "[0m 29s] Epoch 83 [2560/8158] loss=0.0019652790506370366\n",
            "[0m 29s] Epoch 83 [5120/8158] loss=0.001949221728136763\n",
            "[0m 29s] Epoch 83 [7680/8158] loss=0.0019396760500967503\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7962878152839727\n",
            "[0m 29s] Epoch 84 [2560/8158] loss=0.0018707237904891372\n",
            "[0m 29s] Epoch 84 [5120/8158] loss=0.0019404951017349958\n",
            "[0m 29s] Epoch 84 [7680/8158] loss=0.0019506452488712966\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.795944808051763\n",
            "[0m 30s] Epoch 85 [2560/8158] loss=0.001925750041846186\n",
            "[0m 30s] Epoch 85 [5120/8158] loss=0.0019310809497255833\n",
            "[0m 30s] Epoch 85 [7680/8158] loss=0.0019409397228931388\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.795183622573287\n",
            "[0m 30s] Epoch 86 [2560/8158] loss=0.0019682250334881247\n",
            "[0m 30s] Epoch 86 [5120/8158] loss=0.0019498468085657806\n",
            "[0m 30s] Epoch 86 [7680/8158] loss=0.0019385911485490699\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7958026779831681\n",
            "[0m 30s] Epoch 87 [2560/8158] loss=0.0019457495189271867\n",
            "[0m 30s] Epoch 87 [5120/8158] loss=0.0019269912736490369\n",
            "[0m 30s] Epoch 87 [7680/8158] loss=0.0019426316294508675\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dev set: AUC  0.795600537441166\n",
            "[0m 31s] Epoch 88 [2560/8158] loss=0.001956246967893094\n",
            "[0m 31s] Epoch 88 [5120/8158] loss=0.0019717136456165463\n",
            "[0m 31s] Epoch 88 [7680/8158] loss=0.0019553821572723487\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7958203652805932\n",
            "[0m 31s] Epoch 89 [2560/8158] loss=0.0019256091210991145\n",
            "[0m 31s] Epoch 89 [5120/8158] loss=0.0019552650686819107\n",
            "[0m 31s] Epoch 89 [7680/8158] loss=0.0019461525681739053\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7961520021073151\n",
            "[0m 31s] Epoch 90 [2560/8158] loss=0.0019224790506996215\n",
            "[0m 32s] Epoch 90 [5120/8158] loss=0.0019264093425590545\n",
            "[0m 32s] Epoch 90 [7680/8158] loss=0.0019413159461691976\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7957938343344555\n",
            "[0m 32s] Epoch 91 [2560/8158] loss=0.001969692250713706\n",
            "[0m 32s] Epoch 91 [5120/8158] loss=0.0019670100416988136\n",
            "[0m 32s] Epoch 91 [7680/8158] loss=0.0019401121302507819\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7960496684579267\n",
            "[0m 32s] Epoch 92 [2560/8158] loss=0.0020099911955185233\n",
            "[0m 32s] Epoch 92 [5120/8158] loss=0.001965085422853008\n",
            "[0m 32s] Epoch 92 [7680/8158] loss=0.0019420805426004033\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7957363506178237\n",
            "[0m 32s] Epoch 93 [2560/8158] loss=0.001975466776639223\n",
            "[0m 33s] Epoch 93 [5120/8158] loss=0.0019577003142330794\n",
            "[0m 33s] Epoch 93 [7680/8158] loss=0.0019645613889830806\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7957275069691112\n",
            "[0m 33s] Epoch 94 [2560/8158] loss=0.0019455202738754452\n",
            "[0m 33s] Epoch 94 [5120/8158] loss=0.001928989199222997\n",
            "[0m 33s] Epoch 94 [7680/8158] loss=0.0019416859101814528\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7958525814294748\n",
            "[0m 33s] Epoch 95 [2560/8158] loss=0.001943440386094153\n",
            "[0m 33s] Epoch 95 [5120/8158] loss=0.0019441472017206252\n",
            "[0m 33s] Epoch 95 [7680/8158] loss=0.0019355180944936972\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7957565646720238\n",
            "[0m 34s] Epoch 96 [2560/8158] loss=0.001941198343411088\n",
            "[0m 34s] Epoch 96 [5120/8158] loss=0.0019473951775580644\n",
            "[0m 34s] Epoch 96 [7680/8158] loss=0.0019453808548860253\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7955162069337997\n",
            "[0m 34s] Epoch 97 [2560/8158] loss=0.001953295397106558\n",
            "[0m 34s] Epoch 97 [5120/8158] loss=0.0019338366342708468\n",
            "[0m 34s] Epoch 97 [7680/8158] loss=0.001933477163159599\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7955834818329346\n",
            "[0m 34s] Epoch 98 [2560/8158] loss=0.0019697421579621733\n",
            "[0m 35s] Epoch 98 [5120/8158] loss=0.0019423197256401182\n",
            "[0m 35s] Epoch 98 [7680/8158] loss=0.0019443773470508555\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7958405793347932\n",
            "[0m 35s] Epoch 99 [2560/8158] loss=0.0019624676438979804\n",
            "[0m 35s] Epoch 99 [5120/8158] loss=0.001961637916974723\n",
            "[0m 35s] Epoch 99 [7680/8158] loss=0.00193582185699294\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.795668128184898\n",
            "[0m 35s] Epoch 100 [2560/8158] loss=0.0019137777271680534\n",
            "[0m 35s] Epoch 100 [5120/8158] loss=0.001938679750310257\n",
            "[0m 36s] Epoch 100 [7680/8158] loss=0.0019428683794103564\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7952556351413752\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "w8SNjKVLEob_",
        "outputId": "63159791-31d7-4902-eb6d-a8915280d113"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "epoch = np.arange(1, len(acc_list) + 1, 1)\r\n",
        "acc_list = np.array(acc_list)\r\n",
        "plt.plot(epoch, acc_list)\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.grid()\r\n",
        "plt.show()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "XgycT-y8g-W2",
        "outputId": "990456d4-a487-4d30-bb7d-6299bd47bdbb"
      },
      "source": [
        "epoch = np.arange(1, len(acc_list) + 1, 1)\r\n",
        "acc_list = np.array(auc_list)\r\n",
        "plt.plot(epoch, auc_list)\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylabel('AUC')\r\n",
        "plt.grid()\r\n",
        "plt.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gebti5UQT-XR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTcbNhj3RTDG",
        "outputId": "a189d08c-d40c-4c94-d3e8-51dde15ced95"
      },
      "source": [
        "def predModel(): #for devset\r\n",
        "  correct = 0\r\n",
        "  total = len(devset)\r\n",
        "  proba = []\r\n",
        "  print(\"evaluating trained model ...\")\r\n",
        "  with torch.no_grad():\r\n",
        "    for i, (features, labels) in enumerate(devloader, 1):\r\n",
        "      inputs, seq_lengths, target = make_tensors(features, labels)\r\n",
        "      output = classifier(inputs, seq_lengths)\r\n",
        "      pred = output.max(dim=1, keepdim=True)[1]\r\n",
        "      proba_i = proba1(output)\r\n",
        "      proba.append(proba_i)\r\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\r\n",
        "\r\n",
        "    percent = '%.2f' % (100 * correct / total)\r\n",
        "    print(f'Dev set: Accuracy {correct}/{total} {percent}%')\r\n",
        "\r\n",
        "  return correct / total, proba\r\n",
        "\r\n",
        "a, proba = predModel()\r\n",
        "probas = proba[0]\r\n",
        "for i in range(1, len(proba)):\r\n",
        "  probas = torch.cat((probas, proba[i]))\r\n",
        "probas = np.array(probas).reshape(-1, 1)\r\n",
        "probas"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.07022686],\n",
              "       [0.31205487],\n",
              "       [0.4768338 ],\n",
              "       ...,\n",
              "       [0.42758897],\n",
              "       [0.72439045],\n",
              "       [0.6430584 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypv9hFX_8lH3",
        "outputId": "73862b85-d525-48f0-e76d-88a311448e4f"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\r\n",
        "roc_auc_score(y_dev, probas)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7964488960283805"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4Rsoue8bVup"
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pxT7TmE8Heg"
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd_Apc2bB7XI",
        "outputId": "45f4d61b-1d8f-4168-e72a-34718f1b28d3"
      },
      "source": [
        "def make_tensors2(features):\r\n",
        "  seq_lengths = torch.LongTensor(features.size(0))\r\n",
        "  seq_lengths = seq_lengths.long()\r\n",
        "  return create_tensor(features), create_tensor(seq_lengths)\r\n",
        "\r\n",
        "def testProba(loader_to_pred):\r\n",
        "  proba = []\r\n",
        "  print(\"making prediction ...\")\r\n",
        "  with torch.no_grad():\r\n",
        "    for i, (features) in enumerate(loader_to_pred, 1):\r\n",
        "      inputs, seq_lengths = make_tensors2(features)\r\n",
        "      output = classifier(inputs, seq_lengths)\r\n",
        "      pred = output.max(dim=1, keepdim=True)[1]\r\n",
        "      proba_i = proba1(output)\r\n",
        "      proba.append(proba_i)\r\n",
        "\r\n",
        "\r\n",
        "  return proba\r\n",
        "\r\n",
        "proba = testProba(testloader)\r\n",
        "probas = proba[0]\r\n",
        "for i in range(1, len(proba)):\r\n",
        "  probas = torch.cat((probas, proba[i]))\r\n",
        "probas = np.array(probas).reshape(-1)\r\n",
        "probas"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "making prediction ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.03299821, 0.08941725, 0.6744743 , ..., 0.07172112, 0.40681252,\n",
              "       0.38884482], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlAPQ_uvCjyz",
        "outputId": "3667caaa-d210-47f1-838e-51ba39c041a2"
      },
      "source": [
        "probas.shape"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2773,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2bXyH9ncH-4",
        "outputId": "0e915205-d036-4176-fc3b-9b7bfa54fd60"
      },
      "source": [
        "probas.max()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.99898624"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8NzFRwSCrVQ"
      },
      "source": [
        "output=pd.DataFrame({'user_id':test_df.user_id,'proba':probas})\r\n",
        "output.to_csv('/content/drive/MyDrive/Colab Notebooks/xtzx/proj/result.csv', index=None) "
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBJ6VJrWthzj",
        "outputId": "2af3db1d-ab40-436e-f870-b1a46f400e55"
      },
      "source": [
        "proba = testProba(testloader)\r\n",
        "probas = proba[0]\r\n",
        "for i in range(1, len(proba)):\r\n",
        "  probas = torch.cat((probas, proba[i]))\r\n",
        "probas = np.array(probas).reshape(-1)\r\n",
        "test_feature_df['rnn'] = probas\r\n",
        "\r\n",
        "def predProba(loader_to_pred):\r\n",
        "  proba = []\r\n",
        "  print(\"making prediction ...\")\r\n",
        "  with torch.no_grad():\r\n",
        "    for i, (features, labels) in enumerate(loader_to_pred, 1):\r\n",
        "      inputs, seq_lengths = make_tensors2(features)\r\n",
        "      output = classifier(inputs, seq_lengths)\r\n",
        "      pred = output.max(dim=1, keepdim=True)[1]\r\n",
        "      proba_i = proba1(output)\r\n",
        "      proba.append(proba_i)\r\n",
        "\r\n",
        "\r\n",
        "  return proba\r\n",
        "\r\n",
        "proba = predProba(trainloader)\r\n",
        "probas = proba[0]\r\n",
        "for i in range(1, len(proba)):\r\n",
        "  probas = torch.cat((probas, proba[i]))\r\n",
        "probas = np.array(probas).reshape(-1)\r\n",
        "train_feature_df['rnn'] = probas\r\n",
        "\r\n",
        "proba = predProba(devloader)\r\n",
        "probas = proba[0]\r\n",
        "for i in range(1, len(proba)):\r\n",
        "  probas = torch.cat((probas, proba[i]))\r\n",
        "probas = np.array(probas).reshape(-1)\r\n",
        "dev_feature_df['rnn'] = probas\r\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "making prediction ...\n",
            "making prediction ...\n",
            "making prediction ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYDwKGuDtbI1",
        "outputId": "68074dab-a582-4a45-e984-ffb08f9d59a3"
      },
      "source": [
        "feature_names = train_feature_df.columns[3:]\r\n",
        "feature_names"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['last_day', 'rounds', 'num_days', 'max_level', '1.0last_level',\n",
              "       '1.0last_trytimes', '1.0max_retrytimes', '1.0num_levels',\n",
              "       '1.0passrate_ratio', '1.0retrytimes_ratio', '1.0num_rounds',\n",
              "       '1.0total_duration', '1.0num_help', '1.0win_duration_ratio',\n",
              "       '2.0last_level', '2.0last_trytimes', '2.0max_retrytimes',\n",
              "       '2.0num_levels', '2.0passrate_ratio', '2.0retrytimes_ratio',\n",
              "       '2.0num_rounds', '2.0total_duration', '2.0num_help',\n",
              "       '2.0win_duration_ratio', '2.0last_time', '3.0last_level',\n",
              "       '3.0last_trytimes', '3.0max_retrytimes', '3.0num_levels',\n",
              "       '3.0passrate_ratio', '3.0retrytimes_ratio', '3.0num_rounds',\n",
              "       '3.0total_duration', '3.0num_help', '3.0win_duration_ratio',\n",
              "       '3.0last_time', '4.0last_level', '4.0last_trytimes',\n",
              "       '4.0max_retrytimes', '4.0num_levels', '4.0passrate_ratio',\n",
              "       '4.0retrytimes_ratio', '4.0num_rounds', '4.0total_duration',\n",
              "       '4.0num_help', '4.0win_duration_ratio', '4.0last_time', 'rnn'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xNaGcvwyVVD"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npLfA1aDC3Ub"
      },
      "source": [
        "#将训练集和验证集的特征提取出来，放到矩阵里\r\n",
        "x_train = train_feature_df[feature_names].values\r\n",
        "x_train = scaler.fit_transform(x_train)\r\n",
        "y_train = train_feature_df['label']\r\n",
        "x_dev = dev_feature_df[feature_names].values\r\n",
        "x_dev = scaler.transform(x_dev)\r\n",
        "y_dev = dev_feature_df['label']\r\n",
        "x_test = test_feature_df[feature_names].values\r\n",
        "x_test = scaler.transform(x_test)\r\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3h-rgx9u4hW",
        "outputId": "b93c8a91-de78-402f-d10e-a7e6462d0ba2"
      },
      "source": [
        "import lightgbm as lgb\r\n",
        "gbm = lgb.LGBMClassifier(boosting_type='gbdt',\r\n",
        "                         objective = 'binary',\r\n",
        "                         metric = 'auc',\r\n",
        "                         verbose = 0,\r\n",
        "                         max_depth = 4,\r\n",
        "                         learning_rate = 0.02,\r\n",
        "                         num_leaves = 35,\r\n",
        "                         feature_fraction=0.6,\r\n",
        "                         bagging_fraction= 0.7,\r\n",
        "                         bagging_freq= 6,\r\n",
        "                         lambda_l1= 0,\r\n",
        "                         lambda_l2= 38,\r\n",
        "                         cat_smooth = 1,\r\n",
        "                         class_weight = 'balanced',\r\n",
        "                         )\r\n",
        "gbm.fit(x_train, y_train)\r\n",
        "proba = gbm.predict_proba(x_dev)[:, 1]\r\n",
        "roc_auc_score(y_dev, proba)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8003817929487063"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "f-CZWGxFwc29",
        "outputId": "71ef71d9-f11b-4356-9371-3a7b62349c2e"
      },
      "source": [
        "from xgboost import XGBClassifier\r\n",
        "model = XGBClassifier(n_estimators = 80,\r\n",
        "      max_depth = 3,\r\n",
        "      min_child_weight = 1,\r\n",
        "      gamma = 0,\r\n",
        "      subsample = 1,\r\n",
        "      colsample_bytree = 1,\r\n",
        "      reg_alpha = 0,\r\n",
        "      reg_lambda = 1 ,\r\n",
        "      learning_rate = 0.1,\r\n",
        "      )\r\n",
        "model.fit(x_train, y_train)\r\n",
        "proba = model.predict_proba(x_dev)[:, 1]\r\n",
        "print(roc_auc_score(y_dev, proba))\r\n",
        "\r\n",
        "def create_feature_map(features):\r\n",
        "    outfile = open('clf.fmap','w')\r\n",
        "    for i,f in enumerate(features):\r\n",
        "        outfile.write('{0}\\t{1}\\tq\\n'.format(i,f))\r\n",
        "    outfile.close()\r\n",
        "create_feature_map(feature_names)\r\n",
        "\r\n",
        "import xgboost as xgb\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "xgb.plot_tree(model, num_trees=0, fmap='clf.fmap')\r\n",
        "fig = plt.gcf()\r\n",
        "fig.set_size_inches((240, 240))\r\n",
        "# fig.savefig('tree.png')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7990283988510837\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 17280x17280 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aeupb3faLdo7"
      },
      "source": [
        "\\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI_9gcI_Xza1",
        "outputId": "35e390e5-e861-4986-cfc0-a84e325bbd69"
      },
      "source": [
        "#调参\r\n",
        "HIDDEN_SIZEs = [64, 128, 192, 256, 384, 512]\r\n",
        "N_EPOCHS  = 60\r\n",
        "accs = []\r\n",
        "aucs = []\r\n",
        "for HIDDEN_SIZE in HIDDEN_SIZEs:\r\n",
        "  \r\n",
        "  if __name__ == '__main__':\r\n",
        "    classifier = RNNClassifier(11, HIDDEN_SIZE, N_CLASS, N_LAYER, bidirectional = False)\r\n",
        "    if USE_GPU:\r\n",
        "      device = torch.device(\"cuda:0\")\r\n",
        "      classifier.to(device)\r\n",
        "\r\n",
        "    criterion = torch.nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.0005)\r\n",
        "\r\n",
        "    start = time.time()\r\n",
        "    print(\"Training for %d epochs...\" % N_EPOCHS)\r\n",
        "    acc_list = []\r\n",
        "    auc_list = []\r\n",
        "    for epoch in range(1, N_EPOCHS + 1):\r\n",
        "      # Train cycle\r\n",
        "      trainModel()\r\n",
        "      acc, auc = devModel_auc()\r\n",
        "      acc_list.append(acc)\r\n",
        "      auc_list.append(auc)\r\n",
        "    accs.append(acc_list)\r\n",
        "    aucs.append(auc_list)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.0025588596239686013\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.002515297138597816\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.002488892145144443\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7637352919067348\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.002358920522965491\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.0023558591376058756\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.0023325604231407243\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1758/2658 66.14%\n",
            "Dec set: AUC  0.7650602599906383\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.0021973100258037446\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.002181825810112059\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.0021527899118761224\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1957/2658 73.63%\n",
            "Dec set: AUC  0.7662206730395684\n",
            "[0m 0s] Epoch 4 [2560/8158] loss=0.002105749025940895\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.002061964152380824\n",
            "[0m 0s] Epoch 4 [7680/8158] loss=0.0020523226819932463\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1938/2658 72.91%\n",
            "Dec set: AUC  0.7802340661138543\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.0020466311369091274\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.0020302701741456985\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.0020273737764606873\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1970/2658 74.12%\n",
            "Dec set: AUC  0.7849486784114532\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.002038598037324846\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.0020294884219765664\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.002008998483264198\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dec set: AUC  0.7877217939720427\n",
            "[0m 1s] Epoch 7 [2560/8158] loss=0.0019922399311326443\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.0019867926952429117\n",
            "[0m 1s] Epoch 7 [7680/8158] loss=0.0019872846237073342\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1973/2658 74.23%\n",
            "Dec set: AUC  0.7884100193486401\n",
            "[0m 1s] Epoch 8 [2560/8158] loss=0.002049769670702517\n",
            "[0m 1s] Epoch 8 [5120/8158] loss=0.0020234275376424193\n",
            "[0m 1s] Epoch 8 [7680/8158] loss=0.002005925999643902\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7885606772213509\n",
            "[0m 1s] Epoch 9 [2560/8158] loss=0.001964572351425886\n",
            "[0m 1s] Epoch 9 [5120/8158] loss=0.0019794234656728806\n",
            "[0m 1s] Epoch 9 [7680/8158] loss=0.002007888067358484\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dec set: AUC  0.7888341986422474\n",
            "[0m 2s] Epoch 10 [2560/8158] loss=0.0019455846515484155\n",
            "[0m 2s] Epoch 10 [5120/8158] loss=0.00199149054242298\n",
            "[0m 2s] Epoch 10 [7680/8158] loss=0.0019916182616725566\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1979/2658 74.45%\n",
            "Dec set: AUC  0.7888168271894189\n",
            "[0m 2s] Epoch 11 [2560/8158] loss=0.0019988127751275897\n",
            "[0m 2s] Epoch 11 [5120/8158] loss=0.002007559029152617\n",
            "[0m 2s] Epoch 11 [7680/8158] loss=0.001988705530917893\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7889425333389763\n",
            "[0m 2s] Epoch 12 [2560/8158] loss=0.001990109903272241\n",
            "[0m 2s] Epoch 12 [5120/8158] loss=0.0019669531146064402\n",
            "[0m 2s] Epoch 12 [7680/8158] loss=0.0019794671679846942\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7891623611784035\n",
            "[0m 2s] Epoch 13 [2560/8158] loss=0.001934797700960189\n",
            "[0m 3s] Epoch 13 [5120/8158] loss=0.0020069528312887997\n",
            "[0m 3s] Epoch 13 [7680/8158] loss=0.0019976098750097058\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7891724682055036\n",
            "[0m 3s] Epoch 14 [2560/8158] loss=0.0020185521454550324\n",
            "[0m 3s] Epoch 14 [5120/8158] loss=0.0019987756095360965\n",
            "[0m 3s] Epoch 14 [7680/8158] loss=0.001984313301121195\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dec set: AUC  0.7891686780703412\n",
            "[0m 3s] Epoch 15 [2560/8158] loss=0.001968656771350652\n",
            "[0m 3s] Epoch 15 [5120/8158] loss=0.001977719261776656\n",
            "[0m 3s] Epoch 15 [7680/8158] loss=0.001990758596609036\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7893859791529932\n",
            "[0m 3s] Epoch 16 [2560/8158] loss=0.0020038264920003712\n",
            "[0m 3s] Epoch 16 [5120/8158] loss=0.0019805842952337117\n",
            "[0m 3s] Epoch 16 [7680/8158] loss=0.001978338179954638\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7894939980051255\n",
            "[0m 4s] Epoch 17 [2560/8158] loss=0.0019384053535759448\n",
            "[0m 4s] Epoch 17 [5120/8158] loss=0.0019565590075217186\n",
            "[0m 4s] Epoch 17 [7680/8158] loss=0.0019802916639794907\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7894712571941502\n",
            "[0m 4s] Epoch 18 [2560/8158] loss=0.002006295893806964\n",
            "[0m 4s] Epoch 18 [5120/8158] loss=0.001973754243226722\n",
            "[0m 4s] Epoch 18 [7680/8158] loss=0.0019789996130081516\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7896557104387271\n",
            "[0m 4s] Epoch 19 [2560/8158] loss=0.002038176322821528\n",
            "[0m 4s] Epoch 19 [5120/8158] loss=0.0019940145721193403\n",
            "[0m 4s] Epoch 19 [7680/8158] loss=0.0019887050963006913\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7897378300339153\n",
            "[0m 4s] Epoch 20 [2560/8158] loss=0.0019676519325003026\n",
            "[0m 4s] Epoch 20 [5120/8158] loss=0.0019925986242014916\n",
            "[0m 5s] Epoch 20 [7680/8158] loss=0.001991496968548745\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7898768016565416\n",
            "[0m 5s] Epoch 21 [2560/8158] loss=0.0020395448198542\n",
            "[0m 5s] Epoch 21 [5120/8158] loss=0.0019943315244745465\n",
            "[0m 5s] Epoch 21 [7680/8158] loss=0.0019875410633782547\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7899908215560147\n",
            "[0m 5s] Epoch 22 [2560/8158] loss=0.001917532191146165\n",
            "[0m 5s] Epoch 22 [5120/8158] loss=0.0019614728225860745\n",
            "[0m 5s] Epoch 22 [7680/8158] loss=0.0019821866842297214\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dec set: AUC  0.7900947344283876\n",
            "[0m 5s] Epoch 23 [2560/8158] loss=0.002011127129662782\n",
            "[0m 5s] Epoch 23 [5120/8158] loss=0.0020023639081045985\n",
            "[0m 5s] Epoch 23 [7680/8158] loss=0.001981392405771961\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7902406546321451\n",
            "[0m 5s] Epoch 24 [2560/8158] loss=0.0019973644986748694\n",
            "[0m 5s] Epoch 24 [5120/8158] loss=0.0020011888118460774\n",
            "[0m 6s] Epoch 24 [7680/8158] loss=0.0019836436957120897\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7903932075724374\n",
            "[0m 6s] Epoch 25 [2560/8158] loss=0.0020061201765201988\n",
            "[0m 6s] Epoch 25 [5120/8158] loss=0.001999611617065966\n",
            "[0m 6s] Epoch 25 [7680/8158] loss=0.0019824661935369174\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7905621844317672\n",
            "[0m 6s] Epoch 26 [2560/8158] loss=0.00193418487906456\n",
            "[0m 6s] Epoch 26 [5120/8158] loss=0.0019735724607016893\n",
            "[0m 6s] Epoch 26 [7680/8158] loss=0.00197249927247564\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7907150532166561\n",
            "[0m 6s] Epoch 27 [2560/8158] loss=0.00200099335052073\n",
            "[0m 6s] Epoch 27 [5120/8158] loss=0.0019672298280056567\n",
            "[0m 6s] Epoch 27 [7680/8158] loss=0.0019761307165026666\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7909576218670584\n",
            "[0m 6s] Epoch 28 [2560/8158] loss=0.0019756655558012424\n",
            "[0m 6s] Epoch 28 [5120/8158] loss=0.001977822225308046\n",
            "[0m 6s] Epoch 28 [7680/8158] loss=0.001981448525718103\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dec set: AUC  0.7911294413277601\n",
            "[0m 6s] Epoch 29 [2560/8158] loss=0.0020190890529192982\n",
            "[0m 7s] Epoch 29 [5120/8158] loss=0.0019697187934070827\n",
            "[0m 7s] Epoch 29 [7680/8158] loss=0.001959578902460635\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7913119995047557\n",
            "[0m 7s] Epoch 30 [2560/8158] loss=0.0019822991569526495\n",
            "[0m 7s] Epoch 30 [5120/8158] loss=0.001992101059295237\n",
            "[0m 7s] Epoch 30 [7680/8158] loss=0.001975866119998197\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.791422229269066\n",
            "[0m 7s] Epoch 31 [2560/8158] loss=0.0019131285022012889\n",
            "[0m 7s] Epoch 31 [5120/8158] loss=0.0019598124374169855\n",
            "[0m 7s] Epoch 31 [7680/8158] loss=0.0019729499937966464\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7915378283915234\n",
            "[0m 7s] Epoch 32 [2560/8158] loss=0.0020004729740321636\n",
            "[0m 7s] Epoch 32 [5120/8158] loss=0.0019745784695260225\n",
            "[0m 7s] Epoch 32 [7680/8158] loss=0.0019658387172967196\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7916758524803592\n",
            "[0m 7s] Epoch 33 [2560/8158] loss=0.0019583294284529983\n",
            "[0m 7s] Epoch 33 [5120/8158] loss=0.0019915639073587953\n",
            "[0m 7s] Epoch 33 [7680/8158] loss=0.001972751982975751\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dec set: AUC  0.7918773613331674\n",
            "[0m 8s] Epoch 34 [2560/8158] loss=0.001977281679864973\n",
            "[0m 8s] Epoch 34 [5120/8158] loss=0.0019797983812168242\n",
            "[0m 8s] Epoch 34 [7680/8158] loss=0.0019611460350764296\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7919777999149746\n",
            "[0m 8s] Epoch 35 [2560/8158] loss=0.0019779760506935416\n",
            "[0m 8s] Epoch 35 [5120/8158] loss=0.0019510490470565855\n",
            "[0m 8s] Epoch 35 [7680/8158] loss=0.0019667466714357335\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7921837305921391\n",
            "[0m 8s] Epoch 36 [2560/8158] loss=0.0019466816214844585\n",
            "[0m 8s] Epoch 36 [5120/8158] loss=0.00197051705326885\n",
            "[0m 8s] Epoch 36 [7680/8158] loss=0.001963571746212741\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7923315458634781\n",
            "[0m 8s] Epoch 37 [2560/8158] loss=0.002010973554570228\n",
            "[0m 8s] Epoch 37 [5120/8158] loss=0.0019650400790851563\n",
            "[0m 8s] Epoch 37 [7680/8158] loss=0.0019524747117732962\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7924326161344791\n",
            "[0m 8s] Epoch 38 [2560/8158] loss=0.002052318956702948\n",
            "[0m 9s] Epoch 38 [5120/8158] loss=0.002012208040105179\n",
            "[0m 9s] Epoch 38 [7680/8158] loss=0.001970114232972264\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7925431617433863\n",
            "[0m 9s] Epoch 39 [2560/8158] loss=0.00192838367074728\n",
            "[0m 9s] Epoch 39 [5120/8158] loss=0.0019436978618614376\n",
            "[0m 9s] Epoch 39 [7680/8158] loss=0.001952290857055535\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7927225614744132\n",
            "[0m 9s] Epoch 40 [2560/8158] loss=0.0019969812943600117\n",
            "[0m 9s] Epoch 40 [5120/8158] loss=0.0019530004763510078\n",
            "[0m 9s] Epoch 40 [7680/8158] loss=0.0019635634458002944\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7928627964754269\n",
            "[0m 9s] Epoch 41 [2560/8158] loss=0.0019647662877105175\n",
            "[0m 9s] Epoch 41 [5120/8158] loss=0.001966027106391266\n",
            "[0m 9s] Epoch 41 [7680/8158] loss=0.00196438212490951\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7929468111381965\n",
            "[0m 9s] Epoch 42 [2560/8158] loss=0.001974793174304068\n",
            "[0m 10s] Epoch 42 [5120/8158] loss=0.0019881604588590563\n",
            "[0m 10s] Epoch 42 [7680/8158] loss=0.001974484398184965\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dec set: AUC  0.7929941878277282\n",
            "[0m 10s] Epoch 43 [2560/8158] loss=0.001912101055495441\n",
            "[0m 10s] Epoch 43 [5120/8158] loss=0.0019666693173348905\n",
            "[0m 10s] Epoch 43 [7680/8158] loss=0.001977512473240495\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7930188237062846\n",
            "[0m 10s] Epoch 44 [2560/8158] loss=0.0019491821294650435\n",
            "[0m 10s] Epoch 44 [5120/8158] loss=0.0019277560990303756\n",
            "[0m 10s] Epoch 44 [7680/8158] loss=0.0019671709393151104\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7931836945858551\n",
            "[0m 10s] Epoch 45 [2560/8158] loss=0.0019765214645303788\n",
            "[0m 10s] Epoch 45 [5120/8158] loss=0.001960861898260191\n",
            "[0m 10s] Epoch 45 [7680/8158] loss=0.0019738495854350426\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7930687271525915\n",
            "[0m 10s] Epoch 46 [2560/8158] loss=0.002022826473694295\n",
            "[0m 10s] Epoch 46 [5120/8158] loss=0.0019489624071866273\n",
            "[0m 10s] Epoch 46 [7680/8158] loss=0.001959049100211511\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7932841331676623\n",
            "[0m 11s] Epoch 47 [2560/8158] loss=0.001957014121580869\n",
            "[0m 11s] Epoch 47 [5120/8158] loss=0.0019721655931789427\n",
            "[0m 11s] Epoch 47 [7680/8158] loss=0.001961477069805066\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7932493902620057\n",
            "[0m 11s] Epoch 48 [2560/8158] loss=0.0018979224609211088\n",
            "[0m 11s] Epoch 48 [5120/8158] loss=0.001965481339721009\n",
            "[0m 11s] Epoch 48 [7680/8158] loss=0.00196852619604518\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7932095938427992\n",
            "[0m 11s] Epoch 49 [2560/8158] loss=0.0020060230046510695\n",
            "[0m 11s] Epoch 49 [5120/8158] loss=0.0019670938258059324\n",
            "[0m 11s] Epoch 49 [7680/8158] loss=0.0019637083207877976\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7932936085055686\n",
            "[0m 11s] Epoch 50 [2560/8158] loss=0.001975065830629319\n",
            "[0m 11s] Epoch 50 [5120/8158] loss=0.0019376712618395687\n",
            "[0m 11s] Epoch 50 [7680/8158] loss=0.001958691041606168\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7934388970201326\n",
            "[0m 11s] Epoch 51 [2560/8158] loss=0.00195305107627064\n",
            "[0m 11s] Epoch 51 [5120/8158] loss=0.0019510328595060856\n",
            "[0m 12s] Epoch 51 [7680/8158] loss=0.001959400352401038\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7934572160067517\n",
            "[0m 12s] Epoch 52 [2560/8158] loss=0.0019993567490018903\n",
            "[0m 12s] Epoch 52 [5120/8158] loss=0.0019604158005677164\n",
            "[0m 12s] Epoch 52 [7680/8158] loss=0.0019571513092766207\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7935690249940464\n",
            "[0m 12s] Epoch 53 [2560/8158] loss=0.0018810574314557017\n",
            "[0m 12s] Epoch 53 [5120/8158] loss=0.001970298500964418\n",
            "[0m 12s] Epoch 53 [7680/8158] loss=0.001964389579370618\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7935993460753465\n",
            "[0m 12s] Epoch 54 [2560/8158] loss=0.0019326498731970787\n",
            "[0m 12s] Epoch 54 [5120/8158] loss=0.00196463389438577\n",
            "[0m 12s] Epoch 54 [7680/8158] loss=0.001957285252865404\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7937307374276479\n",
            "[0m 12s] Epoch 55 [2560/8158] loss=0.0019829390570521356\n",
            "[0m 12s] Epoch 55 [5120/8158] loss=0.001955607469426468\n",
            "[0m 12s] Epoch 55 [7680/8158] loss=0.001959247347743561\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7936176650619656\n",
            "[0m 12s] Epoch 56 [2560/8158] loss=0.0019567834679037333\n",
            "[0m 13s] Epoch 56 [5120/8158] loss=0.0019732916203793137\n",
            "[0m 13s] Epoch 56 [7680/8158] loss=0.0019576186508250735\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.7937863260766984\n",
            "[0m 13s] Epoch 57 [2560/8158] loss=0.0020076735294424\n",
            "[0m 13s] Epoch 57 [5120/8158] loss=0.0019668911176268012\n",
            "[0m 13s] Epoch 57 [7680/8158] loss=0.001961680590951194\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.7938839220571339\n",
            "[0m 13s] Epoch 58 [2560/8158] loss=0.0018918346147984266\n",
            "[0m 13s] Epoch 58 [5120/8158] loss=0.0019216280023101717\n",
            "[0m 13s] Epoch 58 [7680/8158] loss=0.0019507893788007398\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.7940080489837068\n",
            "[0m 13s] Epoch 59 [2560/8158] loss=0.0019978020107373597\n",
            "[0m 13s] Epoch 59 [5120/8158] loss=0.001974350173259154\n",
            "[0m 13s] Epoch 59 [7680/8158] loss=0.0019644964137114584\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7940048905377382\n",
            "[0m 13s] Epoch 60 [2560/8158] loss=0.001964172511361539\n",
            "[0m 13s] Epoch 60 [5120/8158] loss=0.0019626198103651405\n",
            "[0m 14s] Epoch 60 [7680/8158] loss=0.0019575405633077024\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7940004687133817\n",
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.0025684305699542167\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.0024856202653609216\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.0024388803091521063\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7654386418176983\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.0022805006010457873\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.002232112898491323\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.0021989799415071806\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1959/2658 73.70%\n",
            "Dec set: AUC  0.7672989664933102\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.0020566800609230994\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.002063351223478094\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.0020419492463891703\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1968/2658 74.04%\n",
            "Dec set: AUC  0.7856817537208073\n",
            "[0m 0s] Epoch 4 [2560/8158] loss=0.0020179447950795294\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.0020167780166957527\n",
            "[0m 0s] Epoch 4 [7680/8158] loss=0.0020214824277597168\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1973/2658 74.23%\n",
            "Dec set: AUC  0.7873620469761986\n",
            "[0m 0s] Epoch 5 [2560/8158] loss=0.0019963241065852344\n",
            "[0m 0s] Epoch 5 [5120/8158] loss=0.002014191425405443\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.002002845983952284\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dec set: AUC  0.7883342166453893\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.0020216275355778635\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.0019997015537228436\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.002006803813856095\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7887119667832554\n",
            "[0m 1s] Epoch 7 [2560/8158] loss=0.0019972512847743927\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.0019938136509153994\n",
            "[0m 1s] Epoch 7 [7680/8158] loss=0.0020021668945749602\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7890025438123832\n",
            "[0m 1s] Epoch 8 [2560/8158] loss=0.002002989733591676\n",
            "[0m 1s] Epoch 8 [5120/8158] loss=0.0019883836794178933\n",
            "[0m 1s] Epoch 8 [7680/8158] loss=0.0019966903841122986\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7890252846233585\n",
            "[0m 1s] Epoch 9 [2560/8158] loss=0.00199639901984483\n",
            "[0m 1s] Epoch 9 [5120/8158] loss=0.001984262216137722\n",
            "[0m 1s] Epoch 9 [7680/8158] loss=0.0019908613913382093\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7891983674624476\n",
            "[0m 1s] Epoch 10 [2560/8158] loss=0.0019724391167983413\n",
            "[0m 2s] Epoch 10 [5120/8158] loss=0.001971891731955111\n",
            "[0m 2s] Epoch 10 [7680/8158] loss=0.0019780602228517334\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7893442876662053\n",
            "[0m 2s] Epoch 11 [2560/8158] loss=0.0020015730406157672\n",
            "[0m 2s] Epoch 11 [5120/8158] loss=0.001991611474659294\n",
            "[0m 2s] Epoch 11 [7680/8158] loss=0.0019904393702745438\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dec set: AUC  0.7893626066528242\n",
            "[0m 2s] Epoch 12 [2560/8158] loss=0.001984793960582465\n",
            "[0m 2s] Epoch 12 [5120/8158] loss=0.0019875642901752144\n",
            "[0m 2s] Epoch 12 [7680/8158] loss=0.001982656994368881\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7894737839509254\n",
            "[0m 2s] Epoch 13 [2560/8158] loss=0.0019581284723244607\n",
            "[0m 2s] Epoch 13 [5120/8158] loss=0.0019622440624516456\n",
            "[0m 2s] Epoch 13 [7680/8158] loss=0.001984564921197792\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7896576055063085\n",
            "[0m 2s] Epoch 14 [2560/8158] loss=0.001980176672805101\n",
            "[0m 2s] Epoch 14 [5120/8158] loss=0.001977516454644501\n",
            "[0m 3s] Epoch 14 [7680/8158] loss=0.001982756199625631\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7897826799666722\n",
            "[0m 3s] Epoch 15 [2560/8158] loss=0.001960201992187649\n",
            "[0m 3s] Epoch 15 [5120/8158] loss=0.0019900729705113916\n",
            "[0m 3s] Epoch 15 [7680/8158] loss=0.0019902497025517125\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dec set: AUC  0.7900284070630432\n",
            "[0m 3s] Epoch 16 [2560/8158] loss=0.002024076459929347\n",
            "[0m 3s] Epoch 16 [5120/8158] loss=0.002004993468290195\n",
            "[0m 3s] Epoch 16 [7680/8158] loss=0.001990809078173091\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7903278277408836\n",
            "[0m 3s] Epoch 17 [2560/8158] loss=0.0020052571431733667\n",
            "[0m 3s] Epoch 17 [5120/8158] loss=0.002000181551557034\n",
            "[0m 3s] Epoch 17 [7680/8158] loss=0.0019764213357120752\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7904162642280095\n",
            "[0m 3s] Epoch 18 [2560/8158] loss=0.001943232282064855\n",
            "[0m 4s] Epoch 18 [5120/8158] loss=0.0019811968435533346\n",
            "[0m 4s] Epoch 18 [7680/8158] loss=0.0019779217119018236\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7906828370677745\n",
            "[0m 4s] Epoch 19 [2560/8158] loss=0.002014470682479441\n",
            "[0m 4s] Epoch 19 [5120/8158] loss=0.001979295088676736\n",
            "[0m 4s] Epoch 19 [7680/8158] loss=0.001974817171382407\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7908647635555763\n",
            "[0m 4s] Epoch 20 [2560/8158] loss=0.0020061225863173605\n",
            "[0m 4s] Epoch 20 [5120/8158] loss=0.0019953143957536666\n",
            "[0m 4s] Epoch 20 [7680/8158] loss=0.001976030607086917\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7910978568680724\n",
            "[0m 4s] Epoch 21 [2560/8158] loss=0.0019212604383938015\n",
            "[0m 4s] Epoch 21 [5120/8158] loss=0.0019641451770439746\n",
            "[0m 4s] Epoch 21 [7680/8158] loss=0.001965050003491342\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7914219134244693\n",
            "[0m 4s] Epoch 22 [2560/8158] loss=0.0019446148653514683\n",
            "[0m 4s] Epoch 22 [5120/8158] loss=0.0019504639669321478\n",
            "[0m 4s] Epoch 22 [7680/8158] loss=0.0019701054009298486\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7918066121434667\n",
            "[0m 5s] Epoch 23 [2560/8158] loss=0.00195408578729257\n",
            "[0m 5s] Epoch 23 [5120/8158] loss=0.0019555864215362816\n",
            "[0m 5s] Epoch 23 [7680/8158] loss=0.0019517272904825708\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1975/2658 74.30%\n",
            "Dec set: AUC  0.7920125428206313\n",
            "[0m 5s] Epoch 24 [2560/8158] loss=0.0019414457958191634\n",
            "[0m 5s] Epoch 24 [5120/8158] loss=0.0019401324971113354\n",
            "[0m 5s] Epoch 24 [7680/8158] loss=0.001975244656205177\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7922525847142586\n",
            "[0m 5s] Epoch 25 [2560/8158] loss=0.0019530457444489001\n",
            "[0m 5s] Epoch 25 [5120/8158] loss=0.0019631186907645313\n",
            "[0m 5s] Epoch 25 [7680/8158] loss=0.0019729820700983207\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7925867482977554\n",
            "[0m 5s] Epoch 26 [2560/8158] loss=0.002036536659579724\n",
            "[0m 5s] Epoch 26 [5120/8158] loss=0.001974972226889804\n",
            "[0m 5s] Epoch 26 [7680/8158] loss=0.001966975606046617\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.792712454447313\n",
            "[0m 5s] Epoch 27 [2560/8158] loss=0.0019572173478081822\n",
            "[0m 5s] Epoch 27 [5120/8158] loss=0.0019862870452925564\n",
            "[0m 6s] Epoch 27 [7680/8158] loss=0.0019711329640510183\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7926972939066628\n",
            "[0m 6s] Epoch 28 [2560/8158] loss=0.0019795660045929253\n",
            "[0m 6s] Epoch 28 [5120/8158] loss=0.001976079639280215\n",
            "[0m 6s] Epoch 28 [7680/8158] loss=0.001987211355784287\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7931874847210175\n",
            "[0m 6s] Epoch 29 [2560/8158] loss=0.0019734948640689255\n",
            "[0m 6s] Epoch 29 [5120/8158] loss=0.0019734963250812145\n",
            "[0m 6s] Epoch 29 [7680/8158] loss=0.0019651721386859814\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7931318960719671\n",
            "[0m 6s] Epoch 30 [2560/8158] loss=0.001993206120096147\n",
            "[0m 6s] Epoch 30 [5120/8158] loss=0.0019863876397721468\n",
            "[0m 6s] Epoch 30 [7680/8158] loss=0.00196872684561337\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7932648666472528\n",
            "[0m 6s] Epoch 31 [2560/8158] loss=0.001906217250507325\n",
            "[0m 6s] Epoch 31 [5120/8158] loss=0.0019524392206221818\n",
            "[0m 6s] Epoch 31 [7680/8158] loss=0.0019674386596307157\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.793585448913084\n",
            "[0m 7s] Epoch 32 [2560/8158] loss=0.0019963725004345178\n",
            "[0m 7s] Epoch 32 [5120/8158] loss=0.001968392595881596\n",
            "[0m 7s] Epoch 32 [7680/8158] loss=0.001960545677381257\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7937661120224984\n",
            "[0m 7s] Epoch 33 [2560/8158] loss=0.001953742408659309\n",
            "[0m 7s] Epoch 33 [5120/8158] loss=0.0019522335031069815\n",
            "[0m 7s] Epoch 33 [7680/8158] loss=0.001959671437119444\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2009/2658 75.58%\n",
            "Dec set: AUC  0.793627140399872\n",
            "[0m 7s] Epoch 34 [2560/8158] loss=0.0019531479803845285\n",
            "[0m 7s] Epoch 34 [5120/8158] loss=0.0019717921095434576\n",
            "[0m 7s] Epoch 34 [7680/8158] loss=0.0019755201530642806\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7937092599950601\n",
            "[0m 7s] Epoch 35 [2560/8158] loss=0.0020531446556560696\n",
            "[0m 7s] Epoch 35 [5120/8158] loss=0.00199173521832563\n",
            "[0m 8s] Epoch 35 [7680/8158] loss=0.001954651402775198\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7938078035092859\n",
            "[0m 8s] Epoch 36 [2560/8158] loss=0.001957637770101428\n",
            "[0m 8s] Epoch 36 [5120/8158] loss=0.0019572841236367823\n",
            "[0m 8s] Epoch 36 [7680/8158] loss=0.0019479033110352854\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7941672346605334\n",
            "[0m 8s] Epoch 37 [2560/8158] loss=0.0019256706465967\n",
            "[0m 8s] Epoch 37 [5120/8158] loss=0.0019353571289684624\n",
            "[0m 8s] Epoch 37 [7680/8158] loss=0.0019550417588713267\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7940017320917692\n",
            "[0m 8s] Epoch 38 [2560/8158] loss=0.001909754704684019\n",
            "[0m 8s] Epoch 38 [5120/8158] loss=0.0019171535677742213\n",
            "[0m 8s] Epoch 38 [7680/8158] loss=0.001959311546913038\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7942341937150715\n",
            "[0m 8s] Epoch 39 [2560/8158] loss=0.0019535414641723035\n",
            "[0m 8s] Epoch 39 [5120/8158] loss=0.001968570810277015\n",
            "[0m 8s] Epoch 39 [7680/8158] loss=0.0019710700182865065\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7941110143222891\n",
            "[0m 8s] Epoch 40 [2560/8158] loss=0.0019513059756718575\n",
            "[0m 9s] Epoch 40 [5120/8158] loss=0.001959867973346263\n",
            "[0m 9s] Epoch 40 [7680/8158] loss=0.001958238418834905\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dec set: AUC  0.7943662167565665\n",
            "[0m 9s] Epoch 41 [2560/8158] loss=0.001959972910117358\n",
            "[0m 9s] Epoch 41 [5120/8158] loss=0.001961912150727585\n",
            "[0m 9s] Epoch 41 [7680/8158] loss=0.0019588577289444704\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7943940110810919\n",
            "[0m 9s] Epoch 42 [2560/8158] loss=0.0019452846026979387\n",
            "[0m 9s] Epoch 42 [5120/8158] loss=0.0019708745065145196\n",
            "[0m 9s] Epoch 42 [7680/8158] loss=0.0019660298945382237\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7943876941891543\n",
            "[0m 9s] Epoch 43 [2560/8158] loss=0.0019537814310751855\n",
            "[0m 9s] Epoch 43 [5120/8158] loss=0.001929712062701583\n",
            "[0m 9s] Epoch 43 [7680/8158] loss=0.00195581087609753\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.794209557836515\n",
            "[0m 9s] Epoch 44 [2560/8158] loss=0.0019339343067258596\n",
            "[0m 10s] Epoch 44 [5120/8158] loss=0.0019710552296601236\n",
            "[0m 10s] Epoch 44 [7680/8158] loss=0.001958645705599338\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7944805525006364\n",
            "[0m 10s] Epoch 45 [2560/8158] loss=0.0019712608074769376\n",
            "[0m 10s] Epoch 45 [5120/8158] loss=0.001960127823986113\n",
            "[0m 10s] Epoch 45 [7680/8158] loss=0.001957628099868695\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dec set: AUC  0.7945790960148623\n",
            "[0m 10s] Epoch 46 [2560/8158] loss=0.0019961553742177786\n",
            "[0m 10s] Epoch 46 [5120/8158] loss=0.0019391551613807678\n",
            "[0m 10s] Epoch 46 [7680/8158] loss=0.0019517608607808748\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7943684276687447\n",
            "[0m 10s] Epoch 47 [2560/8158] loss=0.002009950974024832\n",
            "[0m 10s] Epoch 47 [5120/8158] loss=0.0019675556395668536\n",
            "[0m 10s] Epoch 47 [7680/8158] loss=0.0019638214415560167\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7946725860155384\n",
            "[0m 10s] Epoch 48 [2560/8158] loss=0.002012645942158997\n",
            "[0m 10s] Epoch 48 [5120/8158] loss=0.00195459418464452\n",
            "[0m 10s] Epoch 48 [7680/8158] loss=0.001968940064155807\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2009/2658 75.58%\n",
            "Dec set: AUC  0.7947262795970076\n",
            "[0m 11s] Epoch 49 [2560/8158] loss=0.0019928275141865015\n",
            "[0m 11s] Epoch 49 [5120/8158] loss=0.0019769470673054457\n",
            "[0m 11s] Epoch 49 [7680/8158] loss=0.001962596837741633\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7948734631791528\n",
            "[0m 11s] Epoch 50 [2560/8158] loss=0.001959114067722112\n",
            "[0m 11s] Epoch 50 [5120/8158] loss=0.0019467165984679013\n",
            "[0m 11s] Epoch 50 [7680/8158] loss=0.0019493835357328256\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7948721998007652\n",
            "[0m 11s] Epoch 51 [2560/8158] loss=0.001965092518366873\n",
            "[0m 11s] Epoch 51 [5120/8158] loss=0.001953390904236585\n",
            "[0m 11s] Epoch 51 [7680/8158] loss=0.0019646594184450803\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7950629699372797\n",
            "[0m 11s] Epoch 52 [2560/8158] loss=0.001968894910532981\n",
            "[0m 11s] Epoch 52 [5120/8158] loss=0.001993185706669465\n",
            "[0m 11s] Epoch 52 [7680/8158] loss=0.001961268647573888\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7951495113568241\n",
            "[0m 11s] Epoch 53 [2560/8158] loss=0.0019104961771517991\n",
            "[0m 11s] Epoch 53 [5120/8158] loss=0.0019349441456142813\n",
            "[0m 12s] Epoch 53 [7680/8158] loss=0.0019532011084568996\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7948791483818965\n",
            "[0m 12s] Epoch 54 [2560/8158] loss=0.0019504867726936937\n",
            "[0m 12s] Epoch 54 [5120/8158] loss=0.00195728843100369\n",
            "[0m 12s] Epoch 54 [7680/8158] loss=0.0019467386921557287\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7950863424374484\n",
            "[0m 12s] Epoch 55 [2560/8158] loss=0.0019472882500849663\n",
            "[0m 12s] Epoch 55 [5120/8158] loss=0.0019519582798238843\n",
            "[0m 12s] Epoch 55 [7680/8158] loss=0.0019603868985238176\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dec set: AUC  0.7950907642618048\n",
            "[0m 12s] Epoch 56 [2560/8158] loss=0.0018913243082351982\n",
            "[0m 12s] Epoch 56 [5120/8158] loss=0.0019055590266361833\n",
            "[0m 12s] Epoch 56 [7680/8158] loss=0.001945476319330434\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7952322626412061\n",
            "[0m 12s] Epoch 57 [2560/8158] loss=0.0019471629057079554\n",
            "[0m 12s] Epoch 57 [5120/8158] loss=0.0019546337018255143\n",
            "[0m 12s] Epoch 57 [7680/8158] loss=0.0019544331279272836\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7952884829794504\n",
            "[0m 13s] Epoch 58 [2560/8158] loss=0.0019213128020055593\n",
            "[0m 13s] Epoch 58 [5120/8158] loss=0.0019170815241523087\n",
            "[0m 13s] Epoch 58 [7680/8158] loss=0.0019386677459503213\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7953036435201007\n",
            "[0m 13s] Epoch 59 [2560/8158] loss=0.0019400740275159478\n",
            "[0m 13s] Epoch 59 [5120/8158] loss=0.0019442799966782331\n",
            "[0m 13s] Epoch 59 [7680/8158] loss=0.001953752377691368\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7952992216957444\n",
            "[0m 13s] Epoch 60 [2560/8158] loss=0.0019752749358303845\n",
            "[0m 13s] Epoch 60 [5120/8158] loss=0.0019875142665114255\n",
            "[0m 13s] Epoch 60 [7680/8158] loss=0.0019502570192950466\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7955506339948593\n",
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.0025579337729141117\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.0024722034111618997\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.002423876663669944\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.767364030480267\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.00221891060937196\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.0021619156235828997\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.002143437376556297\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1934/2658 72.76%\n",
            "Dec set: AUC  0.7720517959871314\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.0020923553733155132\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.002059954823926091\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.0020290442315551143\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7883582208347519\n",
            "[0m 0s] Epoch 4 [2560/8158] loss=0.0020038367831148205\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.002042192645603791\n",
            "[0m 0s] Epoch 4 [7680/8158] loss=0.002004768808061878\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1979/2658 74.45%\n",
            "Dec set: AUC  0.7882047203606692\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.002070502005517483\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.0020675193751230835\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.002023199076453845\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1968/2658 74.04%\n",
            "Dec set: AUC  0.7878054927902155\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.0020868265884928404\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.002003347483696416\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.002015165115396182\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1966/2658 73.97%\n",
            "Dec set: AUC  0.7896260210466207\n",
            "[0m 1s] Epoch 7 [2560/8158] loss=0.001971586502622813\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.0020044529694132505\n",
            "[0m 1s] Epoch 7 [7680/8158] loss=0.002001782471779734\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7895170546606978\n",
            "[0m 1s] Epoch 8 [2560/8158] loss=0.0019677583943121134\n",
            "[0m 1s] Epoch 8 [5120/8158] loss=0.0019827844109386204\n",
            "[0m 1s] Epoch 8 [7680/8158] loss=0.001991224898180614\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7894169319234873\n",
            "[0m 2s] Epoch 9 [2560/8158] loss=0.001994329015724361\n",
            "[0m 2s] Epoch 9 [5120/8158] loss=0.001994102291064337\n",
            "[0m 2s] Epoch 9 [7680/8158] loss=0.0019802630180493\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7894472530047876\n",
            "[0m 2s] Epoch 10 [2560/8158] loss=0.0019912869902327657\n",
            "[0m 2s] Epoch 10 [5120/8158] loss=0.001977826305665076\n",
            "[0m 2s] Epoch 10 [7680/8158] loss=0.0019819434732198716\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7894933663159318\n",
            "[0m 2s] Epoch 11 [2560/8158] loss=0.0019534629303961993\n",
            "[0m 2s] Epoch 11 [5120/8158] loss=0.0019863139779772608\n",
            "[0m 2s] Epoch 11 [7680/8158] loss=0.001983471983112395\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7897852067234471\n",
            "[0m 2s] Epoch 12 [2560/8158] loss=0.0020215281285345553\n",
            "[0m 2s] Epoch 12 [5120/8158] loss=0.0019914008560590448\n",
            "[0m 3s] Epoch 12 [7680/8158] loss=0.001984698115848005\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7898054207776473\n",
            "[0m 3s] Epoch 13 [2560/8158] loss=0.001999421720393002\n",
            "[0m 3s] Epoch 13 [5120/8158] loss=0.0019921756058465688\n",
            "[0m 3s] Epoch 13 [7680/8158] loss=0.0019784098956733944\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dec set: AUC  0.7901389526719506\n",
            "[0m 3s] Epoch 14 [2560/8158] loss=0.0020162362372502683\n",
            "[0m 3s] Epoch 14 [5120/8158] loss=0.0020003432699013503\n",
            "[0m 3s] Epoch 14 [7680/8158] loss=0.001982534556494405\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dec set: AUC  0.7903170890245897\n",
            "[0m 3s] Epoch 15 [2560/8158] loss=0.001955988188274205\n",
            "[0m 3s] Epoch 15 [5120/8158] loss=0.0019622727239038797\n",
            "[0m 3s] Epoch 15 [7680/8158] loss=0.0019799296782972912\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7905489189586983\n",
            "[0m 3s] Epoch 16 [2560/8158] loss=0.001995105412788689\n",
            "[0m 3s] Epoch 16 [5120/8158] loss=0.001983641256811097\n",
            "[0m 3s] Epoch 16 [7680/8158] loss=0.001972944378697624\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7906923124056808\n",
            "[0m 4s] Epoch 17 [2560/8158] loss=0.001957764779217541\n",
            "[0m 4s] Epoch 17 [5120/8158] loss=0.0020075881911907345\n",
            "[0m 4s] Epoch 17 [7680/8158] loss=0.001971015400098016\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dec set: AUC  0.7911755546389043\n",
            "[0m 4s] Epoch 18 [2560/8158] loss=0.0019429827225394547\n",
            "[0m 4s] Epoch 18 [5120/8158] loss=0.001969722210196778\n",
            "[0m 4s] Epoch 18 [7680/8158] loss=0.0019767802907153962\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7909159303802706\n",
            "[0m 4s] Epoch 19 [2560/8158] loss=0.0019352168892510235\n",
            "[0m 4s] Epoch 19 [5120/8158] loss=0.0019540281500667334\n",
            "[0m 4s] Epoch 19 [7680/8158] loss=0.0019822775192248326\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.791541302682089\n",
            "[0m 4s] Epoch 20 [2560/8158] loss=0.0019610173301771285\n",
            "[0m 4s] Epoch 20 [5120/8158] loss=0.0019722693716175852\n",
            "[0m 4s] Epoch 20 [7680/8158] loss=0.001969746946512411\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7918148241029854\n",
            "[0m 4s] Epoch 21 [2560/8158] loss=0.001971432659775019\n",
            "[0m 5s] Epoch 21 [5120/8158] loss=0.0019637654942926018\n",
            "[0m 5s] Epoch 21 [7680/8158] loss=0.0019754885463044046\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7920927673482382\n",
            "[0m 5s] Epoch 22 [2560/8158] loss=0.0019322851090691983\n",
            "[0m 5s] Epoch 22 [5120/8158] loss=0.0019563102396205068\n",
            "[0m 5s] Epoch 22 [7680/8158] loss=0.0019669554700764516\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7924067168775352\n",
            "[0m 5s] Epoch 23 [2560/8158] loss=0.0019351725815795362\n",
            "[0m 5s] Epoch 23 [5120/8158] loss=0.0019525570853147656\n",
            "[0m 5s] Epoch 23 [7680/8158] loss=0.0019694110533843437\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7927105593797318\n",
            "[0m 5s] Epoch 24 [2560/8158] loss=0.0019494125619530678\n",
            "[0m 5s] Epoch 24 [5120/8158] loss=0.0019595184945501386\n",
            "[0m 5s] Epoch 24 [7680/8158] loss=0.0019694148058382174\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7929095414757649\n",
            "[0m 5s] Epoch 25 [2560/8158] loss=0.0020263692480511964\n",
            "[0m 5s] Epoch 25 [5120/8158] loss=0.0019879084546118976\n",
            "[0m 6s] Epoch 25 [7680/8158] loss=0.001979777973610908\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7931363178963233\n",
            "[0m 6s] Epoch 26 [2560/8158] loss=0.001924657472409308\n",
            "[0m 6s] Epoch 26 [5120/8158] loss=0.0019477994123008103\n",
            "[0m 6s] Epoch 26 [7680/8158] loss=0.0019588655054879685\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7931799044506926\n",
            "[0m 6s] Epoch 27 [2560/8158] loss=0.0019337928970344365\n",
            "[0m 6s] Epoch 27 [5120/8158] loss=0.001939301285892725\n",
            "[0m 6s] Epoch 27 [7680/8158] loss=0.001964542797456185\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dec set: AUC  0.7932733944513686\n",
            "[0m 6s] Epoch 28 [2560/8158] loss=0.001970178366173059\n",
            "[0m 6s] Epoch 28 [5120/8158] loss=0.0019632213399745522\n",
            "[0m 6s] Epoch 28 [7680/8158] loss=0.0019716889636280637\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7933371950599379\n",
            "[0m 6s] Epoch 29 [2560/8158] loss=0.0019442090764641763\n",
            "[0m 6s] Epoch 29 [5120/8158] loss=0.0019446115649770946\n",
            "[0m 7s] Epoch 29 [7680/8158] loss=0.001964194292668253\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7936012411429278\n",
            "[0m 7s] Epoch 30 [2560/8158] loss=0.001858449273277074\n",
            "[0m 7s] Epoch 30 [5120/8158] loss=0.0019204655312933029\n",
            "[0m 7s] Epoch 30 [7680/8158] loss=0.0019537981483154\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1974/2658 74.27%\n",
            "Dec set: AUC  0.7933555140465567\n",
            "[0m 7s] Epoch 31 [2560/8158] loss=0.00195723072392866\n",
            "[0m 7s] Epoch 31 [5120/8158] loss=0.001978391851298511\n",
            "[0m 7s] Epoch 31 [7680/8158] loss=0.0019779416538464527\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7931571636397173\n",
            "[0m 7s] Epoch 32 [2560/8158] loss=0.001902410387992859\n",
            "[0m 7s] Epoch 32 [5120/8158] loss=0.0019222202769014984\n",
            "[0m 7s] Epoch 32 [7680/8158] loss=0.001957229831411193\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7930011364088595\n",
            "[0m 7s] Epoch 33 [2560/8158] loss=0.0019506584969349206\n",
            "[0m 7s] Epoch 33 [5120/8158] loss=0.0019627451780252158\n",
            "[0m 7s] Epoch 33 [7680/8158] loss=0.0019637342309579255\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7937345275628104\n",
            "[0m 8s] Epoch 34 [2560/8158] loss=0.0020026579848490657\n",
            "[0m 8s] Epoch 34 [5120/8158] loss=0.0019653277238830926\n",
            "[0m 8s] Epoch 34 [7680/8158] loss=0.0019623454970618087\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.7936802022921474\n",
            "[0m 8s] Epoch 35 [2560/8158] loss=0.0019254672108218074\n",
            "[0m 8s] Epoch 35 [5120/8158] loss=0.0019337911624461412\n",
            "[0m 8s] Epoch 35 [7680/8158] loss=0.001955940457992256\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.7941470206063331\n",
            "[0m 8s] Epoch 36 [2560/8158] loss=0.0019762574112974106\n",
            "[0m 8s] Epoch 36 [5120/8158] loss=0.0019782595103606583\n",
            "[0m 8s] Epoch 36 [7680/8158] loss=0.0019549360149540007\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7942840971613783\n",
            "[0m 8s] Epoch 37 [2560/8158] loss=0.0019364578532986344\n",
            "[0m 9s] Epoch 37 [5120/8158] loss=0.0019519164052326232\n",
            "[0m 9s] Epoch 37 [7680/8158] loss=0.0019493791507557035\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dec set: AUC  0.7939992053349944\n",
            "[0m 9s] Epoch 38 [2560/8158] loss=0.0019763272255659105\n",
            "[0m 9s] Epoch 38 [5120/8158] loss=0.001954236754681915\n",
            "[0m 9s] Epoch 38 [7680/8158] loss=0.00194860395276919\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7943308421617161\n",
            "[0m 9s] Epoch 39 [2560/8158] loss=0.0020168879185803235\n",
            "[0m 9s] Epoch 39 [5120/8158] loss=0.0019682902144268154\n",
            "[0m 9s] Epoch 39 [7680/8158] loss=0.0019683593690084916\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dec set: AUC  0.7946492135153694\n",
            "[0m 9s] Epoch 40 [2560/8158] loss=0.0019729359657503663\n",
            "[0m 9s] Epoch 40 [5120/8158] loss=0.00197420590557158\n",
            "[0m 9s] Epoch 40 [7680/8158] loss=0.0019646589256202183\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2010/2658 75.62%\n",
            "Dec set: AUC  0.7942443007421717\n",
            "[0m 9s] Epoch 41 [2560/8158] loss=0.001904735085554421\n",
            "[0m 10s] Epoch 41 [5120/8158] loss=0.0019379002624191344\n",
            "[0m 10s] Epoch 41 [7680/8158] loss=0.0019568351407845816\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dec set: AUC  0.7945854129068001\n",
            "[0m 10s] Epoch 42 [2560/8158] loss=0.001960204669740051\n",
            "[0m 10s] Epoch 42 [5120/8158] loss=0.001967427524505183\n",
            "[0m 10s] Epoch 42 [7680/8158] loss=0.001969247388963898\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7945216122982305\n",
            "[0m 10s] Epoch 43 [2560/8158] loss=0.0019995798938907683\n",
            "[0m 10s] Epoch 43 [5120/8158] loss=0.001977319293655455\n",
            "[0m 10s] Epoch 43 [7680/8158] loss=0.0019514977000653743\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.79484945898979\n",
            "[0m 10s] Epoch 44 [2560/8158] loss=0.0019437869894318282\n",
            "[0m 10s] Epoch 44 [5120/8158] loss=0.001950704545015469\n",
            "[0m 10s] Epoch 44 [7680/8158] loss=0.0019546134940659006\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7948254548004273\n",
            "[0m 10s] Epoch 45 [2560/8158] loss=0.001866491639520973\n",
            "[0m 10s] Epoch 45 [5120/8158] loss=0.001922223181463778\n",
            "[0m 11s] Epoch 45 [7680/8158] loss=0.0019505538162775337\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7947610225026642\n",
            "[0m 11s] Epoch 46 [2560/8158] loss=0.001984600827563554\n",
            "[0m 11s] Epoch 46 [5120/8158] loss=0.001972259039757773\n",
            "[0m 11s] Epoch 46 [7680/8158] loss=0.0019505073704446354\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dec set: AUC  0.7948842018954467\n",
            "[0m 11s] Epoch 47 [2560/8158] loss=0.0019593976554460823\n",
            "[0m 11s] Epoch 47 [5120/8158] loss=0.001964857766870409\n",
            "[0m 11s] Epoch 47 [7680/8158] loss=0.0019525970448739827\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7949922207475788\n",
            "[0m 11s] Epoch 48 [2560/8158] loss=0.0019236821797676384\n",
            "[0m 11s] Epoch 48 [5120/8158] loss=0.0019309530151076614\n",
            "[0m 11s] Epoch 48 [7680/8158] loss=0.0019501583379072447\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7951210853431052\n",
            "[0m 11s] Epoch 49 [2560/8158] loss=0.0019560036016628146\n",
            "[0m 11s] Epoch 49 [5120/8158] loss=0.001939161488553509\n",
            "[0m 11s] Epoch 49 [7680/8158] loss=0.0019423190620727836\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7949435806796598\n",
            "[0m 12s] Epoch 50 [2560/8158] loss=0.001946625846903771\n",
            "[0m 12s] Epoch 50 [5120/8158] loss=0.0019247829332016408\n",
            "[0m 12s] Epoch 50 [7680/8158] loss=0.001947267248760909\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dec set: AUC  0.7951438261540803\n",
            "[0m 12s] Epoch 51 [2560/8158] loss=0.0019221011199988425\n",
            "[0m 12s] Epoch 51 [5120/8158] loss=0.0019258125452324749\n",
            "[0m 12s] Epoch 51 [7680/8158] loss=0.0019482563288571934\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7947787098000892\n",
            "[0m 12s] Epoch 52 [2560/8158] loss=0.0019339037360623478\n",
            "[0m 12s] Epoch 52 [5120/8158] loss=0.0019484789576381446\n",
            "[0m 12s] Epoch 52 [7680/8158] loss=0.0019522148184478282\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7945974150014813\n",
            "[0m 12s] Epoch 53 [2560/8158] loss=0.0018855743226595223\n",
            "[0m 12s] Epoch 53 [5120/8158] loss=0.0019309446797706187\n",
            "[0m 12s] Epoch 53 [7680/8158] loss=0.0019495331255408625\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7950136981801667\n",
            "[0m 12s] Epoch 54 [2560/8158] loss=0.0019124730839394032\n",
            "[0m 12s] Epoch 54 [5120/8158] loss=0.001945563405752182\n",
            "[0m 13s] Epoch 54 [7680/8158] loss=0.0019559104228392245\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7952581618981502\n",
            "[0m 13s] Epoch 55 [2560/8158] loss=0.0019113711430691182\n",
            "[0m 13s] Epoch 55 [5120/8158] loss=0.0019246109353844077\n",
            "[0m 13s] Epoch 55 [7680/8158] loss=0.0019495886711714168\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7955083108188776\n",
            "[0m 13s] Epoch 56 [2560/8158] loss=0.0019570492790080608\n",
            "[0m 13s] Epoch 56 [5120/8158] loss=0.0019469226652290672\n",
            "[0m 13s] Epoch 56 [7680/8158] loss=0.001960893825162202\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7953068019660695\n",
            "[0m 13s] Epoch 57 [2560/8158] loss=0.001957176229916513\n",
            "[0m 13s] Epoch 57 [5120/8158] loss=0.0019241628586314618\n",
            "[0m 13s] Epoch 57 [7680/8158] loss=0.0019436215283349157\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7949757968285411\n",
            "[0m 13s] Epoch 58 [2560/8158] loss=0.001935893902555108\n",
            "[0m 13s] Epoch 58 [5120/8158] loss=0.0019491359242238104\n",
            "[0m 13s] Epoch 58 [7680/8158] loss=0.0019532076044318576\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dec set: AUC  0.7954779897375773\n",
            "[0m 14s] Epoch 59 [2560/8158] loss=0.00196509282104671\n",
            "[0m 14s] Epoch 59 [5120/8158] loss=0.0019507343065924942\n",
            "[0m 14s] Epoch 59 [7680/8158] loss=0.0019517738954164087\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.794839036118093\n",
            "[0m 14s] Epoch 60 [2560/8158] loss=0.0019497880013659597\n",
            "[0m 14s] Epoch 60 [5120/8158] loss=0.0019610078539699315\n",
            "[0m 14s] Epoch 60 [7680/8158] loss=0.001948781361958633\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7948185062192958\n",
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.002516050264239311\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.002461125992704183\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.0023983355533952516\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1760/2658 66.22%\n",
            "Dec set: AUC  0.7653015652626531\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.0021346245426684617\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.0021209569647908212\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.002100504030628751\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1947/2658 73.25%\n",
            "Dec set: AUC  0.7836717187062753\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.002044887258671224\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.0020352520514279604\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.0020233835792168977\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1972/2658 74.19%\n",
            "Dec set: AUC  0.7867297260932486\n",
            "[0m 0s] Epoch 4 [2560/8158] loss=0.0020299018826335667\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.0019971328671090304\n",
            "[0m 0s] Epoch 4 [7680/8158] loss=0.0020142551977187393\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7889400065822014\n",
            "[0m 0s] Epoch 5 [2560/8158] loss=0.0020255335723049937\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.002019790967460722\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.0020144599839113653\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1931/2658 72.65%\n",
            "Dec set: AUC  0.7889488502309139\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.0020069222431629897\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.0020048008474987\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.0020136140403337775\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.788923582663164\n",
            "[0m 1s] Epoch 7 [2560/8158] loss=0.001979499403387308\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.0019914745003916322\n",
            "[0m 1s] Epoch 7 [7680/8158] loss=0.0019906932565694055\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.789215423070679\n",
            "[0m 1s] Epoch 8 [2560/8158] loss=0.0019834782695397736\n",
            "[0m 1s] Epoch 8 [5120/8158] loss=0.001978545036399737\n",
            "[0m 1s] Epoch 8 [7680/8158] loss=0.0019846422558960817\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7892823821252171\n",
            "[0m 2s] Epoch 9 [2560/8158] loss=0.001973942678887397\n",
            "[0m 2s] Epoch 9 [5120/8158] loss=0.0019593166012782603\n",
            "[0m 2s] Epoch 9 [7680/8158] loss=0.0019851397761764624\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7894163002342935\n",
            "[0m 2s] Epoch 10 [2560/8158] loss=0.0020442124805413187\n",
            "[0m 2s] Epoch 10 [5120/8158] loss=0.0020030678948387503\n",
            "[0m 2s] Epoch 10 [7680/8158] loss=0.001983378295941899\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7896942434795461\n",
            "[0m 2s] Epoch 11 [2560/8158] loss=0.001978947245515883\n",
            "[0m 2s] Epoch 11 [5120/8158] loss=0.0020080881658941507\n",
            "[0m 2s] Epoch 11 [7680/8158] loss=0.001987304200883955\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7898003672640972\n",
            "[0m 2s] Epoch 12 [2560/8158] loss=0.0019871094613336028\n",
            "[0m 2s] Epoch 12 [5120/8158] loss=0.0019799512985628097\n",
            "[0m 2s] Epoch 12 [7680/8158] loss=0.001992069250748803\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7901572716585696\n",
            "[0m 2s] Epoch 13 [2560/8158] loss=0.001996306038927287\n",
            "[0m 3s] Epoch 13 [5120/8158] loss=0.001978049625176936\n",
            "[0m 3s] Epoch 13 [7680/8158] loss=0.001987518505969395\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7904554289580223\n",
            "[0m 3s] Epoch 14 [2560/8158] loss=0.001956294302362949\n",
            "[0m 3s] Epoch 14 [5120/8158] loss=0.0019900570390746\n",
            "[0m 3s] Epoch 14 [7680/8158] loss=0.0019788541093779108\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.790744110919569\n",
            "[0m 3s] Epoch 15 [2560/8158] loss=0.002016326517332345\n",
            "[0m 3s] Epoch 15 [5120/8158] loss=0.0019895447127055376\n",
            "[0m 3s] Epoch 15 [7680/8158] loss=0.001968562901796152\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7911692377469667\n",
            "[0m 3s] Epoch 16 [2560/8158] loss=0.002027154213283211\n",
            "[0m 3s] Epoch 16 [5120/8158] loss=0.0019904036773368715\n",
            "[0m 3s] Epoch 16 [7680/8158] loss=0.00198229281231761\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7915109816007888\n",
            "[0m 4s] Epoch 17 [2560/8158] loss=0.001968672580551356\n",
            "[0m 4s] Epoch 17 [5120/8158] loss=0.0020016783964820206\n",
            "[0m 4s] Epoch 17 [7680/8158] loss=0.00197417657279099\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.79187609795478\n",
            "[0m 4s] Epoch 18 [2560/8158] loss=0.0019239306217059492\n",
            "[0m 4s] Epoch 18 [5120/8158] loss=0.001973959384486079\n",
            "[0m 4s] Epoch 18 [7680/8158] loss=0.0019771567429415883\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7922829057955588\n",
            "[0m 4s] Epoch 19 [2560/8158] loss=0.0019879211438819768\n",
            "[0m 4s] Epoch 19 [5120/8158] loss=0.0019672916329000144\n",
            "[0m 4s] Epoch 19 [7680/8158] loss=0.001966269574283312\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7925235793783799\n",
            "[0m 4s] Epoch 20 [2560/8158] loss=0.0019001496490091085\n",
            "[0m 4s] Epoch 20 [5120/8158] loss=0.0019246174430008977\n",
            "[0m 4s] Epoch 20 [7680/8158] loss=0.0019672193525669475\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7925646391759741\n",
            "[0m 4s] Epoch 21 [2560/8158] loss=0.001894906780216843\n",
            "[0m 5s] Epoch 21 [5120/8158] loss=0.001952573552262038\n",
            "[0m 5s] Epoch 21 [7680/8158] loss=0.0019786605262197554\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7922228953221521\n",
            "[0m 5s] Epoch 22 [2560/8158] loss=0.001945049362257123\n",
            "[0m 5s] Epoch 22 [5120/8158] loss=0.001965264370664954\n",
            "[0m 5s] Epoch 22 [7680/8158] loss=0.001968871623588105\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.793077570801304\n",
            "[0m 5s] Epoch 23 [2560/8158] loss=0.001985971012618393\n",
            "[0m 5s] Epoch 23 [5120/8158] loss=0.00197304401663132\n",
            "[0m 5s] Epoch 23 [7680/8158] loss=0.001970541935103635\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7934016273577009\n",
            "[0m 5s] Epoch 24 [2560/8158] loss=0.0019326978013850748\n",
            "[0m 5s] Epoch 24 [5120/8158] loss=0.0019317669968586415\n",
            "[0m 6s] Epoch 24 [7680/8158] loss=0.0019692355146010716\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7936543030352033\n",
            "[0m 6s] Epoch 25 [2560/8158] loss=0.0019350287853740155\n",
            "[0m 6s] Epoch 25 [5120/8158] loss=0.0019457234360743315\n",
            "[0m 6s] Epoch 25 [7680/8158] loss=0.0019708318286575377\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1979/2658 74.45%\n",
            "Dec set: AUC  0.7937673754008857\n",
            "[0m 6s] Epoch 26 [2560/8158] loss=0.001990189263597131\n",
            "[0m 6s] Epoch 26 [5120/8158] loss=0.0019837401108816267\n",
            "[0m 6s] Epoch 26 [7680/8158] loss=0.001972268409250925\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7938640238475303\n",
            "[0m 6s] Epoch 27 [2560/8158] loss=0.0019443034077994525\n",
            "[0m 6s] Epoch 27 [5120/8158] loss=0.0019322203879710287\n",
            "[0m 6s] Epoch 27 [7680/8158] loss=0.0019671297166496517\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.7938577069555929\n",
            "[0m 6s] Epoch 28 [2560/8158] loss=0.0020237187738530336\n",
            "[0m 6s] Epoch 28 [5120/8158] loss=0.0019523089111316949\n",
            "[0m 6s] Epoch 28 [7680/8158] loss=0.0019732445478439333\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7936663051298847\n",
            "[0m 7s] Epoch 29 [2560/8158] loss=0.001945597678422928\n",
            "[0m 7s] Epoch 29 [5120/8158] loss=0.0019386179395951332\n",
            "[0m 7s] Epoch 29 [7680/8158] loss=0.0019695652066729964\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7939767803686159\n",
            "[0m 7s] Epoch 30 [2560/8158] loss=0.0019901601015590133\n",
            "[0m 7s] Epoch 30 [5120/8158] loss=0.0019543511676602065\n",
            "[0m 7s] Epoch 30 [7680/8158] loss=0.0019578912877477707\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7942168222622432\n",
            "[0m 7s] Epoch 31 [2560/8158] loss=0.00193417074624449\n",
            "[0m 7s] Epoch 31 [5120/8158] loss=0.001972484984435141\n",
            "[0m 7s] Epoch 31 [7680/8158] loss=0.00196444905983905\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dec set: AUC  0.7936909410084413\n",
            "[0m 7s] Epoch 32 [2560/8158] loss=0.0019832532736472784\n",
            "[0m 7s] Epoch 32 [5120/8158] loss=0.0019621646555606277\n",
            "[0m 8s] Epoch 32 [7680/8158] loss=0.001957757247146219\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.794304942904772\n",
            "[0m 8s] Epoch 33 [2560/8158] loss=0.0019271298544481397\n",
            "[0m 8s] Epoch 33 [5120/8158] loss=0.0019200677750632166\n",
            "[0m 8s] Epoch 33 [7680/8158] loss=0.001956686299915115\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7947578640566955\n",
            "[0m 8s] Epoch 34 [2560/8158] loss=0.001955350418575108\n",
            "[0m 8s] Epoch 34 [5120/8158] loss=0.001972037076484412\n",
            "[0m 8s] Epoch 34 [7680/8158] loss=0.001957860100083053\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7944357025678798\n",
            "[0m 8s] Epoch 35 [2560/8158] loss=0.0019105525338090957\n",
            "[0m 8s] Epoch 35 [5120/8158] loss=0.0019311393843963742\n",
            "[0m 8s] Epoch 35 [7680/8158] loss=0.001953800298118343\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7947073289211949\n",
            "[0m 8s] Epoch 36 [2560/8158] loss=0.0019273379119113087\n",
            "[0m 8s] Epoch 36 [5120/8158] loss=0.0019199359114281834\n",
            "[0m 8s] Epoch 36 [7680/8158] loss=0.001953936379868537\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7948254548004272\n",
            "[0m 9s] Epoch 37 [2560/8158] loss=0.0019858346320688726\n",
            "[0m 9s] Epoch 37 [5120/8158] loss=0.001965916674816981\n",
            "[0m 9s] Epoch 37 [7680/8158] loss=0.0019629212833630542\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7947104873671637\n",
            "[0m 9s] Epoch 38 [2560/8158] loss=0.0019703511614352466\n",
            "[0m 9s] Epoch 38 [5120/8158] loss=0.0019618505437392743\n",
            "[0m 9s] Epoch 38 [7680/8158] loss=0.001965059976403912\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7943346322968787\n",
            "[0m 9s] Epoch 39 [2560/8158] loss=0.0019608627422712742\n",
            "[0m 9s] Epoch 39 [5120/8158] loss=0.0019462397205643356\n",
            "[0m 9s] Epoch 39 [7680/8158] loss=0.0019536327531871696\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.794948002504016\n",
            "[0m 9s] Epoch 40 [2560/8158] loss=0.0019546836614608766\n",
            "[0m 9s] Epoch 40 [5120/8158] loss=0.0019424392492510378\n",
            "[0m 10s] Epoch 40 [7680/8158] loss=0.0019450565140383938\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.7948267181788149\n",
            "[0m 10s] Epoch 41 [2560/8158] loss=0.001974746270570904\n",
            "[0m 10s] Epoch 41 [5120/8158] loss=0.001971152366604656\n",
            "[0m 10s] Epoch 41 [7680/8158] loss=0.001952437439467758\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dec set: AUC  0.7948058724354209\n",
            "[0m 10s] Epoch 42 [2560/8158] loss=0.0019745740806683897\n",
            "[0m 10s] Epoch 42 [5120/8158] loss=0.0019420771102886647\n",
            "[0m 10s] Epoch 42 [7680/8158] loss=0.001943004069228967\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7950443351060638\n",
            "[0m 10s] Epoch 43 [2560/8158] loss=0.001969765208195895\n",
            "[0m 10s] Epoch 43 [5120/8158] loss=0.00194945857510902\n",
            "[0m 10s] Epoch 43 [7680/8158] loss=0.0019456749859576425\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7947783939554924\n",
            "[0m 10s] Epoch 44 [2560/8158] loss=0.0019854142097756266\n",
            "[0m 10s] Epoch 44 [5120/8158] loss=0.0019797213259153067\n",
            "[0m 10s] Epoch 44 [7680/8158] loss=0.00195260817805926\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2010/2658 75.62%\n",
            "Dec set: AUC  0.7950244368964605\n",
            "[0m 11s] Epoch 45 [2560/8158] loss=0.001972922636196017\n",
            "[0m 11s] Epoch 45 [5120/8158] loss=0.0019726087513845412\n",
            "[0m 11s] Epoch 45 [7680/8158] loss=0.0019560690658787885\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7952827977767067\n",
            "[0m 11s] Epoch 46 [2560/8158] loss=0.0020297356648370625\n",
            "[0m 11s] Epoch 46 [5120/8158] loss=0.0019569788593798874\n",
            "[0m 11s] Epoch 46 [7680/8158] loss=0.001954028002607326\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dec set: AUC  0.7953143822363945\n",
            "[0m 11s] Epoch 47 [2560/8158] loss=0.0019730605999939144\n",
            "[0m 11s] Epoch 47 [5120/8158] loss=0.0019479447102639825\n",
            "[0m 11s] Epoch 47 [7680/8158] loss=0.001958444396344324\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7952865879118691\n",
            "[0m 11s] Epoch 48 [2560/8158] loss=0.0019354093121364713\n",
            "[0m 11s] Epoch 48 [5120/8158] loss=0.0019491205108352005\n",
            "[0m 11s] Epoch 48 [7680/8158] loss=0.0019521109061315656\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7951400360189178\n",
            "[0m 11s] Epoch 49 [2560/8158] loss=0.0019280562410131097\n",
            "[0m 12s] Epoch 49 [5120/8158] loss=0.0019521453708875925\n",
            "[0m 12s] Epoch 49 [7680/8158] loss=0.0019500386551953852\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.795284692844288\n",
            "[0m 12s] Epoch 50 [2560/8158] loss=0.0019556385930627586\n",
            "[0m 12s] Epoch 50 [5120/8158] loss=0.001945501088630408\n",
            "[0m 12s] Epoch 50 [7680/8158] loss=0.001955501319995771\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7953952384531954\n",
            "[0m 12s] Epoch 51 [2560/8158] loss=0.0018929470446892084\n",
            "[0m 12s] Epoch 51 [5120/8158] loss=0.0019342470972333104\n",
            "[0m 12s] Epoch 51 [7680/8158] loss=0.001950489644271632\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7954729362240273\n",
            "[0m 12s] Epoch 52 [2560/8158] loss=0.001987697195727378\n",
            "[0m 12s] Epoch 52 [5120/8158] loss=0.0019792292034253476\n",
            "[0m 12s] Epoch 52 [7680/8158] loss=0.0019530132218884926\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7952575302089565\n",
            "[0m 12s] Epoch 53 [2560/8158] loss=0.001993100333493203\n",
            "[0m 12s] Epoch 53 [5120/8158] loss=0.0019513115112204105\n",
            "[0m 13s] Epoch 53 [7680/8158] loss=0.0019564207488050062\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7955259981163028\n",
            "[0m 13s] Epoch 54 [2560/8158] loss=0.001945996016729623\n",
            "[0m 13s] Epoch 54 [5120/8158] loss=0.001957966451300308\n",
            "[0m 13s] Epoch 54 [7680/8158] loss=0.00194533991937836\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7945020299332242\n",
            "[0m 13s] Epoch 55 [2560/8158] loss=0.0019693647976964713\n",
            "[0m 13s] Epoch 55 [5120/8158] loss=0.0019708464737050236\n",
            "[0m 13s] Epoch 55 [7680/8158] loss=0.0019530726053441565\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7957104513608796\n",
            "[0m 13s] Epoch 56 [2560/8158] loss=0.0019931221613660455\n",
            "[0m 13s] Epoch 56 [5120/8158] loss=0.0019209952268283814\n",
            "[0m 13s] Epoch 56 [7680/8158] loss=0.0019428560820718606\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7957230851447548\n",
            "[0m 13s] Epoch 57 [2560/8158] loss=0.0019854106358252466\n",
            "[0m 13s] Epoch 57 [5120/8158] loss=0.0019505742413457483\n",
            "[0m 13s] Epoch 57 [7680/8158] loss=0.0019554013192343215\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7951267705458489\n",
            "[0m 14s] Epoch 58 [2560/8158] loss=0.0020188264083117247\n",
            "[0m 14s] Epoch 58 [5120/8158] loss=0.0019559063075575978\n",
            "[0m 14s] Epoch 58 [7680/8158] loss=0.0019354787965615591\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.795318172371557\n",
            "[0m 14s] Epoch 59 [2560/8158] loss=0.0019085770356468857\n",
            "[0m 14s] Epoch 59 [5120/8158] loss=0.0019309754541609436\n",
            "[0m 14s] Epoch 59 [7680/8158] loss=0.001946643996052444\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dec set: AUC  0.7956927640634545\n",
            "[0m 14s] Epoch 60 [2560/8158] loss=0.0019967893487773834\n",
            "[0m 14s] Epoch 60 [5120/8158] loss=0.0019809786754194647\n",
            "[0m 14s] Epoch 60 [7680/8158] loss=0.0019552601889396707\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dec set: AUC  0.7951722521677993\n",
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.0024784615263342856\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.0023938684607855976\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.002314569583783547\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1883/2658 70.84%\n",
            "Dec set: AUC  0.7670601879780703\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.0021037536207586527\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.0020930047729052605\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.002074283497252812\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1926/2658 72.46%\n",
            "Dec set: AUC  0.788785242729731\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.001968765666242689\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.0020126693416386843\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.0020151496554414432\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1971/2658 74.15%\n",
            "Dec set: AUC  0.7868112139992433\n",
            "[0m 0s] Epoch 4 [2560/8158] loss=0.0019987433101050557\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.00200876435264945\n",
            "[0m 0s] Epoch 4 [7680/8158] loss=0.0020096968587798377\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1977/2658 74.38%\n",
            "Dec set: AUC  0.788982329758183\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.001958203257527202\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.00199728780426085\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.0019981282607962688\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.788780189216181\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.0020048227626830338\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.001992852095281705\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.0019965216245812676\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7889772762446331\n",
            "[0m 1s] Epoch 7 [2560/8158] loss=0.0019913057796657084\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.0019777131325099616\n",
            "[0m 1s] Epoch 7 [7680/8158] loss=0.0019818877684883774\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7894346192209126\n",
            "[0m 1s] Epoch 8 [2560/8158] loss=0.002003665326628834\n",
            "[0m 1s] Epoch 8 [5120/8158] loss=0.0019884672365151346\n",
            "[0m 1s] Epoch 8 [7680/8158] loss=0.0019970265682786705\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1965/2658 73.93%\n",
            "Dec set: AUC  0.7896999286822901\n",
            "[0m 2s] Epoch 9 [2560/8158] loss=0.002006386942230165\n",
            "[0m 2s] Epoch 9 [5120/8158] loss=0.001997664454393089\n",
            "[0m 2s] Epoch 9 [7680/8158] loss=0.0019974591365704932\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7902343377402077\n",
            "[0m 2s] Epoch 10 [2560/8158] loss=0.0020163395791314544\n",
            "[0m 2s] Epoch 10 [5120/8158] loss=0.001972625299822539\n",
            "[0m 2s] Epoch 10 [7680/8158] loss=0.001982702435149501\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7903613072681527\n",
            "[0m 2s] Epoch 11 [2560/8158] loss=0.001957692054565996\n",
            "[0m 2s] Epoch 11 [5120/8158] loss=0.0019930856535211204\n",
            "[0m 2s] Epoch 11 [7680/8158] loss=0.0019800851199155054\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7904788014581913\n",
            "[0m 2s] Epoch 12 [2560/8158] loss=0.0019711820757947864\n",
            "[0m 2s] Epoch 12 [5120/8158] loss=0.001967531471746042\n",
            "[0m 2s] Epoch 12 [7680/8158] loss=0.0019797939069879553\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7907415841627939\n",
            "[0m 2s] Epoch 13 [2560/8158] loss=0.0019603578024543823\n",
            "[0m 3s] Epoch 13 [5120/8158] loss=0.001980369503144175\n",
            "[0m 3s] Epoch 13 [7680/8158] loss=0.001979844163482388\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7913931715661533\n",
            "[0m 3s] Epoch 14 [2560/8158] loss=0.0020021517178975047\n",
            "[0m 3s] Epoch 14 [5120/8158] loss=0.0019780629547312854\n",
            "[0m 3s] Epoch 14 [7680/8158] loss=0.001967934538455059\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dec set: AUC  0.7917162805887595\n",
            "[0m 3s] Epoch 15 [2560/8158] loss=0.0019075774936936796\n",
            "[0m 3s] Epoch 15 [5120/8158] loss=0.0019578485807869583\n",
            "[0m 3s] Epoch 15 [7680/8158] loss=0.0019725874182768164\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7922468995115146\n",
            "[0m 3s] Epoch 16 [2560/8158] loss=0.0019927646266296507\n",
            "[0m 3s] Epoch 16 [5120/8158] loss=0.0019808190176263452\n",
            "[0m 3s] Epoch 16 [7680/8158] loss=0.001982448366470635\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7925614807300054\n",
            "[0m 3s] Epoch 17 [2560/8158] loss=0.0018797843251377345\n",
            "[0m 3s] Epoch 17 [5120/8158] loss=0.0019513334787916391\n",
            "[0m 4s] Epoch 17 [7680/8158] loss=0.0019638665098076066\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7921054011321134\n",
            "[0m 4s] Epoch 18 [2560/8158] loss=0.001931893778964877\n",
            "[0m 4s] Epoch 18 [5120/8158] loss=0.001962303672917187\n",
            "[0m 4s] Epoch 18 [7680/8158] loss=0.001974201361493518\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7928078395155703\n",
            "[0m 4s] Epoch 19 [2560/8158] loss=0.0020020657568238674\n",
            "[0m 4s] Epoch 19 [5120/8158] loss=0.0019801580638159066\n",
            "[0m 4s] Epoch 19 [7680/8158] loss=0.0019824676720115045\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7928868006647896\n",
            "[0m 4s] Epoch 20 [2560/8158] loss=0.001957106194458902\n",
            "[0m 4s] Epoch 20 [5120/8158] loss=0.001945998682640493\n",
            "[0m 4s] Epoch 20 [7680/8158] loss=0.0019628879497759046\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.792994819516922\n",
            "[0m 4s] Epoch 21 [2560/8158] loss=0.0019914130913093685\n",
            "[0m 4s] Epoch 21 [5120/8158] loss=0.001981485862052068\n",
            "[0m 5s] Epoch 21 [7680/8158] loss=0.0019632973164940876\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7928792203944647\n",
            "[0m 5s] Epoch 22 [2560/8158] loss=0.0019718428957276046\n",
            "[0m 5s] Epoch 22 [5120/8158] loss=0.0019794552645180374\n",
            "[0m 5s] Epoch 22 [7680/8158] loss=0.0019720262615010144\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7940756397274387\n",
            "[0m 5s] Epoch 23 [2560/8158] loss=0.0019863360561430455\n",
            "[0m 5s] Epoch 23 [5120/8158] loss=0.0019580558873713017\n",
            "[0m 5s] Epoch 23 [7680/8158] loss=0.0019659433940735957\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7941148044574515\n",
            "[0m 5s] Epoch 24 [2560/8158] loss=0.0019452730775810778\n",
            "[0m 5s] Epoch 24 [5120/8158] loss=0.001934179477393627\n",
            "[0m 5s] Epoch 24 [7680/8158] loss=0.0019607420195825397\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7937844310091171\n",
            "[0m 6s] Epoch 25 [2560/8158] loss=0.0019962391816079617\n",
            "[0m 6s] Epoch 25 [5120/8158] loss=0.0019706106162630022\n",
            "[0m 6s] Epoch 25 [7680/8158] loss=0.001964985470597943\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7936454593864908\n",
            "[0m 6s] Epoch 26 [2560/8158] loss=0.0019647869514301418\n",
            "[0m 6s] Epoch 26 [5120/8158] loss=0.0019425306527409703\n",
            "[0m 6s] Epoch 26 [7680/8158] loss=0.001963371321714173\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7938349661446176\n",
            "[0m 6s] Epoch 27 [2560/8158] loss=0.0019019754021428526\n",
            "[0m 6s] Epoch 27 [5120/8158] loss=0.0019371603790204971\n",
            "[0m 6s] Epoch 27 [7680/8158] loss=0.0019404319968695442\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7942386155394278\n",
            "[0m 6s] Epoch 28 [2560/8158] loss=0.001936781161930412\n",
            "[0m 7s] Epoch 28 [5120/8158] loss=0.0019564833433832972\n",
            "[0m 7s] Epoch 28 [7680/8158] loss=0.001963582553435117\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1977/2658 74.38%\n",
            "Dec set: AUC  0.7927882571505638\n",
            "[0m 7s] Epoch 29 [2560/8158] loss=0.0019973346614278855\n",
            "[0m 7s] Epoch 29 [5120/8158] loss=0.0019664050196297467\n",
            "[0m 7s] Epoch 29 [7680/8158] loss=0.0019695879193022845\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7946669008127945\n",
            "[0m 7s] Epoch 30 [2560/8158] loss=0.0019448918290436267\n",
            "[0m 7s] Epoch 30 [5120/8158] loss=0.0019676354073453694\n",
            "[0m 7s] Epoch 30 [7680/8158] loss=0.0019564279975990456\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7939745694564379\n",
            "[0m 7s] Epoch 31 [2560/8158] loss=0.0019753900240175428\n",
            "[0m 7s] Epoch 31 [5120/8158] loss=0.0019872179720550774\n",
            "[0m 7s] Epoch 31 [7680/8158] loss=0.0019471018109470605\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7938033816849298\n",
            "[0m 8s] Epoch 32 [2560/8158] loss=0.001988238608464599\n",
            "[0m 8s] Epoch 32 [5120/8158] loss=0.0019741166732273998\n",
            "[0m 8s] Epoch 32 [7680/8158] loss=0.0019634294401233393\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dec set: AUC  0.7942986260128347\n",
            "[0m 8s] Epoch 33 [2560/8158] loss=0.0019980598357506097\n",
            "[0m 8s] Epoch 33 [5120/8158] loss=0.0019756303168833255\n",
            "[0m 8s] Epoch 33 [7680/8158] loss=0.0019559095536048213\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7948412470302713\n",
            "[0m 8s] Epoch 34 [2560/8158] loss=0.001940017519518733\n",
            "[0m 8s] Epoch 34 [5120/8158] loss=0.0019555525737814604\n",
            "[0m 8s] Epoch 34 [7680/8158] loss=0.0019502890529111028\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dec set: AUC  0.7944748672978927\n",
            "[0m 8s] Epoch 35 [2560/8158] loss=0.0019751718849875034\n",
            "[0m 8s] Epoch 35 [5120/8158] loss=0.001951548212673515\n",
            "[0m 8s] Epoch 35 [7680/8158] loss=0.0019539280755755803\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7949113645307782\n",
            "[0m 9s] Epoch 36 [2560/8158] loss=0.0019589400617405772\n",
            "[0m 9s] Epoch 36 [5120/8158] loss=0.0019429356092587114\n",
            "[0m 9s] Epoch 36 [7680/8158] loss=0.001955456950236112\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dec set: AUC  0.7944849743249927\n",
            "[0m 9s] Epoch 37 [2560/8158] loss=0.0019120998098514975\n",
            "[0m 9s] Epoch 37 [5120/8158] loss=0.0019274069753009826\n",
            "[0m 9s] Epoch 37 [7680/8158] loss=0.001961362517128388\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7947193310158763\n",
            "[0m 9s] Epoch 38 [2560/8158] loss=0.002003589062951505\n",
            "[0m 9s] Epoch 38 [5120/8158] loss=0.0019755974761210384\n",
            "[0m 9s] Epoch 38 [7680/8158] loss=0.001948934382138153\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2013/2658 75.73%\n",
            "Dec set: AUC  0.7947351232457202\n",
            "[0m 9s] Epoch 39 [2560/8158] loss=0.0019325644010677935\n",
            "[0m 9s] Epoch 39 [5120/8158] loss=0.0019449579471256585\n",
            "[0m 9s] Epoch 39 [7680/8158] loss=0.0019557683068948486\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7948949406117405\n",
            "[0m 9s] Epoch 40 [2560/8158] loss=0.0019241717411205173\n",
            "[0m 10s] Epoch 40 [5120/8158] loss=0.0019445849291514605\n",
            "[0m 10s] Epoch 40 [7680/8158] loss=0.0019480420004886886\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7949031525712592\n",
            "[0m 10s] Epoch 41 [2560/8158] loss=0.0018841041484847665\n",
            "[0m 10s] Epoch 41 [5120/8158] loss=0.001944846409605816\n",
            "[0m 10s] Epoch 41 [7680/8158] loss=0.0019526303901026647\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7945525650687245\n",
            "[0m 10s] Epoch 42 [2560/8158] loss=0.0019216833868995308\n",
            "[0m 10s] Epoch 42 [5120/8158] loss=0.0019297211780212819\n",
            "[0m 10s] Epoch 42 [7680/8158] loss=0.0019448058757310113\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7953371230473698\n",
            "[0m 10s] Epoch 43 [2560/8158] loss=0.001948645757511258\n",
            "[0m 10s] Epoch 43 [5120/8158] loss=0.0019404980062972753\n",
            "[0m 10s] Epoch 43 [7680/8158] loss=0.001953998200284938\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7952783759523506\n",
            "[0m 10s] Epoch 44 [2560/8158] loss=0.001925361144822091\n",
            "[0m 10s] Epoch 44 [5120/8158] loss=0.001924114063149318\n",
            "[0m 11s] Epoch 44 [7680/8158] loss=0.001948496598439912\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7925608490408115\n",
            "[0m 11s] Epoch 45 [2560/8158] loss=0.0019325563218444586\n",
            "[0m 11s] Epoch 45 [5120/8158] loss=0.001938719383906573\n",
            "[0m 11s] Epoch 45 [7680/8158] loss=0.0019448220652217667\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7946466867585943\n",
            "[0m 11s] Epoch 46 [2560/8158] loss=0.0019915578537620603\n",
            "[0m 11s] Epoch 46 [5120/8158] loss=0.0019637768738903104\n",
            "[0m 11s] Epoch 46 [7680/8158] loss=0.001948578159014384\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7953390181149511\n",
            "[0m 11s] Epoch 47 [2560/8158] loss=0.0019242303678765893\n",
            "[0m 11s] Epoch 47 [5120/8158] loss=0.0019269991666078568\n",
            "[0m 11s] Epoch 47 [7680/8158] loss=0.0019505620002746582\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.7951627768298931\n",
            "[0m 11s] Epoch 48 [2560/8158] loss=0.0019434530986472964\n",
            "[0m 11s] Epoch 48 [5120/8158] loss=0.0019477516005281359\n",
            "[0m 11s] Epoch 48 [7680/8158] loss=0.001939260761719197\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2012/2658 75.70%\n",
            "Dec set: AUC  0.7935292285748397\n",
            "[0m 12s] Epoch 49 [2560/8158] loss=0.0019519078661687673\n",
            "[0m 12s] Epoch 49 [5120/8158] loss=0.0019387223292142153\n",
            "[0m 12s] Epoch 49 [7680/8158] loss=0.0019417451112531126\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7947799731784768\n",
            "[0m 12s] Epoch 50 [2560/8158] loss=0.001968454278539866\n",
            "[0m 12s] Epoch 50 [5120/8158] loss=0.0019479225622490049\n",
            "[0m 12s] Epoch 50 [7680/8158] loss=0.001950820356917878\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7952818502429162\n",
            "[0m 12s] Epoch 51 [2560/8158] loss=0.0019393578753806652\n",
            "[0m 12s] Epoch 51 [5120/8158] loss=0.001959458360215649\n",
            "[0m 12s] Epoch 51 [7680/8158] loss=0.0019461709423922002\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7947471253404016\n",
            "[0m 12s] Epoch 52 [2560/8158] loss=0.001944287249352783\n",
            "[0m 13s] Epoch 52 [5120/8158] loss=0.0019384905346669257\n",
            "[0m 13s] Epoch 52 [7680/8158] loss=0.0019355302676558495\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dec set: AUC  0.7946018368258376\n",
            "[0m 13s] Epoch 53 [2560/8158] loss=0.002002569066826254\n",
            "[0m 13s] Epoch 53 [5120/8158] loss=0.0019302413740660996\n",
            "[0m 13s] Epoch 53 [7680/8158] loss=0.0019343116126644114\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2009/2658 75.58%\n",
            "Dec set: AUC  0.7946814296642509\n",
            "[0m 13s] Epoch 54 [2560/8158] loss=0.0018759677768684924\n",
            "[0m 13s] Epoch 54 [5120/8158] loss=0.0019524910487234593\n",
            "[0m 13s] Epoch 54 [7680/8158] loss=0.0019422412811157604\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7947856583812207\n",
            "[0m 13s] Epoch 55 [2560/8158] loss=0.0018944747396744787\n",
            "[0m 13s] Epoch 55 [5120/8158] loss=0.0019422899174969643\n",
            "[0m 13s] Epoch 55 [7680/8158] loss=0.00194699497660622\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7936227185755156\n",
            "[0m 13s] Epoch 56 [2560/8158] loss=0.0019270380842499435\n",
            "[0m 13s] Epoch 56 [5120/8158] loss=0.0019701202400028706\n",
            "[0m 14s] Epoch 56 [7680/8158] loss=0.0019424817059189082\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.7946858514886072\n",
            "[0m 14s] Epoch 57 [2560/8158] loss=0.0018849618034437298\n",
            "[0m 14s] Epoch 57 [5120/8158] loss=0.0019242130743805318\n",
            "[0m 14s] Epoch 57 [7680/8158] loss=0.0019470413060237964\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2010/2658 75.62%\n",
            "Dec set: AUC  0.7952158387221685\n",
            "[0m 14s] Epoch 58 [2560/8158] loss=0.0019472451647743582\n",
            "[0m 14s] Epoch 58 [5120/8158] loss=0.0019467135716695338\n",
            "[0m 14s] Epoch 58 [7680/8158] loss=0.001937450918679436\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7941640762145645\n",
            "[0m 14s] Epoch 59 [2560/8158] loss=0.0019736518850550057\n",
            "[0m 14s] Epoch 59 [5120/8158] loss=0.0019309950177557767\n",
            "[0m 14s] Epoch 59 [7680/8158] loss=0.001931470655836165\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7945115052711305\n",
            "[0m 15s] Epoch 60 [2560/8158] loss=0.0019290434080176055\n",
            "[0m 15s] Epoch 60 [5120/8158] loss=0.0019287338946014643\n",
            "[0m 15s] Epoch 60 [7680/8158] loss=0.001932567257123689\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7937680070900796\n",
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.0024221653351560236\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.0023401652462780474\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.002253575646318495\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1951/2658 73.40%\n",
            "Dec set: AUC  0.7704908919893598\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.0020732267876155674\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.002038294298108667\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.0020421764076066513\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1918/2658 72.16%\n",
            "Dec set: AUC  0.7867688908232616\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.0020207689376547933\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.0020333184918854387\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.002017591765616089\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dec set: AUC  0.7883001054289266\n",
            "[0m 1s] Epoch 4 [2560/8158] loss=0.0020488395122811196\n",
            "[0m 1s] Epoch 4 [5120/8158] loss=0.002028273738687858\n",
            "[0m 1s] Epoch 4 [7680/8158] loss=0.002004636071311931\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dec set: AUC  0.7885692050254667\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.0019456765614449978\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.0019908457703422754\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.0019951986459394297\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1962/2658 73.81%\n",
            "Dec set: AUC  0.7889785396230207\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.002059215132612735\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.002021187054924667\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.001999190061663588\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1979/2658 74.45%\n",
            "Dec set: AUC  0.7894042981396121\n",
            "[0m 2s] Epoch 7 [2560/8158] loss=0.0019928426248952747\n",
            "[0m 2s] Epoch 7 [5120/8158] loss=0.0020135508209932597\n",
            "[0m 2s] Epoch 7 [7680/8158] loss=0.0019907032100794217\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.789697401925515\n",
            "[0m 2s] Epoch 8 [2560/8158] loss=0.0019642383907921613\n",
            "[0m 2s] Epoch 8 [5120/8158] loss=0.001994246384128928\n",
            "[0m 2s] Epoch 8 [7680/8158] loss=0.0019869407871738074\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7897176159797152\n",
            "[0m 2s] Epoch 9 [2560/8158] loss=0.002013049251399934\n",
            "[0m 2s] Epoch 9 [5120/8158] loss=0.0019955791358370334\n",
            "[0m 2s] Epoch 9 [7680/8158] loss=0.0019831813173368574\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1972/2658 74.19%\n",
            "Dec set: AUC  0.790231179294239\n",
            "[0m 3s] Epoch 10 [2560/8158] loss=0.0020206839544698596\n",
            "[0m 3s] Epoch 10 [5120/8158] loss=0.0020076171436812728\n",
            "[0m 3s] Epoch 10 [7680/8158] loss=0.0019808160024695097\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dec set: AUC  0.7907030511219748\n",
            "[0m 3s] Epoch 11 [2560/8158] loss=0.0019495594780892134\n",
            "[0m 3s] Epoch 11 [5120/8158] loss=0.0019623207626864314\n",
            "[0m 3s] Epoch 11 [7680/8158] loss=0.001981945320342978\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7909247740289832\n",
            "[0m 3s] Epoch 12 [2560/8158] loss=0.0020262943115085365\n",
            "[0m 3s] Epoch 12 [5120/8158] loss=0.0019971862086094914\n",
            "[0m 3s] Epoch 12 [7680/8158] loss=0.0019716495104754963\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7918104022786293\n",
            "[0m 3s] Epoch 13 [2560/8158] loss=0.0019832826685160397\n",
            "[0m 4s] Epoch 13 [5120/8158] loss=0.0019629649235866965\n",
            "[0m 4s] Epoch 13 [7680/8158] loss=0.001976331905461848\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7921054011321134\n",
            "[0m 4s] Epoch 14 [2560/8158] loss=0.001969071594066918\n",
            "[0m 4s] Epoch 14 [5120/8158] loss=0.0019614438351709396\n",
            "[0m 4s] Epoch 14 [7680/8158] loss=0.0019752352149225772\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7922216319437645\n",
            "[0m 4s] Epoch 15 [2560/8158] loss=0.0019754757056944074\n",
            "[0m 4s] Epoch 15 [5120/8158] loss=0.001953134429641068\n",
            "[0m 4s] Epoch 15 [7680/8158] loss=0.001964255263252805\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7931312643827733\n",
            "[0m 4s] Epoch 16 [2560/8158] loss=0.0019609990529716017\n",
            "[0m 4s] Epoch 16 [5120/8158] loss=0.0019634172436781227\n",
            "[0m 5s] Epoch 16 [7680/8158] loss=0.0019759979060230155\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7931173672205107\n",
            "[0m 5s] Epoch 17 [2560/8158] loss=0.0019767163670621813\n",
            "[0m 5s] Epoch 17 [5120/8158] loss=0.0019958140153903516\n",
            "[0m 5s] Epoch 17 [7680/8158] loss=0.001964307805368056\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7926777115416564\n",
            "[0m 5s] Epoch 18 [2560/8158] loss=0.00197166329016909\n",
            "[0m 5s] Epoch 18 [5120/8158] loss=0.001986204704735428\n",
            "[0m 5s] Epoch 18 [7680/8158] loss=0.0019760694238357248\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7928255268129953\n",
            "[0m 5s] Epoch 19 [2560/8158] loss=0.0019048790098167955\n",
            "[0m 5s] Epoch 19 [5120/8158] loss=0.00194119299412705\n",
            "[0m 6s] Epoch 19 [7680/8158] loss=0.0019669568981043996\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7933915203306009\n",
            "[0m 6s] Epoch 20 [2560/8158] loss=0.001997939369175583\n",
            "[0m 6s] Epoch 20 [5120/8158] loss=0.0019553602032829076\n",
            "[0m 6s] Epoch 20 [7680/8158] loss=0.001962594866442184\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7936719903326285\n",
            "[0m 6s] Epoch 21 [2560/8158] loss=0.0019623611005954444\n",
            "[0m 6s] Epoch 21 [5120/8158] loss=0.0019706212042365223\n",
            "[0m 6s] Epoch 21 [7680/8158] loss=0.001953763918330272\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.794344107634785\n",
            "[0m 6s] Epoch 22 [2560/8158] loss=0.001957881357520819\n",
            "[0m 6s] Epoch 22 [5120/8158] loss=0.0019567033392377197\n",
            "[0m 6s] Epoch 22 [7680/8158] loss=0.0019707168142000836\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dec set: AUC  0.7941741832416647\n",
            "[0m 7s] Epoch 23 [2560/8158] loss=0.0020024391007609664\n",
            "[0m 7s] Epoch 23 [5120/8158] loss=0.001974670868366957\n",
            "[0m 7s] Epoch 23 [7680/8158] loss=0.0019618187138500313\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7939436166859438\n",
            "[0m 7s] Epoch 24 [2560/8158] loss=0.001963848411105573\n",
            "[0m 7s] Epoch 24 [5120/8158] loss=0.00197421534685418\n",
            "[0m 7s] Epoch 24 [7680/8158] loss=0.0019540245567137996\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dec set: AUC  0.794437597635461\n",
            "[0m 7s] Epoch 25 [2560/8158] loss=0.001981308253016323\n",
            "[0m 7s] Epoch 25 [5120/8158] loss=0.0019775927416048945\n",
            "[0m 7s] Epoch 25 [7680/8158] loss=0.0019523469071524838\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7944319124327173\n",
            "[0m 8s] Epoch 26 [2560/8158] loss=0.00198035032954067\n",
            "[0m 8s] Epoch 26 [5120/8158] loss=0.001948832831112668\n",
            "[0m 8s] Epoch 26 [7680/8158] loss=0.0019671983124377825\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2010/2658 75.62%\n",
            "Dec set: AUC  0.7935608130345275\n",
            "[0m 8s] Epoch 27 [2560/8158] loss=0.0019148218911141158\n",
            "[0m 8s] Epoch 27 [5120/8158] loss=0.0019373896182514727\n",
            "[0m 8s] Epoch 27 [7680/8158] loss=0.0019512611324898899\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7950042228422602\n",
            "[0m 8s] Epoch 28 [2560/8158] loss=0.0019322226522490381\n",
            "[0m 8s] Epoch 28 [5120/8158] loss=0.001953354338183999\n",
            "[0m 8s] Epoch 28 [7680/8158] loss=0.001960312908825775\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7937010480355414\n",
            "[0m 9s] Epoch 29 [2560/8158] loss=0.001963753718882799\n",
            "[0m 9s] Epoch 29 [5120/8158] loss=0.001972419035155326\n",
            "[0m 9s] Epoch 29 [7680/8158] loss=0.0019605290377512573\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2010/2658 75.62%\n",
            "Dec set: AUC  0.7946984852724823\n",
            "[0m 9s] Epoch 30 [2560/8158] loss=0.001990940107498318\n",
            "[0m 9s] Epoch 30 [5120/8158] loss=0.001957251160638407\n",
            "[0m 9s] Epoch 30 [7680/8158] loss=0.001958591145618508\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7949303152065907\n",
            "[0m 9s] Epoch 31 [2560/8158] loss=0.0019322987645864487\n",
            "[0m 9s] Epoch 31 [5120/8158] loss=0.0019535762374289336\n",
            "[0m 9s] Epoch 31 [7680/8158] loss=0.001954968840194245\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7948425104086587\n",
            "[0m 10s] Epoch 32 [2560/8158] loss=0.0020156469312496484\n",
            "[0m 10s] Epoch 32 [5120/8158] loss=0.001992350706132129\n",
            "[0m 10s] Epoch 32 [7680/8158] loss=0.001957798365037888\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7946308945287505\n",
            "[0m 10s] Epoch 33 [2560/8158] loss=0.0019200486480258406\n",
            "[0m 10s] Epoch 33 [5120/8158] loss=0.0019360115693416446\n",
            "[0m 10s] Epoch 33 [7680/8158] loss=0.0019553092618783316\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dec set: AUC  0.7949397905444971\n",
            "[0m 10s] Epoch 34 [2560/8158] loss=0.0019538819207809864\n",
            "[0m 10s] Epoch 34 [5120/8158] loss=0.0019509401521645487\n",
            "[0m 10s] Epoch 34 [7680/8158] loss=0.001943236356601119\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7947515471647578\n",
            "[0m 11s] Epoch 35 [2560/8158] loss=0.0019102820777334272\n",
            "[0m 11s] Epoch 35 [5120/8158] loss=0.0018979985092300923\n",
            "[0m 11s] Epoch 35 [7680/8158] loss=0.0019512045003163318\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7950667600724421\n",
            "[0m 11s] Epoch 36 [2560/8158] loss=0.0020357122644782065\n",
            "[0m 11s] Epoch 36 [5120/8158] loss=0.0019655933720059693\n",
            "[0m 11s] Epoch 36 [7680/8158] loss=0.0019522607093676926\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7945955199339001\n",
            "[0m 11s] Epoch 37 [2560/8158] loss=0.0019648388610221446\n",
            "[0m 11s] Epoch 37 [5120/8158] loss=0.001963427447481081\n",
            "[0m 11s] Epoch 37 [7680/8158] loss=0.0019526705960743128\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7962909737299414\n",
            "[0m 11s] Epoch 38 [2560/8158] loss=0.0019383449805900454\n",
            "[0m 12s] Epoch 38 [5120/8158] loss=0.0019365394546184689\n",
            "[0m 12s] Epoch 38 [7680/8158] loss=0.0019490047513196866\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7945626720958247\n",
            "[0m 12s] Epoch 39 [2560/8158] loss=0.0019182647462002932\n",
            "[0m 12s] Epoch 39 [5120/8158] loss=0.0019151608576066792\n",
            "[0m 12s] Epoch 39 [7680/8158] loss=0.001941789275345703\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2010/2658 75.62%\n",
            "Dec set: AUC  0.7949960108827414\n",
            "[0m 12s] Epoch 40 [2560/8158] loss=0.0019068766967393457\n",
            "[0m 12s] Epoch 40 [5120/8158] loss=0.0019180905946996063\n",
            "[0m 12s] Epoch 40 [7680/8158] loss=0.0019444803396860759\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7948879920306091\n",
            "[0m 12s] Epoch 41 [2560/8158] loss=0.002002123813144863\n",
            "[0m 12s] Epoch 41 [5120/8158] loss=0.0019793980522081257\n",
            "[0m 12s] Epoch 41 [7680/8158] loss=0.0019527427774543563\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.794380429763426\n",
            "[0m 13s] Epoch 42 [2560/8158] loss=0.0019031464704312383\n",
            "[0m 13s] Epoch 42 [5120/8158] loss=0.0019464135635644197\n",
            "[0m 13s] Epoch 42 [7680/8158] loss=0.001946458996584018\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7942379838502341\n",
            "[0m 13s] Epoch 43 [2560/8158] loss=0.0019013135693967342\n",
            "[0m 13s] Epoch 43 [5120/8158] loss=0.0019628070236649363\n",
            "[0m 13s] Epoch 43 [7680/8158] loss=0.0019450065912678838\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2012/2658 75.70%\n",
            "Dec set: AUC  0.795253740073794\n",
            "[0m 13s] Epoch 44 [2560/8158] loss=0.0019495442742481829\n",
            "[0m 13s] Epoch 44 [5120/8158] loss=0.001963355834595859\n",
            "[0m 13s] Epoch 44 [7680/8158] loss=0.0019428434044433138\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dec set: AUC  0.7948917821657716\n",
            "[0m 14s] Epoch 45 [2560/8158] loss=0.0019050320144742728\n",
            "[0m 14s] Epoch 45 [5120/8158] loss=0.001927580632036552\n",
            "[0m 14s] Epoch 45 [7680/8158] loss=0.0019459916627965867\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dec set: AUC  0.7935317553316147\n",
            "[0m 14s] Epoch 46 [2560/8158] loss=0.0019390279660001398\n",
            "[0m 14s] Epoch 46 [5120/8158] loss=0.0019362570834346115\n",
            "[0m 14s] Epoch 46 [7680/8158] loss=0.0019475324855496485\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7955525290624406\n",
            "[0m 14s] Epoch 47 [2560/8158] loss=0.0019459931529127062\n",
            "[0m 14s] Epoch 47 [5120/8158] loss=0.0019283665518742056\n",
            "[0m 14s] Epoch 47 [7680/8158] loss=0.0019434926255295674\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7951476162892428\n",
            "[0m 14s] Epoch 48 [2560/8158] loss=0.0018935907748527825\n",
            "[0m 15s] Epoch 48 [5120/8158] loss=0.0019131003646180034\n",
            "[0m 15s] Epoch 48 [7680/8158] loss=0.0019297507940791547\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7939265610777122\n",
            "[0m 15s] Epoch 49 [2560/8158] loss=0.0019190122373402118\n",
            "[0m 15s] Epoch 49 [5120/8158] loss=0.0019283999630715699\n",
            "[0m 15s] Epoch 49 [7680/8158] loss=0.001940086049338182\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.794755968989114\n",
            "[0m 15s] Epoch 50 [2560/8158] loss=0.0019072919618338346\n",
            "[0m 15s] Epoch 50 [5120/8158] loss=0.0019342243438586592\n",
            "[0m 15s] Epoch 50 [7680/8158] loss=0.0019358937589762113\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7946315262179442\n",
            "[0m 15s] Epoch 51 [2560/8158] loss=0.00195343418745324\n",
            "[0m 16s] Epoch 51 [5120/8158] loss=0.0019341520848684013\n",
            "[0m 16s] Epoch 51 [7680/8158] loss=0.0019362474132018784\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7942619880395969\n",
            "[0m 16s] Epoch 52 [2560/8158] loss=0.0019157814444042743\n",
            "[0m 16s] Epoch 52 [5120/8158] loss=0.001962869771523401\n",
            "[0m 16s] Epoch 52 [7680/8158] loss=0.0019339347956702114\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7942790436478282\n",
            "[0m 16s] Epoch 53 [2560/8158] loss=0.0019568086252547802\n",
            "[0m 16s] Epoch 53 [5120/8158] loss=0.0019322855572681875\n",
            "[0m 16s] Epoch 53 [7680/8158] loss=0.0019356832606717944\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7950370706803355\n",
            "[0m 16s] Epoch 54 [2560/8158] loss=0.001904165034648031\n",
            "[0m 16s] Epoch 54 [5120/8158] loss=0.0019380226614885033\n",
            "[0m 17s] Epoch 54 [7680/8158] loss=0.0019442315174577137\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7945582502714684\n",
            "[0m 17s] Epoch 55 [2560/8158] loss=0.0019304004381410778\n",
            "[0m 17s] Epoch 55 [5120/8158] loss=0.001931041874922812\n",
            "[0m 17s] Epoch 55 [7680/8158] loss=0.0019322433159686624\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7933972055333446\n",
            "[0m 17s] Epoch 56 [2560/8158] loss=0.0019770048558712004\n",
            "[0m 17s] Epoch 56 [5120/8158] loss=0.00194859451148659\n",
            "[0m 17s] Epoch 56 [7680/8158] loss=0.0019448320614174008\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7944988714872554\n",
            "[0m 17s] Epoch 57 [2560/8158] loss=0.0018928213161416353\n",
            "[0m 17s] Epoch 57 [5120/8158] loss=0.0019196754321455956\n",
            "[0m 17s] Epoch 57 [7680/8158] loss=0.0019339799182489514\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.794657741319485\n",
            "[0m 18s] Epoch 58 [2560/8158] loss=0.001948724826797843\n",
            "[0m 18s] Epoch 58 [5120/8158] loss=0.0019569307158235462\n",
            "[0m 18s] Epoch 58 [7680/8158] loss=0.0019372790896644194\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7930560933687163\n",
            "[0m 18s] Epoch 59 [2560/8158] loss=0.001903326902538538\n",
            "[0m 18s] Epoch 59 [5120/8158] loss=0.0019346570828929544\n",
            "[0m 18s] Epoch 59 [7680/8158] loss=0.0019293535772400598\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7927869937721763\n",
            "[0m 18s] Epoch 60 [2560/8158] loss=0.0019163762102834881\n",
            "[0m 18s] Epoch 60 [5120/8158] loss=0.0019198593508917838\n",
            "[0m 18s] Epoch 60 [7680/8158] loss=0.0019258668529801072\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7931281059368045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "TOxVvqjkb1dj",
        "outputId": "ff266e13-e50f-480d-816d-7418e4a19009"
      },
      "source": [
        "\r\n",
        "for i, HIDDEN_SIZE in enumerate(HIDDEN_SIZEs):\r\n",
        "\r\n",
        "  epoch = np.arange(1, len(acc_list) + 1, 1)\r\n",
        "  plt.plot(epoch[10:], aucs[i][10:], label = str(HIDDEN_SIZE))\r\n",
        "\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylabel('AUC')\r\n",
        "plt.legend()\r\n",
        "plt.grid()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dYEuovgdA0z",
        "outputId": "29f064bc-855c-4016-f295-1ce6a54d1b90"
      },
      "source": [
        "#调参N_LAYER\r\n",
        "N_EPOCHS  = 60\r\n",
        "HIDDEN_SIZE = 128\r\n",
        "\r\n",
        "N_LAYERs = [1, 2, 3, 4, 5] \r\n",
        "\r\n",
        "accs = []\r\n",
        "aucs = []\r\n",
        "for N_LAYER in N_LAYERs:\r\n",
        "  \r\n",
        "  if __name__ == '__main__':\r\n",
        "    classifier = RNNClassifier(11, HIDDEN_SIZE, N_CLASS, N_LAYER, bidirectional = False)\r\n",
        "    if USE_GPU:\r\n",
        "      device = torch.device(\"cuda:0\")\r\n",
        "      classifier.to(device)\r\n",
        "\r\n",
        "    criterion = torch.nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.0005)\r\n",
        "\r\n",
        "    start = time.time()\r\n",
        "    print(\"Training for %d epochs...\" % N_EPOCHS)\r\n",
        "    acc_list = []\r\n",
        "    auc_list = []\r\n",
        "    for epoch in range(1, N_EPOCHS + 1):\r\n",
        "      # Train cycle\r\n",
        "      trainModel()\r\n",
        "      acc, auc = devModel_auc()\r\n",
        "      acc_list.append(acc)\r\n",
        "      auc_list.append(auc)\r\n",
        "    accs.append(acc_list)\r\n",
        "    aucs.append(auc_list)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.0026507659815251826\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.002572451124433428\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.0025082085048779845\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7691760309325565\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.002344636060297489\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.0023166336468420923\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.0022999994611988465\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7695237758337192\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.00218875827267766\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.0021827931399457158\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.0021727691947792966\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1941/2658 73.02%\n",
            "Dec set: AUC  0.7688832429912504\n",
            "[0m 0s] Epoch 4 [2560/8158] loss=0.0020719136111438273\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.0020901946700178087\n",
            "[0m 0s] Epoch 4 [7680/8158] loss=0.0020751331622401873\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1957/2658 73.63%\n",
            "Dec set: AUC  0.772650637342812\n",
            "[0m 0s] Epoch 5 [2560/8158] loss=0.0020522642880678177\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.0020714383048471064\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.0020389997633174064\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1971/2658 74.15%\n",
            "Dec set: AUC  0.7803288194929178\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.0020715025370009244\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.0020336471498012543\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.0020279282859216132\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1955/2658 73.55%\n",
            "Dec set: AUC  0.7847367466869482\n",
            "[0m 1s] Epoch 7 [2560/8158] loss=0.002034413907676935\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.0020081931725144386\n",
            "[0m 1s] Epoch 7 [7680/8158] loss=0.002009967714548111\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1972/2658 74.19%\n",
            "Dec set: AUC  0.7863993526449143\n",
            "[0m 1s] Epoch 8 [2560/8158] loss=0.002012369269505143\n",
            "[0m 1s] Epoch 8 [5120/8158] loss=0.0019759909366257487\n",
            "[0m 1s] Epoch 8 [7680/8158] loss=0.0019977252813987434\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dec set: AUC  0.7875844015724007\n",
            "[0m 1s] Epoch 9 [2560/8158] loss=0.001978730864357203\n",
            "[0m 1s] Epoch 9 [5120/8158] loss=0.00198607022757642\n",
            "[0m 1s] Epoch 9 [7680/8158] loss=0.0019998482428491117\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1965/2658 73.93%\n",
            "Dec set: AUC  0.7886071063770919\n",
            "[0m 1s] Epoch 10 [2560/8158] loss=0.0019979124655947087\n",
            "[0m 1s] Epoch 10 [5120/8158] loss=0.0020197063218802213\n",
            "[0m 1s] Epoch 10 [7680/8158] loss=0.0019905705470591784\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7886159500258044\n",
            "[0m 2s] Epoch 11 [2560/8158] loss=0.001991119852755219\n",
            "[0m 2s] Epoch 11 [5120/8158] loss=0.0020070718368515373\n",
            "[0m 2s] Epoch 11 [7680/8158] loss=0.0019899499408590295\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7890612909074026\n",
            "[0m 2s] Epoch 12 [2560/8158] loss=0.001967781421262771\n",
            "[0m 2s] Epoch 12 [5120/8158] loss=0.0019805334392003714\n",
            "[0m 2s] Epoch 12 [7680/8158] loss=0.0019977849054460726\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.789158571043241\n",
            "[0m 2s] Epoch 13 [2560/8158] loss=0.001997074706014246\n",
            "[0m 2s] Epoch 13 [5120/8158] loss=0.002005597873358056\n",
            "[0m 2s] Epoch 13 [7680/8158] loss=0.0019909779735219975\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1976/2658 74.34%\n",
            "Dec set: AUC  0.7888973675616229\n",
            "[0m 2s] Epoch 14 [2560/8158] loss=0.0020146725117228927\n",
            "[0m 2s] Epoch 14 [5120/8158] loss=0.0019867687718942762\n",
            "[0m 2s] Epoch 14 [7680/8158] loss=0.0019856291784284014\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7894693621265692\n",
            "[0m 2s] Epoch 15 [2560/8158] loss=0.001989257265813649\n",
            "[0m 2s] Epoch 15 [5120/8158] loss=0.0019829905941151083\n",
            "[0m 2s] Epoch 15 [7680/8158] loss=0.001983025534233699\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7895517975663542\n",
            "[0m 3s] Epoch 16 [2560/8158] loss=0.0020160804502665996\n",
            "[0m 3s] Epoch 16 [5120/8158] loss=0.002001723990542814\n",
            "[0m 3s] Epoch 16 [7680/8158] loss=0.0019922912935726343\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7896860315200274\n",
            "[0m 3s] Epoch 17 [2560/8158] loss=0.0019921482307836413\n",
            "[0m 3s] Epoch 17 [5120/8158] loss=0.0019947062828578055\n",
            "[0m 3s] Epoch 17 [7680/8158] loss=0.001986864119923363\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7897169842905214\n",
            "[0m 3s] Epoch 18 [2560/8158] loss=0.001980299421120435\n",
            "[0m 3s] Epoch 18 [5120/8158] loss=0.0019886339141521603\n",
            "[0m 3s] Epoch 18 [7680/8158] loss=0.001977139823914816\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7898603777375042\n",
            "[0m 3s] Epoch 19 [2560/8158] loss=0.00193175160093233\n",
            "[0m 3s] Epoch 19 [5120/8158] loss=0.0019572100311052052\n",
            "[0m 3s] Epoch 19 [7680/8158] loss=0.0019872424425557257\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7899327061501893\n",
            "[0m 3s] Epoch 20 [2560/8158] loss=0.001940588594879955\n",
            "[0m 3s] Epoch 20 [5120/8158] loss=0.001963355176849291\n",
            "[0m 3s] Epoch 20 [7680/8158] loss=0.001974090545748671\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7900183000359431\n",
            "[0m 3s] Epoch 21 [2560/8158] loss=0.002039745659567416\n",
            "[0m 3s] Epoch 21 [5120/8158] loss=0.00199065794586204\n",
            "[0m 4s] Epoch 21 [7680/8158] loss=0.001983970923659702\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1979/2658 74.45%\n",
            "Dec set: AUC  0.7901193703069441\n",
            "[0m 4s] Epoch 22 [2560/8158] loss=0.0019987267442047596\n",
            "[0m 4s] Epoch 22 [5120/8158] loss=0.002000938431592658\n",
            "[0m 4s] Epoch 22 [7680/8158] loss=0.0019820627368365724\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7903082453758772\n",
            "[0m 4s] Epoch 23 [2560/8158] loss=0.0020218405872583388\n",
            "[0m 4s] Epoch 23 [5120/8158] loss=0.001979522273177281\n",
            "[0m 4s] Epoch 23 [7680/8158] loss=0.00198556756755958\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7903600438897651\n",
            "[0m 4s] Epoch 24 [2560/8158] loss=0.0019489339902065694\n",
            "[0m 4s] Epoch 24 [5120/8158] loss=0.0019742151605896653\n",
            "[0m 4s] Epoch 24 [7680/8158] loss=0.0019833666970953344\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7905659745669298\n",
            "[0m 4s] Epoch 25 [2560/8158] loss=0.001980233087670058\n",
            "[0m 4s] Epoch 25 [5120/8158] loss=0.0019672592054121195\n",
            "[0m 4s] Epoch 25 [7680/8158] loss=0.001971291781713565\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1979/2658 74.45%\n",
            "Dec set: AUC  0.7906095611212989\n",
            "[0m 5s] Epoch 26 [2560/8158] loss=0.001982490357477218\n",
            "[0m 5s] Epoch 26 [5120/8158] loss=0.0019838171429000795\n",
            "[0m 5s] Epoch 26 [7680/8158] loss=0.0019841301216123003\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dec set: AUC  0.7907557971696535\n",
            "[0m 5s] Epoch 27 [2560/8158] loss=0.001974625699222088\n",
            "[0m 5s] Epoch 27 [5120/8158] loss=0.001959454652387649\n",
            "[0m 5s] Epoch 27 [7680/8158] loss=0.0019712499730909863\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7908875043665515\n",
            "[0m 5s] Epoch 28 [2560/8158] loss=0.00202605101512745\n",
            "[0m 5s] Epoch 28 [5120/8158] loss=0.001985237584449351\n",
            "[0m 5s] Epoch 28 [7680/8158] loss=0.001984794142966469\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7909528841981052\n",
            "[0m 5s] Epoch 29 [2560/8158] loss=0.0019435087800957263\n",
            "[0m 5s] Epoch 29 [5120/8158] loss=0.001971324114128947\n",
            "[0m 5s] Epoch 29 [7680/8158] loss=0.0019706932129338385\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7909857320361806\n",
            "[0m 5s] Epoch 30 [2560/8158] loss=0.001979064091574401\n",
            "[0m 6s] Epoch 30 [5120/8158] loss=0.0019641346007119864\n",
            "[0m 6s] Epoch 30 [7680/8158] loss=0.001969823722417156\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7911041737600099\n",
            "[0m 6s] Epoch 31 [2560/8158] loss=0.001980843325145543\n",
            "[0m 6s] Epoch 31 [5120/8158] loss=0.0019832743040751667\n",
            "[0m 6s] Epoch 31 [7680/8158] loss=0.0019766693585552275\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7911717645037418\n",
            "[0m 6s] Epoch 32 [2560/8158] loss=0.001958447298966348\n",
            "[0m 6s] Epoch 32 [5120/8158] loss=0.0019868019269779326\n",
            "[0m 6s] Epoch 32 [7680/8158] loss=0.001981873391196132\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.791300629099268\n",
            "[0m 6s] Epoch 33 [2560/8158] loss=0.001880553411319852\n",
            "[0m 6s] Epoch 33 [5120/8158] loss=0.0019455308269243688\n",
            "[0m 6s] Epoch 33 [7680/8158] loss=0.001966233889106661\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7914219134244692\n",
            "[0m 6s] Epoch 34 [2560/8158] loss=0.001924620650243014\n",
            "[0m 6s] Epoch 34 [5120/8158] loss=0.0019600110594183207\n",
            "[0m 6s] Epoch 34 [7680/8158] loss=0.0019631401480485994\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dec set: AUC  0.7914996111953012\n",
            "[0m 6s] Epoch 35 [2560/8158] loss=0.002009807573631406\n",
            "[0m 6s] Epoch 35 [5120/8158] loss=0.001974081451771781\n",
            "[0m 6s] Epoch 35 [7680/8158] loss=0.0019855140902412436\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7915602533579018\n",
            "[0m 7s] Epoch 36 [2560/8158] loss=0.0019643157138489187\n",
            "[0m 7s] Epoch 36 [5120/8158] loss=0.001979249867144972\n",
            "[0m 7s] Epoch 36 [7680/8158] loss=0.0019709406847444673\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7916512166018027\n",
            "[0m 7s] Epoch 37 [2560/8158] loss=0.0020000526565127075\n",
            "[0m 7s] Epoch 37 [5120/8158] loss=0.0019671242393087597\n",
            "[0m 7s] Epoch 37 [7680/8158] loss=0.0019620779203251004\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7917529185619975\n",
            "[0m 7s] Epoch 38 [2560/8158] loss=0.0020066680037416516\n",
            "[0m 7s] Epoch 38 [5120/8158] loss=0.0019634613243397327\n",
            "[0m 7s] Epoch 38 [7680/8158] loss=0.0019697416224516927\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7918615691033235\n",
            "[0m 7s] Epoch 39 [2560/8158] loss=0.0019094722461886703\n",
            "[0m 7s] Epoch 39 [5120/8158] loss=0.0019446754071395845\n",
            "[0m 7s] Epoch 39 [7680/8158] loss=0.001972095435485244\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.79188620498188\n",
            "[0m 7s] Epoch 40 [2560/8158] loss=0.0019910035654902456\n",
            "[0m 7s] Epoch 40 [5120/8158] loss=0.001989755377871916\n",
            "[0m 7s] Epoch 40 [7680/8158] loss=0.0019723811962952214\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7921123497132447\n",
            "[0m 7s] Epoch 41 [2560/8158] loss=0.001974835863802582\n",
            "[0m 8s] Epoch 41 [5120/8158] loss=0.001966936979442835\n",
            "[0m 8s] Epoch 41 [7680/8158] loss=0.0019728092670751114\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7921420391053513\n",
            "[0m 8s] Epoch 42 [2560/8158] loss=0.0019344731350429355\n",
            "[0m 8s] Epoch 42 [5120/8158] loss=0.001968184014549479\n",
            "[0m 8s] Epoch 42 [7680/8158] loss=0.001962399552576244\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7922228953221518\n",
            "[0m 8s] Epoch 43 [2560/8158] loss=0.0019504847237840294\n",
            "[0m 8s] Epoch 43 [5120/8158] loss=0.0019542487687431278\n",
            "[0m 8s] Epoch 43 [7680/8158] loss=0.0019685844657942654\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7922702720116839\n",
            "[0m 8s] Epoch 44 [2560/8158] loss=0.00202542437473312\n",
            "[0m 8s] Epoch 44 [5120/8158] loss=0.002000557433348149\n",
            "[0m 8s] Epoch 44 [7680/8158] loss=0.0019679682988983887\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7923460747149345\n",
            "[0m 8s] Epoch 45 [2560/8158] loss=0.00198218192672357\n",
            "[0m 9s] Epoch 45 [5120/8158] loss=0.001956756768049672\n",
            "[0m 9s] Epoch 45 [7680/8158] loss=0.0019586424343287943\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7925368448514488\n",
            "[0m 9s] Epoch 46 [2560/8158] loss=0.002000356209464371\n",
            "[0m 9s] Epoch 46 [5120/8158] loss=0.0019828866934403777\n",
            "[0m 9s] Epoch 46 [7680/8158] loss=0.001978905836585909\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.792448408364323\n",
            "[0m 9s] Epoch 47 [2560/8158] loss=0.0019847833784297107\n",
            "[0m 9s] Epoch 47 [5120/8158] loss=0.0019801545655354857\n",
            "[0m 9s] Epoch 47 [7680/8158] loss=0.0019644167662287754\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7926290714737373\n",
            "[0m 9s] Epoch 48 [2560/8158] loss=0.002030041045509279\n",
            "[0m 9s] Epoch 48 [5120/8158] loss=0.0019704596605151893\n",
            "[0m 9s] Epoch 48 [7680/8158] loss=0.0019699698275265592\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7926821333660128\n",
            "[0m 9s] Epoch 49 [2560/8158] loss=0.001963152806274593\n",
            "[0m 9s] Epoch 49 [5120/8158] loss=0.0019815697451122105\n",
            "[0m 9s] Epoch 49 [7680/8158] loss=0.001972652579812954\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7927712015423324\n",
            "[0m 10s] Epoch 50 [2560/8158] loss=0.001984233793336898\n",
            "[0m 10s] Epoch 50 [5120/8158] loss=0.0019755322602577506\n",
            "[0m 10s] Epoch 50 [7680/8158] loss=0.001964840607251972\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7927756233666885\n",
            "[0m 10s] Epoch 51 [2560/8158] loss=0.001998178754001856\n",
            "[0m 10s] Epoch 51 [5120/8158] loss=0.001977469975827262\n",
            "[0m 10s] Epoch 51 [7680/8158] loss=0.0019669277283052605\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1979/2658 74.45%\n",
            "Dec set: AUC  0.7928804837728523\n",
            "[0m 10s] Epoch 52 [2560/8158] loss=0.0019741100259125233\n",
            "[0m 10s] Epoch 52 [5120/8158] loss=0.0019530587829649448\n",
            "[0m 10s] Epoch 52 [7680/8158] loss=0.0019601812702603636\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7928533211375206\n",
            "[0m 10s] Epoch 53 [2560/8158] loss=0.001973175350576639\n",
            "[0m 10s] Epoch 53 [5120/8158] loss=0.0019658946141134946\n",
            "[0m 10s] Epoch 53 [7680/8158] loss=0.001964395718338589\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7928842739080147\n",
            "[0m 10s] Epoch 54 [2560/8158] loss=0.0019353757030330597\n",
            "[0m 10s] Epoch 54 [5120/8158] loss=0.001962388859828934\n",
            "[0m 10s] Epoch 54 [7680/8158] loss=0.001973038275415699\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7930042948548284\n",
            "[0m 10s] Epoch 55 [2560/8158] loss=0.0019538786727935076\n",
            "[0m 11s] Epoch 55 [5120/8158] loss=0.001961148821283132\n",
            "[0m 11s] Epoch 55 [7680/8158] loss=0.001963972405064851\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dec set: AUC  0.7930693588417852\n",
            "[0m 11s] Epoch 56 [2560/8158] loss=0.0020099279354326427\n",
            "[0m 11s] Epoch 56 [5120/8158] loss=0.0019662046281155197\n",
            "[0m 11s] Epoch 56 [7680/8158] loss=0.001978529233019799\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7930965214771168\n",
            "[0m 11s] Epoch 57 [2560/8158] loss=0.0019717578776180743\n",
            "[0m 11s] Epoch 57 [5120/8158] loss=0.001972183503676206\n",
            "[0m 11s] Epoch 57 [7680/8158] loss=0.0019620304733204345\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7932544437755558\n",
            "[0m 11s] Epoch 58 [2560/8158] loss=0.0019442044082097708\n",
            "[0m 11s] Epoch 58 [5120/8158] loss=0.001958190312143415\n",
            "[0m 11s] Epoch 58 [7680/8158] loss=0.001962843188084662\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7932961352623437\n",
            "[0m 11s] Epoch 59 [2560/8158] loss=0.0019367935601621866\n",
            "[0m 11s] Epoch 59 [5120/8158] loss=0.001960027014138177\n",
            "[0m 12s] Epoch 59 [7680/8158] loss=0.001961482781916857\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dec set: AUC  0.7933239295868689\n",
            "[0m 12s] Epoch 60 [2560/8158] loss=0.0019043459673412145\n",
            "[0m 12s] Epoch 60 [5120/8158] loss=0.001949562644585967\n",
            "[0m 12s] Epoch 60 [7680/8158] loss=0.0019640691267947354\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7933302464788066\n",
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.002614260045811534\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.002492348838131875\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.0024604338531692823\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7670639781132327\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.0023185203550383448\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.0022751644602976738\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.0022218043295045694\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1924/2658 72.39%\n",
            "Dec set: AUC  0.7671467293976149\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.002108086261432618\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.0021003236121032386\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.00207278203452006\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1956/2658 73.59%\n",
            "Dec set: AUC  0.7789331022193137\n",
            "[0m 0s] Epoch 4 [2560/8158] loss=0.0020498761674389242\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.0020359806600026785\n",
            "[0m 0s] Epoch 4 [7680/8158] loss=0.0020258557439471283\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1927/2658 72.50%\n",
            "Dec set: AUC  0.7886589048909799\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.002001743612345308\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.002004519832553342\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.002010720180502782\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dec set: AUC  0.7882622040773011\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.0020001327502541244\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.0019841224770061673\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.001988375703028093\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1962/2658 73.81%\n",
            "Dec set: AUC  0.7892688008075515\n",
            "[0m 1s] Epoch 7 [2560/8158] loss=0.001972636894788593\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.0019898107857443392\n",
            "[0m 1s] Epoch 7 [7680/8158] loss=0.0020073796855285763\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7892634314494046\n",
            "[0m 1s] Epoch 8 [2560/8158] loss=0.001927138736937195\n",
            "[0m 1s] Epoch 8 [5120/8158] loss=0.0019194796506781131\n",
            "[0m 1s] Epoch 8 [7680/8158] loss=0.001991185386820386\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dec set: AUC  0.7892899623955423\n",
            "[0m 1s] Epoch 9 [2560/8158] loss=0.0019751788466237487\n",
            "[0m 2s] Epoch 9 [5120/8158] loss=0.001989017863525078\n",
            "[0m 2s] Epoch 9 [7680/8158] loss=0.0019954777128684026\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7891415154350097\n",
            "[0m 2s] Epoch 10 [2560/8158] loss=0.002009638864547014\n",
            "[0m 2s] Epoch 10 [5120/8158] loss=0.001990009070141241\n",
            "[0m 2s] Epoch 10 [7680/8158] loss=0.001984144048765302\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7892514293547231\n",
            "[0m 2s] Epoch 11 [2560/8158] loss=0.0019956599571742117\n",
            "[0m 2s] Epoch 11 [5120/8158] loss=0.0020142323861364274\n",
            "[0m 2s] Epoch 11 [7680/8158] loss=0.001992016685350488\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dec set: AUC  0.7893739770583118\n",
            "[0m 2s] Epoch 12 [2560/8158] loss=0.0019383810344152153\n",
            "[0m 2s] Epoch 12 [5120/8158] loss=0.00198928780737333\n",
            "[0m 2s] Epoch 12 [7680/8158] loss=0.0019885692823057373\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dec set: AUC  0.7893404975310427\n",
            "[0m 3s] Epoch 13 [2560/8158] loss=0.0020167107810266315\n",
            "[0m 3s] Epoch 13 [5120/8158] loss=0.002008212963119149\n",
            "[0m 3s] Epoch 13 [7680/8158] loss=0.0019809977267868815\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7894586234102752\n",
            "[0m 3s] Epoch 14 [2560/8158] loss=0.0019932863651774824\n",
            "[0m 3s] Epoch 14 [5120/8158] loss=0.0019748053571674975\n",
            "[0m 3s] Epoch 14 [7680/8158] loss=0.001989881822373718\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7892381638816544\n",
            "[0m 3s] Epoch 15 [2560/8158] loss=0.0019481239141896367\n",
            "[0m 3s] Epoch 15 [5120/8158] loss=0.0019583236775361002\n",
            "[0m 3s] Epoch 15 [7680/8158] loss=0.0019796167927173276\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7897302497635904\n",
            "[0m 3s] Epoch 16 [2560/8158] loss=0.0019540757639333606\n",
            "[0m 3s] Epoch 16 [5120/8158] loss=0.0019978837575763465\n",
            "[0m 3s] Epoch 16 [7680/8158] loss=0.0019789913242372376\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dec set: AUC  0.7898761699673479\n",
            "[0m 3s] Epoch 17 [2560/8158] loss=0.0019611632567830386\n",
            "[0m 3s] Epoch 17 [5120/8158] loss=0.0019738418573979287\n",
            "[0m 4s] Epoch 17 [7680/8158] loss=0.0019780491245910527\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7897795215207033\n",
            "[0m 4s] Epoch 18 [2560/8158] loss=0.0019766057725064457\n",
            "[0m 4s] Epoch 18 [5120/8158] loss=0.0020047770871315152\n",
            "[0m 4s] Epoch 18 [7680/8158] loss=0.00198027704609558\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7903000334163583\n",
            "[0m 4s] Epoch 19 [2560/8158] loss=0.001976717077195644\n",
            "[0m 4s] Epoch 19 [5120/8158] loss=0.0019781888637226074\n",
            "[0m 4s] Epoch 19 [7680/8158] loss=0.0019868522649630904\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7904042621333282\n",
            "[0m 4s] Epoch 20 [2560/8158] loss=0.0019204972311854362\n",
            "[0m 4s] Epoch 20 [5120/8158] loss=0.0019463666598312558\n",
            "[0m 4s] Epoch 20 [7680/8158] loss=0.001966859505046159\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dec set: AUC  0.7903290911192713\n",
            "[0m 4s] Epoch 21 [2560/8158] loss=0.0019737140275537966\n",
            "[0m 4s] Epoch 21 [5120/8158] loss=0.0020048842881806193\n",
            "[0m 4s] Epoch 21 [7680/8158] loss=0.0019829447109562656\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1969/2658 74.08%\n",
            "Dec set: AUC  0.7907160007504468\n",
            "[0m 5s] Epoch 22 [2560/8158] loss=0.0019283415284007787\n",
            "[0m 5s] Epoch 22 [5120/8158] loss=0.0019654240168165416\n",
            "[0m 5s] Epoch 22 [7680/8158] loss=0.0019725832894134025\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7911578673414792\n",
            "[0m 5s] Epoch 23 [2560/8158] loss=0.0019372574635781348\n",
            "[0m 5s] Epoch 23 [5120/8158] loss=0.0019695941009558737\n",
            "[0m 5s] Epoch 23 [7680/8158] loss=0.0019784365120964747\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7914193866676942\n",
            "[0m 5s] Epoch 24 [2560/8158] loss=0.0019515324733220042\n",
            "[0m 5s] Epoch 24 [5120/8158] loss=0.0019383407721761613\n",
            "[0m 5s] Epoch 24 [7680/8158] loss=0.001962284972735991\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7915615167362894\n",
            "[0m 5s] Epoch 25 [2560/8158] loss=0.001966366148553789\n",
            "[0m 5s] Epoch 25 [5120/8158] loss=0.001992405077908188\n",
            "[0m 5s] Epoch 25 [7680/8158] loss=0.001971880067139864\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.791847040251867\n",
            "[0m 5s] Epoch 26 [2560/8158] loss=0.001968235953245312\n",
            "[0m 5s] Epoch 26 [5120/8158] loss=0.001964109786786139\n",
            "[0m 6s] Epoch 26 [7680/8158] loss=0.0019705607749832175\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7921331954566386\n",
            "[0m 6s] Epoch 27 [2560/8158] loss=0.0019718336989171803\n",
            "[0m 6s] Epoch 27 [5120/8158] loss=0.0019650123082101344\n",
            "[0m 6s] Epoch 27 [7680/8158] loss=0.0019811152558152872\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.792383344377366\n",
            "[0m 6s] Epoch 28 [2560/8158] loss=0.0019970901776105165\n",
            "[0m 6s] Epoch 28 [5120/8158] loss=0.001978444226551801\n",
            "[0m 6s] Epoch 28 [7680/8158] loss=0.001971722231246531\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7926960305282753\n",
            "[0m 6s] Epoch 29 [2560/8158] loss=0.00201095687225461\n",
            "[0m 6s] Epoch 29 [5120/8158] loss=0.0019706717983353885\n",
            "[0m 6s] Epoch 29 [7680/8158] loss=0.001966377235172937\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7928823788404333\n",
            "[0m 6s] Epoch 30 [2560/8158] loss=0.0019110838999040424\n",
            "[0m 6s] Epoch 30 [5120/8158] loss=0.0019416491908486933\n",
            "[0m 7s] Epoch 30 [7680/8158] loss=0.0019629909462916353\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.792836897218483\n",
            "[0m 7s] Epoch 31 [2560/8158] loss=0.0019665625295601785\n",
            "[0m 7s] Epoch 31 [5120/8158] loss=0.0019676224153954537\n",
            "[0m 7s] Epoch 31 [7680/8158] loss=0.0019599038331458967\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7928274218805766\n",
            "[0m 7s] Epoch 32 [2560/8158] loss=0.001948925037868321\n",
            "[0m 7s] Epoch 32 [5120/8158] loss=0.001938635902479291\n",
            "[0m 7s] Epoch 32 [7680/8158] loss=0.00195953434255595\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7931811678290801\n",
            "[0m 7s] Epoch 33 [2560/8158] loss=0.001984361791983247\n",
            "[0m 7s] Epoch 33 [5120/8158] loss=0.00196752049960196\n",
            "[0m 7s] Epoch 33 [7680/8158] loss=0.001954933865151058\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dec set: AUC  0.7928583746510707\n",
            "[0m 7s] Epoch 34 [2560/8158] loss=0.0019591919030062855\n",
            "[0m 7s] Epoch 34 [5120/8158] loss=0.0019561359367799014\n",
            "[0m 7s] Epoch 34 [7680/8158] loss=0.0019628876587376\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7934502674256203\n",
            "[0m 7s] Epoch 35 [2560/8158] loss=0.0019580034189857543\n",
            "[0m 8s] Epoch 35 [5120/8158] loss=0.0019658381002955138\n",
            "[0m 8s] Epoch 35 [7680/8158] loss=0.001963820762466639\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7935936608726029\n",
            "[0m 8s] Epoch 36 [2560/8158] loss=0.001902596780564636\n",
            "[0m 8s] Epoch 36 [5120/8158] loss=0.0019286024325992912\n",
            "[0m 8s] Epoch 36 [7680/8158] loss=0.001949718842903773\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7937477930358794\n",
            "[0m 8s] Epoch 37 [2560/8158] loss=0.0019138022093102337\n",
            "[0m 8s] Epoch 37 [5120/8158] loss=0.0019633119751233607\n",
            "[0m 8s] Epoch 37 [7680/8158] loss=0.001963838542966793\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1974/2658 74.27%\n",
            "Dec set: AUC  0.7937269472924854\n",
            "[0m 8s] Epoch 38 [2560/8158] loss=0.0020255110575817525\n",
            "[0m 8s] Epoch 38 [5120/8158] loss=0.00197788315708749\n",
            "[0m 8s] Epoch 38 [7680/8158] loss=0.001968491333536804\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7942127162824839\n",
            "[0m 8s] Epoch 39 [2560/8158] loss=0.002054603095166385\n",
            "[0m 8s] Epoch 39 [5120/8158] loss=0.0019733324588742107\n",
            "[0m 9s] Epoch 39 [7680/8158] loss=0.001975671388208866\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7941830268903773\n",
            "[0m 9s] Epoch 40 [2560/8158] loss=0.001964066701475531\n",
            "[0m 9s] Epoch 40 [5120/8158] loss=0.0019788413716014474\n",
            "[0m 9s] Epoch 40 [7680/8158] loss=0.0019654274336062373\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7940042588485443\n",
            "[0m 9s] Epoch 41 [2560/8158] loss=0.001995657559018582\n",
            "[0m 9s] Epoch 41 [5120/8158] loss=0.0019523402152117342\n",
            "[0m 9s] Epoch 41 [7680/8158] loss=0.0019617059850133957\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.794334000607685\n",
            "[0m 9s] Epoch 42 [2560/8158] loss=0.001961070578545332\n",
            "[0m 9s] Epoch 42 [5120/8158] loss=0.0019413044152315705\n",
            "[0m 9s] Epoch 42 [7680/8158] loss=0.0019584448581251006\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.7944717088519239\n",
            "[0m 9s] Epoch 43 [2560/8158] loss=0.0019696664996445177\n",
            "[0m 9s] Epoch 43 [5120/8158] loss=0.0019614678632933645\n",
            "[0m 10s] Epoch 43 [7680/8158] loss=0.0019603301693374912\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7943845357431853\n",
            "[0m 10s] Epoch 44 [2560/8158] loss=0.0019279412925243377\n",
            "[0m 10s] Epoch 44 [5120/8158] loss=0.0019169117382261901\n",
            "[0m 10s] Epoch 44 [7680/8158] loss=0.0019515229505486786\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7946492135153692\n",
            "[0m 10s] Epoch 45 [2560/8158] loss=0.0019355962635017932\n",
            "[0m 10s] Epoch 45 [5120/8158] loss=0.0019630640395916998\n",
            "[0m 10s] Epoch 45 [7680/8158] loss=0.0019550090927320223\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7947250162186201\n",
            "[0m 10s] Epoch 46 [2560/8158] loss=0.001958266308065504\n",
            "[0m 10s] Epoch 46 [5120/8158] loss=0.001953882595989853\n",
            "[0m 10s] Epoch 46 [7680/8158] loss=0.001961245477044334\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7947218577726514\n",
            "[0m 10s] Epoch 47 [2560/8158] loss=0.0019480608287267386\n",
            "[0m 10s] Epoch 47 [5120/8158] loss=0.0019465097575448453\n",
            "[0m 10s] Epoch 47 [7680/8158] loss=0.001961705678453048\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7949189448011031\n",
            "[0m 11s] Epoch 48 [2560/8158] loss=0.0019694473361596466\n",
            "[0m 11s] Epoch 48 [5120/8158] loss=0.001987345691304654\n",
            "[0m 11s] Epoch 48 [7680/8158] loss=0.0019523410553423067\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7946953268265137\n",
            "[0m 11s] Epoch 49 [2560/8158] loss=0.001990139379631728\n",
            "[0m 11s] Epoch 49 [5120/8158] loss=0.001963580143637955\n",
            "[0m 11s] Epoch 49 [7680/8158] loss=0.0019640803531122703\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2009/2658 75.58%\n",
            "Dec set: AUC  0.7949132595983595\n",
            "[0m 11s] Epoch 50 [2560/8158] loss=0.00197062665829435\n",
            "[0m 11s] Epoch 50 [5120/8158] loss=0.0019589681236539037\n",
            "[0m 11s] Epoch 50 [7680/8158] loss=0.0019536948724028966\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7948526174357587\n",
            "[0m 11s] Epoch 51 [2560/8158] loss=0.0019462026190012693\n",
            "[0m 11s] Epoch 51 [5120/8158] loss=0.0019503964053001256\n",
            "[0m 11s] Epoch 51 [7680/8158] loss=0.0019581564934924244\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7950105397341978\n",
            "[0m 12s] Epoch 52 [2560/8158] loss=0.0020001700147986414\n",
            "[0m 12s] Epoch 52 [5120/8158] loss=0.001986443530768156\n",
            "[0m 12s] Epoch 52 [7680/8158] loss=0.0019668618876797456\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dec set: AUC  0.795056653045342\n",
            "[0m 12s] Epoch 53 [2560/8158] loss=0.001964338112156838\n",
            "[0m 12s] Epoch 53 [5120/8158] loss=0.001954775571357459\n",
            "[0m 12s] Epoch 53 [7680/8158] loss=0.0019530965015292167\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7951097149376175\n",
            "[0m 12s] Epoch 54 [2560/8158] loss=0.0019248157157562673\n",
            "[0m 12s] Epoch 54 [5120/8158] loss=0.0019600438536144793\n",
            "[0m 12s] Epoch 54 [7680/8158] loss=0.0019607894510651628\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7952581618981502\n",
            "[0m 12s] Epoch 55 [2560/8158] loss=0.0019319083308801054\n",
            "[0m 12s] Epoch 55 [5120/8158] loss=0.0019882080086972564\n",
            "[0m 12s] Epoch 55 [7680/8158] loss=0.001951946159048627\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1967/2658 74.00%\n",
            "Dec set: AUC  0.7951381409513365\n",
            "[0m 12s] Epoch 56 [2560/8158] loss=0.001975609257351607\n",
            "[0m 13s] Epoch 56 [5120/8158] loss=0.0019792102102655917\n",
            "[0m 13s] Epoch 56 [7680/8158] loss=0.00195979771281903\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7949429489904659\n",
            "[0m 13s] Epoch 57 [2560/8158] loss=0.0019109499524347484\n",
            "[0m 13s] Epoch 57 [5120/8158] loss=0.0019259174121543765\n",
            "[0m 13s] Epoch 57 [7680/8158] loss=0.0019480647053569556\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7952057316950685\n",
            "[0m 13s] Epoch 58 [2560/8158] loss=0.0019518295535817741\n",
            "[0m 13s] Epoch 58 [5120/8158] loss=0.0019612187927123157\n",
            "[0m 13s] Epoch 58 [7680/8158] loss=0.001943746030641099\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7952834294659005\n",
            "[0m 13s] Epoch 59 [2560/8158] loss=0.0019018716760911048\n",
            "[0m 13s] Epoch 59 [5120/8158] loss=0.0019486985984258354\n",
            "[0m 13s] Epoch 59 [7680/8158] loss=0.0019473892481376728\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7952461598034689\n",
            "[0m 13s] Epoch 60 [2560/8158] loss=0.0018979777116328478\n",
            "[0m 13s] Epoch 60 [5120/8158] loss=0.001955757464747876\n",
            "[0m 14s] Epoch 60 [7680/8158] loss=0.001952576901142796\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7953889215612578\n",
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.0025960405822843314\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.0025436289608478544\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.002502564216653506\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7534384422039131\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.002394407638348639\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.0023493525572121144\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.002286452621531983\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1905/2658 71.67%\n",
            "Dec set: AUC  0.756798397025502\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.0021565024158917367\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.0021231059450656175\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.0020937765521618227\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1937/2658 72.87%\n",
            "Dec set: AUC  0.7828391523489046\n",
            "[0m 0s] Epoch 4 [2560/8158] loss=0.002024249802343547\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.0020467179478146137\n",
            "[0m 1s] Epoch 4 [7680/8158] loss=0.0020289087357620397\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1944/2658 73.14%\n",
            "Dec set: AUC  0.7888465165815256\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.002013295330107212\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.002016657602507621\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.002018145731805513\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dec set: AUC  0.7886601682693675\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.0020945107797160746\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.002008216019021347\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.0020063470272968214\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.78994376071108\n",
            "[0m 1s] Epoch 7 [2560/8158] loss=0.00198109612101689\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.001980440632905811\n",
            "[0m 1s] Epoch 7 [7680/8158] loss=0.0019954491794730226\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7893802939502494\n",
            "[0m 1s] Epoch 8 [2560/8158] loss=0.001938555180095136\n",
            "[0m 1s] Epoch 8 [5120/8158] loss=0.001971636066446081\n",
            "[0m 2s] Epoch 8 [7680/8158] loss=0.001996145013254136\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7895559035461137\n",
            "[0m 2s] Epoch 9 [2560/8158] loss=0.0019686216488480567\n",
            "[0m 2s] Epoch 9 [5120/8158] loss=0.0019635526812635364\n",
            "[0m 2s] Epoch 9 [7680/8158] loss=0.0019907006450618305\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1959/2658 73.70%\n",
            "Dec set: AUC  0.7894592550994689\n",
            "[0m 2s] Epoch 10 [2560/8158] loss=0.0019962356658652426\n",
            "[0m 2s] Epoch 10 [5120/8158] loss=0.002014412172138691\n",
            "[0m 2s] Epoch 10 [7680/8158] loss=0.0020003394301359853\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.789521476485054\n",
            "[0m 2s] Epoch 11 [2560/8158] loss=0.0020408681826665997\n",
            "[0m 2s] Epoch 11 [5120/8158] loss=0.0020090998033992944\n",
            "[0m 2s] Epoch 11 [7680/8158] loss=0.001989742888448139\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7898477439536289\n",
            "[0m 2s] Epoch 12 [2560/8158] loss=0.001990282372571528\n",
            "[0m 2s] Epoch 12 [5120/8158] loss=0.00197754543623887\n",
            "[0m 3s] Epoch 12 [7680/8158] loss=0.001986567059066147\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.789767519426022\n",
            "[0m 3s] Epoch 13 [2560/8158] loss=0.001979580207262188\n",
            "[0m 3s] Epoch 13 [5120/8158] loss=0.0019762637268286197\n",
            "[0m 3s] Epoch 13 [7680/8158] loss=0.001977440012463679\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7901282139556567\n",
            "[0m 3s] Epoch 14 [2560/8158] loss=0.0019814720610156654\n",
            "[0m 3s] Epoch 14 [5120/8158] loss=0.001983219594694674\n",
            "[0m 3s] Epoch 14 [7680/8158] loss=0.001989467624419679\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7903897332818717\n",
            "[0m 3s] Epoch 15 [2560/8158] loss=0.001998401340097189\n",
            "[0m 3s] Epoch 15 [5120/8158] loss=0.001973837672267109\n",
            "[0m 3s] Epoch 15 [7680/8158] loss=0.001982160032882045\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dec set: AUC  0.7904276346334971\n",
            "[0m 3s] Epoch 16 [2560/8158] loss=0.0019739665207453073\n",
            "[0m 3s] Epoch 16 [5120/8158] loss=0.001989132596645504\n",
            "[0m 4s] Epoch 16 [7680/8158] loss=0.0019791399439175925\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dec set: AUC  0.7909014015288143\n",
            "[0m 4s] Epoch 17 [2560/8158] loss=0.0019947631866671146\n",
            "[0m 4s] Epoch 17 [5120/8158] loss=0.0019511865393724293\n",
            "[0m 4s] Epoch 17 [7680/8158] loss=0.001976200418236355\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7911584990306729\n",
            "[0m 4s] Epoch 18 [2560/8158] loss=0.0019796852837316694\n",
            "[0m 4s] Epoch 18 [5120/8158] loss=0.001969752396689728\n",
            "[0m 4s] Epoch 18 [7680/8158] loss=0.0019694555861254535\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7915299322766015\n",
            "[0m 4s] Epoch 19 [2560/8158] loss=0.0019717618823051454\n",
            "[0m 4s] Epoch 19 [5120/8158] loss=0.0019704462902154773\n",
            "[0m 4s] Epoch 19 [7680/8158] loss=0.001968224257385979\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7918040853866919\n",
            "[0m 5s] Epoch 20 [2560/8158] loss=0.0019811147009022533\n",
            "[0m 5s] Epoch 20 [5120/8158] loss=0.0019820365880150346\n",
            "[0m 5s] Epoch 20 [7680/8158] loss=0.0019832184732270735\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7919361084281867\n",
            "[0m 5s] Epoch 21 [2560/8158] loss=0.0019562361179850996\n",
            "[0m 5s] Epoch 21 [5120/8158] loss=0.001975151553051546\n",
            "[0m 5s] Epoch 21 [7680/8158] loss=0.001965329881447057\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7923972415396286\n",
            "[0m 5s] Epoch 22 [2560/8158] loss=0.0019199722330085934\n",
            "[0m 5s] Epoch 22 [5120/8158] loss=0.001963489537592977\n",
            "[0m 5s] Epoch 22 [7680/8158] loss=0.001967972683875511\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1977/2658 74.38%\n",
            "Dec set: AUC  0.7925248427567674\n",
            "[0m 5s] Epoch 23 [2560/8158] loss=0.0019180132541805506\n",
            "[0m 6s] Epoch 23 [5120/8158] loss=0.001962329482194036\n",
            "[0m 6s] Epoch 23 [7680/8158] loss=0.001969699612042556\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7929550230977154\n",
            "[0m 6s] Epoch 24 [2560/8158] loss=0.0019285636604763567\n",
            "[0m 6s] Epoch 24 [5120/8158] loss=0.0019436775124631821\n",
            "[0m 6s] Epoch 24 [7680/8158] loss=0.0019596610257091624\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.793253812086362\n",
            "[0m 6s] Epoch 25 [2560/8158] loss=0.0019121169461868703\n",
            "[0m 6s] Epoch 25 [5120/8158] loss=0.0019588182447478177\n",
            "[0m 6s] Epoch 25 [7680/8158] loss=0.0019560030040641626\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7935121729666084\n",
            "[0m 6s] Epoch 26 [2560/8158] loss=0.001956090610474348\n",
            "[0m 6s] Epoch 26 [5120/8158] loss=0.0019485299650114029\n",
            "[0m 6s] Epoch 26 [7680/8158] loss=0.0019681901632187266\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7936290354674531\n",
            "[0m 7s] Epoch 27 [2560/8158] loss=0.0019143043784424663\n",
            "[0m 7s] Epoch 27 [5120/8158] loss=0.00193895508418791\n",
            "[0m 7s] Epoch 27 [7680/8158] loss=0.0019628011505119503\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7938330710770364\n",
            "[0m 7s] Epoch 28 [2560/8158] loss=0.0019952233182266355\n",
            "[0m 7s] Epoch 28 [5120/8158] loss=0.0019620747887529435\n",
            "[0m 7s] Epoch 28 [7680/8158] loss=0.0019582007468367615\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7938128570228362\n",
            "[0m 7s] Epoch 29 [2560/8158] loss=0.001950939674861729\n",
            "[0m 7s] Epoch 29 [5120/8158] loss=0.0019776298315264283\n",
            "[0m 7s] Epoch 29 [7680/8158] loss=0.001967236085329205\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7940459503353322\n",
            "[0m 7s] Epoch 30 [2560/8158] loss=0.0019747221609577536\n",
            "[0m 7s] Epoch 30 [5120/8158] loss=0.0019502529175952077\n",
            "[0m 7s] Epoch 30 [7680/8158] loss=0.0019720792304724457\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.794100907295189\n",
            "[0m 8s] Epoch 31 [2560/8158] loss=0.0019424981088377534\n",
            "[0m 8s] Epoch 31 [5120/8158] loss=0.0019601709209382534\n",
            "[0m 8s] Epoch 31 [7680/8158] loss=0.00195930606763189\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7942269292893434\n",
            "[0m 8s] Epoch 32 [2560/8158] loss=0.0019387546926736832\n",
            "[0m 8s] Epoch 32 [5120/8158] loss=0.00193715775385499\n",
            "[0m 8s] Epoch 32 [7680/8158] loss=0.0019485778679760794\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7942619880395968\n",
            "[0m 8s] Epoch 33 [2560/8158] loss=0.001925679005216807\n",
            "[0m 8s] Epoch 33 [5120/8158] loss=0.0019459555158391594\n",
            "[0m 8s] Epoch 33 [7680/8158] loss=0.0019623712946971257\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7942885189857346\n",
            "[0m 8s] Epoch 34 [2560/8158] loss=0.0019597087521106005\n",
            "[0m 8s] Epoch 34 [5120/8158] loss=0.0019801623304374514\n",
            "[0m 9s] Epoch 34 [7680/8158] loss=0.0019601973355747758\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7942133479716776\n",
            "[0m 9s] Epoch 35 [2560/8158] loss=0.001949049229733646\n",
            "[0m 9s] Epoch 35 [5120/8158] loss=0.0019469380320515483\n",
            "[0m 9s] Epoch 35 [7680/8158] loss=0.0019594059752610824\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7944647602707925\n",
            "[0m 9s] Epoch 36 [2560/8158] loss=0.00199948123190552\n",
            "[0m 9s] Epoch 36 [5120/8158] loss=0.0019772761734202504\n",
            "[0m 9s] Epoch 36 [7680/8158] loss=0.00196489121299237\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7947054338536137\n",
            "[0m 9s] Epoch 37 [2560/8158] loss=0.0019366349908523262\n",
            "[0m 9s] Epoch 37 [5120/8158] loss=0.001923078385880217\n",
            "[0m 9s] Epoch 37 [7680/8158] loss=0.0019406181022835274\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7949353687201408\n",
            "[0m 9s] Epoch 38 [2560/8158] loss=0.001938335900194943\n",
            "[0m 10s] Epoch 38 [5120/8158] loss=0.0019386153144296259\n",
            "[0m 10s] Epoch 38 [7680/8158] loss=0.001961616838040451\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7949157863551343\n",
            "[0m 10s] Epoch 39 [2560/8158] loss=0.0019589157891459763\n",
            "[0m 10s] Epoch 39 [5120/8158] loss=0.001979962381301448\n",
            "[0m 10s] Epoch 39 [7680/8158] loss=0.001963067512648801\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7951116100051988\n",
            "[0m 10s] Epoch 40 [2560/8158] loss=0.0020011937478557227\n",
            "[0m 10s] Epoch 40 [5120/8158] loss=0.0019536704348865896\n",
            "[0m 10s] Epoch 40 [7680/8158] loss=0.0019497900075900057\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dec set: AUC  0.7950888691942237\n",
            "[0m 10s] Epoch 41 [2560/8158] loss=0.0019355918397195638\n",
            "[0m 10s] Epoch 41 [5120/8158] loss=0.00196421408909373\n",
            "[0m 10s] Epoch 41 [7680/8158] loss=0.0019443354530570408\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2009/2658 75.58%\n",
            "Dec set: AUC  0.795303011830907\n",
            "[0m 11s] Epoch 42 [2560/8158] loss=0.00199004327878356\n",
            "[0m 11s] Epoch 42 [5120/8158] loss=0.001966195652494207\n",
            "[0m 11s] Epoch 42 [7680/8158] loss=0.0019557875425865253\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7954154525073955\n",
            "[0m 11s] Epoch 43 [2560/8158] loss=0.0019535741303116083\n",
            "[0m 11s] Epoch 43 [5120/8158] loss=0.0019523119379300624\n",
            "[0m 11s] Epoch 43 [7680/8158] loss=0.0019567793933674693\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7951255071674614\n",
            "[0m 11s] Epoch 44 [2560/8158] loss=0.001955671398900449\n",
            "[0m 11s] Epoch 44 [5120/8158] loss=0.0019341665436513722\n",
            "[0m 11s] Epoch 44 [7680/8158] loss=0.0019439352133000891\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1965/2658 73.93%\n",
            "Dec set: AUC  0.7953086970336507\n",
            "[0m 11s] Epoch 45 [2560/8158] loss=0.0019829648896120488\n",
            "[0m 11s] Epoch 45 [5120/8158] loss=0.0019471055129542947\n",
            "[0m 12s] Epoch 45 [7680/8158] loss=0.001961410235768805\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7954666193320898\n",
            "[0m 12s] Epoch 46 [2560/8158] loss=0.001957194285932928\n",
            "[0m 12s] Epoch 46 [5120/8158] loss=0.0019533128943294287\n",
            "[0m 12s] Epoch 46 [7680/8158] loss=0.0019614169917379818\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7956479141306978\n",
            "[0m 12s] Epoch 47 [2560/8158] loss=0.0019433058798313141\n",
            "[0m 12s] Epoch 47 [5120/8158] loss=0.0019432835455518217\n",
            "[0m 12s] Epoch 47 [7680/8158] loss=0.0019469199702143668\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7952562668305689\n",
            "[0m 12s] Epoch 48 [2560/8158] loss=0.001989270281046629\n",
            "[0m 12s] Epoch 48 [5120/8158] loss=0.0019524721428751945\n",
            "[0m 12s] Epoch 48 [7680/8158] loss=0.0019522189356697103\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.795694027441842\n",
            "[0m 12s] Epoch 49 [2560/8158] loss=0.001941399835050106\n",
            "[0m 12s] Epoch 49 [5120/8158] loss=0.0019467362144496292\n",
            "[0m 13s] Epoch 49 [7680/8158] loss=0.0019563992624171077\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7956081177114913\n",
            "[0m 13s] Epoch 50 [2560/8158] loss=0.0019685137667693198\n",
            "[0m 13s] Epoch 50 [5120/8158] loss=0.001956830220296979\n",
            "[0m 13s] Epoch 50 [7680/8158] loss=0.0019460026058368384\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7955139960216214\n",
            "[0m 13s] Epoch 51 [2560/8158] loss=0.001909909280948341\n",
            "[0m 13s] Epoch 51 [5120/8158] loss=0.0019219509151298553\n",
            "[0m 13s] Epoch 51 [7680/8158] loss=0.001942258037161082\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7960708300459174\n",
            "[0m 13s] Epoch 52 [2560/8158] loss=0.0019159208051860333\n",
            "[0m 13s] Epoch 52 [5120/8158] loss=0.0019591678283177316\n",
            "[0m 13s] Epoch 52 [7680/8158] loss=0.001945935453598698\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dec set: AUC  0.7960926233231022\n",
            "[0m 13s] Epoch 53 [2560/8158] loss=0.0019176075817085802\n",
            "[0m 14s] Epoch 53 [5120/8158] loss=0.0019366811553481965\n",
            "[0m 14s] Epoch 53 [7680/8158] loss=0.0019433346150132516\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1971/2658 74.15%\n",
            "Dec set: AUC  0.7959700756195133\n",
            "[0m 14s] Epoch 54 [2560/8158] loss=0.001975143724121153\n",
            "[0m 14s] Epoch 54 [5120/8158] loss=0.0019454975263215601\n",
            "[0m 14s] Epoch 54 [7680/8158] loss=0.001952305668964982\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7960989402150396\n",
            "[0m 14s] Epoch 55 [2560/8158] loss=0.0019016669946722686\n",
            "[0m 14s] Epoch 55 [5120/8158] loss=0.0019299850566312671\n",
            "[0m 14s] Epoch 55 [7680/8158] loss=0.0019499056157656013\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7963528792709296\n",
            "[0m 14s] Epoch 56 [2560/8158] loss=0.00196596784517169\n",
            "[0m 14s] Epoch 56 [5120/8158] loss=0.0019322937878314406\n",
            "[0m 14s] Epoch 56 [7680/8158] loss=0.0019404889239619176\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7960521952147017\n",
            "[0m 14s] Epoch 57 [2560/8158] loss=0.001934159582015127\n",
            "[0m 14s] Epoch 57 [5120/8158] loss=0.0019380169513169676\n",
            "[0m 15s] Epoch 57 [7680/8158] loss=0.0019480809996215006\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7964002559604614\n",
            "[0m 15s] Epoch 58 [2560/8158] loss=0.0019168121973052621\n",
            "[0m 15s] Epoch 58 [5120/8158] loss=0.0019212742394302041\n",
            "[0m 15s] Epoch 58 [7680/8158] loss=0.0019459314838362236\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7958538448078623\n",
            "[0m 15s] Epoch 59 [2560/8158] loss=0.0019644107902422546\n",
            "[0m 15s] Epoch 59 [5120/8158] loss=0.0019605590554419906\n",
            "[0m 15s] Epoch 59 [7680/8158] loss=0.0019485334865748881\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7958652152133499\n",
            "[0m 15s] Epoch 60 [2560/8158] loss=0.0018860434181988239\n",
            "[0m 15s] Epoch 60 [5120/8158] loss=0.001929542951984331\n",
            "[0m 15s] Epoch 60 [7680/8158] loss=0.001956833891260127\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7962246463645971\n",
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.002592425816692412\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.002543061214964837\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.0025170794843385616\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7467602240475232\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.0024316458497196437\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.0024265432031825183\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.0023369303749253352\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1884/2658 70.88%\n",
            "Dec set: AUC  0.748708353521067\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.0021415625466033815\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.00212780392030254\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.0021050615430188674\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1960/2658 73.74%\n",
            "Dec set: AUC  0.7804829516561944\n",
            "[0m 0s] Epoch 4 [2560/8158] loss=0.0020475031109526754\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.002024761843495071\n",
            "[0m 1s] Epoch 4 [7680/8158] loss=0.002032670374804487\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1974/2658 74.27%\n",
            "Dec set: AUC  0.7825757379551084\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.0019502192735671997\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.0019992494897451253\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.0020059246259431043\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1945/2658 73.18%\n",
            "Dec set: AUC  0.7891724682055038\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.002001940633635968\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.0020081703318282963\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.002005005076838036\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1974/2658 74.27%\n",
            "Dec set: AUC  0.7883332691115986\n",
            "[0m 1s] Epoch 7 [2560/8158] loss=0.0020443952642381193\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.0020242333528585734\n",
            "[0m 1s] Epoch 7 [7680/8158] loss=0.001995132638451954\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7892097378679352\n",
            "[0m 2s] Epoch 8 [2560/8158] loss=0.002000983420293778\n",
            "[0m 2s] Epoch 8 [5120/8158] loss=0.0020078048459254206\n",
            "[0m 2s] Epoch 8 [7680/8158] loss=0.002009501785505563\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7890783465156341\n",
            "[0m 2s] Epoch 9 [2560/8158] loss=0.00196206500986591\n",
            "[0m 2s] Epoch 9 [5120/8158] loss=0.001990290667163208\n",
            "[0m 2s] Epoch 9 [7680/8158] loss=0.002000125532504171\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7894118784099372\n",
            "[0m 2s] Epoch 10 [2560/8158] loss=0.001964477589353919\n",
            "[0m 2s] Epoch 10 [5120/8158] loss=0.002009675867157057\n",
            "[0m 2s] Epoch 10 [7680/8158] loss=0.0019877917249687014\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7899033326026795\n",
            "[0m 2s] Epoch 11 [2560/8158] loss=0.002082072151824832\n",
            "[0m 3s] Epoch 11 [5120/8158] loss=0.0020078640198335053\n",
            "[0m 3s] Epoch 11 [7680/8158] loss=0.00198930335463956\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.790290558078452\n",
            "[0m 3s] Epoch 12 [2560/8158] loss=0.0020460453350096943\n",
            "[0m 3s] Epoch 12 [5120/8158] loss=0.0019994958012830464\n",
            "[0m 3s] Epoch 12 [7680/8158] loss=0.001971974146241943\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7904247920321252\n",
            "[0m 3s] Epoch 13 [2560/8158] loss=0.0019870796822942793\n",
            "[0m 3s] Epoch 13 [5120/8158] loss=0.0019940944854170083\n",
            "[0m 3s] Epoch 13 [7680/8158] loss=0.0019725056132301687\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7908369692310512\n",
            "[0m 3s] Epoch 14 [2560/8158] loss=0.0020038495655171572\n",
            "[0m 3s] Epoch 14 [5120/8158] loss=0.001959871774306521\n",
            "[0m 3s] Epoch 14 [7680/8158] loss=0.0019760624893630546\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7909746774752898\n",
            "[0m 4s] Epoch 15 [2560/8158] loss=0.0019152105785906314\n",
            "[0m 4s] Epoch 15 [5120/8158] loss=0.0019517062988597899\n",
            "[0m 4s] Epoch 15 [7680/8158] loss=0.0019713960044706862\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1976/2658 74.34%\n",
            "Dec set: AUC  0.791811033967823\n",
            "[0m 4s] Epoch 16 [2560/8158] loss=0.0019483668147586286\n",
            "[0m 4s] Epoch 16 [5120/8158] loss=0.0019878642342519015\n",
            "[0m 4s] Epoch 16 [7680/8158] loss=0.0019772305075700086\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.792075080050813\n",
            "[0m 4s] Epoch 17 [2560/8158] loss=0.0019691095920279624\n",
            "[0m 4s] Epoch 17 [5120/8158] loss=0.001952194864861667\n",
            "[0m 4s] Epoch 17 [7680/8158] loss=0.0019688722599918644\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7925267378243488\n",
            "[0m 4s] Epoch 18 [2560/8158] loss=0.001959876879118383\n",
            "[0m 4s] Epoch 18 [5120/8158] loss=0.0019566831877455117\n",
            "[0m 5s] Epoch 18 [7680/8158] loss=0.00196221845690161\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7925747462030742\n",
            "[0m 5s] Epoch 19 [2560/8158] loss=0.0019638045225292442\n",
            "[0m 5s] Epoch 19 [5120/8158] loss=0.0019677995354868472\n",
            "[0m 5s] Epoch 19 [7680/8158] loss=0.0019669881594988206\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7930352476253223\n",
            "[0m 5s] Epoch 20 [2560/8158] loss=0.0019849288859404624\n",
            "[0m 5s] Epoch 20 [5120/8158] loss=0.0019660580030176787\n",
            "[0m 5s] Epoch 20 [7680/8158] loss=0.0019753641992186505\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1976/2658 74.34%\n",
            "Dec set: AUC  0.7929727103951407\n",
            "[0m 5s] Epoch 21 [2560/8158] loss=0.0019018526189029216\n",
            "[0m 5s] Epoch 21 [5120/8158] loss=0.001970903860637918\n",
            "[0m 5s] Epoch 21 [7680/8158] loss=0.001968040371624132\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.793380781614307\n",
            "[0m 6s] Epoch 22 [2560/8158] loss=0.0019803653005510567\n",
            "[0m 6s] Epoch 22 [5120/8158] loss=0.001950143271824345\n",
            "[0m 6s] Epoch 22 [7680/8158] loss=0.001958087005186826\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7935664982372712\n",
            "[0m 6s] Epoch 23 [2560/8158] loss=0.002007036958821118\n",
            "[0m 6s] Epoch 23 [5120/8158] loss=0.001951665140222758\n",
            "[0m 6s] Epoch 23 [7680/8158] loss=0.001966880801289032\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.793800223238961\n",
            "[0m 6s] Epoch 24 [2560/8158] loss=0.001947044231928885\n",
            "[0m 6s] Epoch 24 [5120/8158] loss=0.001970654190517962\n",
            "[0m 6s] Epoch 24 [7680/8158] loss=0.0019648703630082308\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7939511969562688\n",
            "[0m 7s] Epoch 25 [2560/8158] loss=0.0019640693557448686\n",
            "[0m 7s] Epoch 25 [5120/8158] loss=0.001981499732937664\n",
            "[0m 7s] Epoch 25 [7680/8158] loss=0.00197182292273889\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7940440552677509\n",
            "[0m 7s] Epoch 26 [2560/8158] loss=0.0019847323768772186\n",
            "[0m 7s] Epoch 26 [5120/8158] loss=0.0019711819535586984\n",
            "[0m 7s] Epoch 26 [7680/8158] loss=0.001960525580216199\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7943264203373599\n",
            "[0m 7s] Epoch 27 [2560/8158] loss=0.0019980504177510737\n",
            "[0m 7s] Epoch 27 [5120/8158] loss=0.001938116573728621\n",
            "[0m 7s] Epoch 27 [7680/8158] loss=0.001952882487482081\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dec set: AUC  0.7943965378378669\n",
            "[0m 8s] Epoch 28 [2560/8158] loss=0.0020042730262503027\n",
            "[0m 8s] Epoch 28 [5120/8158] loss=0.001947612181538716\n",
            "[0m 8s] Epoch 28 [7680/8158] loss=0.001964484186222156\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7945563552038872\n",
            "[0m 8s] Epoch 29 [2560/8158] loss=0.0019290557596832515\n",
            "[0m 8s] Epoch 29 [5120/8158] loss=0.0019571231328882276\n",
            "[0m 8s] Epoch 29 [7680/8158] loss=0.001963896525558084\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dec set: AUC  0.7945367728388807\n",
            "[0m 8s] Epoch 30 [2560/8158] loss=0.00194607675075531\n",
            "[0m 8s] Epoch 30 [5120/8158] loss=0.0019527187047060578\n",
            "[0m 8s] Epoch 30 [7680/8158] loss=0.0019588659090610844\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7943333689184913\n",
            "[0m 8s] Epoch 31 [2560/8158] loss=0.0019483341951854528\n",
            "[0m 8s] Epoch 31 [5120/8158] loss=0.0019478987785987556\n",
            "[0m 8s] Epoch 31 [7680/8158] loss=0.001961037578682105\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7949517926391786\n",
            "[0m 9s] Epoch 32 [2560/8158] loss=0.0019348453730344771\n",
            "[0m 9s] Epoch 32 [5120/8158] loss=0.001970399619312957\n",
            "[0m 9s] Epoch 32 [7680/8158] loss=0.0019508191694815953\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7949315785849783\n",
            "[0m 9s] Epoch 33 [2560/8158] loss=0.0019491098588332533\n",
            "[0m 9s] Epoch 33 [5120/8158] loss=0.0019758326234295964\n",
            "[0m 9s] Epoch 33 [7680/8158] loss=0.00196120358693103\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dec set: AUC  0.7949176814227157\n",
            "[0m 9s] Epoch 34 [2560/8158] loss=0.00191056887852028\n",
            "[0m 9s] Epoch 34 [5120/8158] loss=0.001946302648866549\n",
            "[0m 9s] Epoch 34 [7680/8158] loss=0.0019502772600390017\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7951956246679684\n",
            "[0m 9s] Epoch 35 [2560/8158] loss=0.0019320975989103317\n",
            "[0m 10s] Epoch 35 [5120/8158] loss=0.001964640012010932\n",
            "[0m 10s] Epoch 35 [7680/8158] loss=0.001953266828786582\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7949378954769157\n",
            "[0m 10s] Epoch 36 [2560/8158] loss=0.001951993617694825\n",
            "[0m 10s] Epoch 36 [5120/8158] loss=0.0019508517405483871\n",
            "[0m 10s] Epoch 36 [7680/8158] loss=0.0019586606494461497\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7952461598034689\n",
            "[0m 10s] Epoch 37 [2560/8158] loss=0.0020121691166423263\n",
            "[0m 10s] Epoch 37 [5120/8158] loss=0.001976233092136681\n",
            "[0m 10s] Epoch 37 [7680/8158] loss=0.0019501716946251691\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2012/2658 75.70%\n",
            "Dec set: AUC  0.7950238052072667\n",
            "[0m 10s] Epoch 38 [2560/8158] loss=0.0019237928558140994\n",
            "[0m 10s] Epoch 38 [5120/8158] loss=0.001954097847919911\n",
            "[0m 11s] Epoch 38 [7680/8158] loss=0.0019572068316241107\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7953143822363946\n",
            "[0m 11s] Epoch 39 [2560/8158] loss=0.0020090149249881507\n",
            "[0m 11s] Epoch 39 [5120/8158] loss=0.0019738689705263825\n",
            "[0m 11s] Epoch 39 [7680/8158] loss=0.0019574676873162387\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7953687075070576\n",
            "[0m 11s] Epoch 40 [2560/8158] loss=0.0019547252100892364\n",
            "[0m 11s] Epoch 40 [5120/8158] loss=0.0019689521752297877\n",
            "[0m 11s] Epoch 40 [7680/8158] loss=0.0019454102652768294\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7954457735886958\n",
            "[0m 11s] Epoch 41 [2560/8158] loss=0.0019548746291548013\n",
            "[0m 11s] Epoch 41 [5120/8158] loss=0.0019631525850854815\n",
            "[0m 11s] Epoch 41 [7680/8158] loss=0.0019676663757612306\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7958784806864188\n",
            "[0m 11s] Epoch 42 [2560/8158] loss=0.002018542273435742\n",
            "[0m 12s] Epoch 42 [5120/8158] loss=0.0019411426794249565\n",
            "[0m 12s] Epoch 42 [7680/8158] loss=0.0019537448960666856\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.795735087239436\n",
            "[0m 12s] Epoch 43 [2560/8158] loss=0.0019753098487854002\n",
            "[0m 12s] Epoch 43 [5120/8158] loss=0.0019838205946143715\n",
            "[0m 12s] Epoch 43 [7680/8158] loss=0.0019490072154439986\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7955752698734158\n",
            "[0m 12s] Epoch 44 [2560/8158] loss=0.0019215183448977769\n",
            "[0m 12s] Epoch 44 [5120/8158] loss=0.0019666562904603778\n",
            "[0m 12s] Epoch 44 [7680/8158] loss=0.0019503612342911462\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7954078722370704\n",
            "[0m 12s] Epoch 45 [2560/8158] loss=0.0019079481833614409\n",
            "[0m 12s] Epoch 45 [5120/8158] loss=0.0019336254976224155\n",
            "[0m 12s] Epoch 45 [7680/8158] loss=0.0019419264242363473\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.7958538448078623\n",
            "[0m 13s] Epoch 46 [2560/8158] loss=0.0020104907336644827\n",
            "[0m 13s] Epoch 46 [5120/8158] loss=0.001964633463649079\n",
            "[0m 13s] Epoch 46 [7680/8158] loss=0.0019516434559288124\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7954539855482147\n",
            "[0m 13s] Epoch 47 [2560/8158] loss=0.001906143710948527\n",
            "[0m 13s] Epoch 47 [5120/8158] loss=0.0019284966634586453\n",
            "[0m 13s] Epoch 47 [7680/8158] loss=0.0019445644419950744\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7959801826466135\n",
            "[0m 13s] Epoch 48 [2560/8158] loss=0.0019833094789646565\n",
            "[0m 13s] Epoch 48 [5120/8158] loss=0.0019820743647869675\n",
            "[0m 13s] Epoch 48 [7680/8158] loss=0.0019517149970245857\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7956930799080513\n",
            "[0m 13s] Epoch 49 [2560/8158] loss=0.0019793165381997825\n",
            "[0m 14s] Epoch 49 [5120/8158] loss=0.0019439473515376448\n",
            "[0m 14s] Epoch 49 [7680/8158] loss=0.001948512950912118\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7960623022418019\n",
            "[0m 14s] Epoch 50 [2560/8158] loss=0.0019539653440006077\n",
            "[0m 14s] Epoch 50 [5120/8158] loss=0.0019873960642144083\n",
            "[0m 14s] Epoch 50 [7680/8158] loss=0.0019566271803341806\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7957243485231423\n",
            "[0m 14s] Epoch 51 [2560/8158] loss=0.0020011485554277896\n",
            "[0m 14s] Epoch 51 [5120/8158] loss=0.0019722658442333342\n",
            "[0m 14s] Epoch 51 [7680/8158] loss=0.0019486386716986695\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7956384387927914\n",
            "[0m 14s] Epoch 52 [2560/8158] loss=0.0018831148277968167\n",
            "[0m 14s] Epoch 52 [5120/8158] loss=0.0019339362159371376\n",
            "[0m 15s] Epoch 52 [7680/8158] loss=0.0019549613430475195\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7959972382548449\n",
            "[0m 15s] Epoch 53 [2560/8158] loss=0.0018917152774520218\n",
            "[0m 15s] Epoch 53 [5120/8158] loss=0.0019391360692679882\n",
            "[0m 15s] Epoch 53 [7680/8158] loss=0.0019408231950365008\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dec set: AUC  0.7960730409580956\n",
            "[0m 15s] Epoch 54 [2560/8158] loss=0.001924948301166296\n",
            "[0m 15s] Epoch 54 [5120/8158] loss=0.0019436062895692885\n",
            "[0m 15s] Epoch 54 [7680/8158] loss=0.0019450312635550897\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7961595823776403\n",
            "[0m 15s] Epoch 55 [2560/8158] loss=0.0019182749674655497\n",
            "[0m 15s] Epoch 55 [5120/8158] loss=0.001944068179000169\n",
            "[0m 15s] Epoch 55 [7680/8158] loss=0.0019610326504334807\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dec set: AUC  0.7961463169045714\n",
            "[0m 16s] Epoch 56 [2560/8158] loss=0.00198418915970251\n",
            "[0m 16s] Epoch 56 [5120/8158] loss=0.0019451715343166143\n",
            "[0m 16s] Epoch 56 [7680/8158] loss=0.0019489423371851443\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.7959005898082002\n",
            "[0m 16s] Epoch 57 [2560/8158] loss=0.0018932620878331363\n",
            "[0m 16s] Epoch 57 [5120/8158] loss=0.001930662803351879\n",
            "[0m 16s] Epoch 57 [7680/8158] loss=0.0019549998842800656\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.795639702171179\n",
            "[0m 16s] Epoch 58 [2560/8158] loss=0.0019103725207969546\n",
            "[0m 16s] Epoch 58 [5120/8158] loss=0.0019246738578658552\n",
            "[0m 16s] Epoch 58 [7680/8158] loss=0.0019483996322378517\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7953465983852761\n",
            "[0m 17s] Epoch 59 [2560/8158] loss=0.0019137391471304\n",
            "[0m 17s] Epoch 59 [5120/8158] loss=0.001941352227004245\n",
            "[0m 17s] Epoch 59 [7680/8158] loss=0.0019417331321164965\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7956188564277851\n",
            "[0m 17s] Epoch 60 [2560/8158] loss=0.0019193948712199927\n",
            "[0m 17s] Epoch 60 [5120/8158] loss=0.0019464388431515544\n",
            "[0m 17s] Epoch 60 [7680/8158] loss=0.001946939768580099\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7953004850741319\n",
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.002611728943884373\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.002572186815086752\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.002532716623196999\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7393043964936198\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.002464353246614337\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.002426785964053124\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.0023753185446063678\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1880/2658 70.73%\n",
            "Dec set: AUC  0.7407888660989466\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.002217134926468134\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.002159181231399998\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.0021217603352852166\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1935/2658 72.80%\n",
            "Dec set: AUC  0.7731585154545919\n",
            "[0m 1s] Epoch 4 [2560/8158] loss=0.002033317112363875\n",
            "[0m 1s] Epoch 4 [5120/8158] loss=0.002046083228196949\n",
            "[0m 1s] Epoch 4 [7680/8158] loss=0.0020561228545072177\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1923/2658 72.35%\n",
            "Dec set: AUC  0.7887713455674686\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.0020461602136492727\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.0020156481594312938\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.002027693580991278\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7885799437417605\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.0020349632832221687\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.002028286410495639\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.002013898175209761\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1960/2658 73.74%\n",
            "Dec set: AUC  0.7888427264463629\n",
            "[0m 2s] Epoch 7 [2560/8158] loss=0.0020320031442679467\n",
            "[0m 2s] Epoch 7 [5120/8158] loss=0.0020117348758503795\n",
            "[0m 2s] Epoch 7 [7680/8158] loss=0.001992965939765175\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1964/2658 73.89%\n",
            "Dec set: AUC  0.788311475834414\n",
            "[0m 2s] Epoch 8 [2560/8158] loss=0.002041866979561746\n",
            "[0m 2s] Epoch 8 [5120/8158] loss=0.002001327951438725\n",
            "[0m 2s] Epoch 8 [7680/8158] loss=0.0019954954429219165\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7882742061719824\n",
            "[0m 2s] Epoch 9 [2560/8158] loss=0.002011053531896323\n",
            "[0m 2s] Epoch 9 [5120/8158] loss=0.0019983177829999478\n",
            "[0m 2s] Epoch 9 [7680/8158] loss=0.0019830401715201637\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7893379707742678\n",
            "[0m 2s] Epoch 10 [2560/8158] loss=0.0019903132575564086\n",
            "[0m 3s] Epoch 10 [5120/8158] loss=0.0019902656262274832\n",
            "[0m 3s] Epoch 10 [7680/8158] loss=0.0019858119970497987\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7897441469258528\n",
            "[0m 3s] Epoch 11 [2560/8158] loss=0.0019954390823841094\n",
            "[0m 3s] Epoch 11 [5120/8158] loss=0.0019885366898961367\n",
            "[0m 3s] Epoch 11 [7680/8158] loss=0.001986983810396244\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1974/2658 74.27%\n",
            "Dec set: AUC  0.7890669761101463\n",
            "[0m 3s] Epoch 12 [2560/8158] loss=0.0019612979725934566\n",
            "[0m 3s] Epoch 12 [5120/8158] loss=0.001970362925203517\n",
            "[0m 3s] Epoch 12 [7680/8158] loss=0.0019916849676519634\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7902027532805199\n",
            "[0m 3s] Epoch 13 [2560/8158] loss=0.001959723373875022\n",
            "[0m 4s] Epoch 13 [5120/8158] loss=0.0019660832826048137\n",
            "[0m 4s] Epoch 13 [7680/8158] loss=0.001968085311818868\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7903682558492842\n",
            "[0m 4s] Epoch 14 [2560/8158] loss=0.0020246020052582026\n",
            "[0m 4s] Epoch 14 [5120/8158] loss=0.002014949411386624\n",
            "[0m 4s] Epoch 14 [7680/8158] loss=0.001984889576366792\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7908129650416883\n",
            "[0m 4s] Epoch 15 [2560/8158] loss=0.0019964322564192116\n",
            "[0m 4s] Epoch 15 [5120/8158] loss=0.001988382008858025\n",
            "[0m 4s] Epoch 15 [7680/8158] loss=0.0019753040513023736\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7908262305147572\n",
            "[0m 4s] Epoch 16 [2560/8158] loss=0.0019428277388215064\n",
            "[0m 4s] Epoch 16 [5120/8158] loss=0.001961536187445745\n",
            "[0m 5s] Epoch 16 [7680/8158] loss=0.0019751554044584434\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7912374601798924\n",
            "[0m 5s] Epoch 17 [2560/8158] loss=0.0019583676708862185\n",
            "[0m 5s] Epoch 17 [5120/8158] loss=0.001962731179082766\n",
            "[0m 5s] Epoch 17 [7680/8158] loss=0.0019705160986632107\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7914023310594628\n",
            "[0m 5s] Epoch 18 [2560/8158] loss=0.0019077860051766038\n",
            "[0m 5s] Epoch 18 [5120/8158] loss=0.0019684475555550307\n",
            "[0m 5s] Epoch 18 [7680/8158] loss=0.0019703575021897753\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7918640958600985\n",
            "[0m 6s] Epoch 19 [2560/8158] loss=0.0019817071850411596\n",
            "[0m 6s] Epoch 19 [5120/8158] loss=0.0019731321604922413\n",
            "[0m 6s] Epoch 19 [7680/8158] loss=0.0019686768064275384\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1974/2658 74.27%\n",
            "Dec set: AUC  0.7923972415396288\n",
            "[0m 6s] Epoch 20 [2560/8158] loss=0.00199381064157933\n",
            "[0m 6s] Epoch 20 [5120/8158] loss=0.001970905676716939\n",
            "[0m 6s] Epoch 20 [7680/8158] loss=0.0019719503781137366\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7922121566058582\n",
            "[0m 6s] Epoch 21 [2560/8158] loss=0.001955958444159478\n",
            "[0m 6s] Epoch 21 [5120/8158] loss=0.0019649079185910523\n",
            "[0m 6s] Epoch 21 [7680/8158] loss=0.001964532075605045\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7928223683670266\n",
            "[0m 7s] Epoch 22 [2560/8158] loss=0.0019602959509938955\n",
            "[0m 7s] Epoch 22 [5120/8158] loss=0.0019767060643061996\n",
            "[0m 7s] Epoch 22 [7680/8158] loss=0.001959834950199972\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dec set: AUC  0.792606330662762\n",
            "[0m 7s] Epoch 23 [2560/8158] loss=0.0020014557521790266\n",
            "[0m 7s] Epoch 23 [5120/8158] loss=0.001970877154963091\n",
            "[0m 7s] Epoch 23 [7680/8158] loss=0.0019698727449091774\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dec set: AUC  0.7927907839073387\n",
            "[0m 7s] Epoch 24 [2560/8158] loss=0.0019202591851353646\n",
            "[0m 7s] Epoch 24 [5120/8158] loss=0.0019232813268899918\n",
            "[0m 7s] Epoch 24 [7680/8158] loss=0.001953321811743081\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7936337731364063\n",
            "[0m 7s] Epoch 25 [2560/8158] loss=0.002014592615887523\n",
            "[0m 8s] Epoch 25 [5120/8158] loss=0.0019968737673480065\n",
            "[0m 8s] Epoch 25 [7680/8158] loss=0.0019706557116781673\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7938008549281549\n",
            "[0m 8s] Epoch 26 [2560/8158] loss=0.0019152565975673497\n",
            "[0m 8s] Epoch 26 [5120/8158] loss=0.0019613086769822985\n",
            "[0m 8s] Epoch 26 [7680/8158] loss=0.001964414333148549\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7940093123620944\n",
            "[0m 8s] Epoch 27 [2560/8158] loss=0.0019490710808895528\n",
            "[0m 8s] Epoch 27 [5120/8158] loss=0.0019658821518532934\n",
            "[0m 8s] Epoch 27 [7680/8158] loss=0.001956538918117682\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7938194897593706\n",
            "[0m 8s] Epoch 28 [2560/8158] loss=0.0019717403571121394\n",
            "[0m 9s] Epoch 28 [5120/8158] loss=0.0019660528050735593\n",
            "[0m 9s] Epoch 28 [7680/8158] loss=0.0019630803532587984\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.794199450809415\n",
            "[0m 9s] Epoch 29 [2560/8158] loss=0.001935584587045014\n",
            "[0m 9s] Epoch 29 [5120/8158] loss=0.0019485196156892926\n",
            "[0m 9s] Epoch 29 [7680/8158] loss=0.0019639971122766536\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7944590750680487\n",
            "[0m 9s] Epoch 30 [2560/8158] loss=0.001965100527741015\n",
            "[0m 9s] Epoch 30 [5120/8158] loss=0.0019604949979111553\n",
            "[0m 9s] Epoch 30 [7680/8158] loss=0.001966272147061924\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7946428966234318\n",
            "[0m 9s] Epoch 31 [2560/8158] loss=0.001940442796330899\n",
            "[0m 9s] Epoch 31 [5120/8158] loss=0.001943263754947111\n",
            "[0m 10s] Epoch 31 [7680/8158] loss=0.001956017129123211\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7946852197994135\n",
            "[0m 10s] Epoch 32 [2560/8158] loss=0.001973901886958629\n",
            "[0m 10s] Epoch 32 [5120/8158] loss=0.0019445624086074532\n",
            "[0m 10s] Epoch 32 [7680/8158] loss=0.001956097265550246\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7949126279091656\n",
            "[0m 10s] Epoch 33 [2560/8158] loss=0.00189525872701779\n",
            "[0m 10s] Epoch 33 [5120/8158] loss=0.0019246040610596538\n",
            "[0m 10s] Epoch 33 [7680/8158] loss=0.0019505086704157293\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7951292973026239\n",
            "[0m 10s] Epoch 34 [2560/8158] loss=0.0019374012015759944\n",
            "[0m 11s] Epoch 34 [5120/8158] loss=0.0019906942325178534\n",
            "[0m 11s] Epoch 34 [7680/8158] loss=0.0019570385261128346\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7950370706803355\n",
            "[0m 11s] Epoch 35 [2560/8158] loss=0.0019748206599615515\n",
            "[0m 11s] Epoch 35 [5120/8158] loss=0.001951807300793007\n",
            "[0m 11s] Epoch 35 [7680/8158] loss=0.0019624863712427516\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7946656374344069\n",
            "[0m 11s] Epoch 36 [2560/8158] loss=0.0019041060702875256\n",
            "[0m 11s] Epoch 36 [5120/8158] loss=0.001942056108964607\n",
            "[0m 11s] Epoch 36 [7680/8158] loss=0.0019508029023806255\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7952865879118692\n",
            "[0m 11s] Epoch 37 [2560/8158] loss=0.0019018080551177264\n",
            "[0m 11s] Epoch 37 [5120/8158] loss=0.0019365225860383361\n",
            "[0m 12s] Epoch 37 [7680/8158] loss=0.001955603901296854\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7951229804106865\n",
            "[0m 12s] Epoch 38 [2560/8158] loss=0.0019541318994015454\n",
            "[0m 12s] Epoch 38 [5120/8158] loss=0.0019480605842545629\n",
            "[0m 12s] Epoch 38 [7680/8158] loss=0.001949759367077301\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7955496864610687\n",
            "[0m 12s] Epoch 39 [2560/8158] loss=0.0019694035057909788\n",
            "[0m 12s] Epoch 39 [5120/8158] loss=0.0019653976312838497\n",
            "[0m 12s] Epoch 39 [7680/8158] loss=0.0019529618672095238\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7955929571708409\n",
            "[0m 13s] Epoch 40 [2560/8158] loss=0.0019142297096550465\n",
            "[0m 13s] Epoch 40 [5120/8158] loss=0.0019445473444648087\n",
            "[0m 13s] Epoch 40 [7680/8158] loss=0.0019485226599499582\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7948879920306091\n",
            "[0m 13s] Epoch 41 [2560/8158] loss=0.0020113356644287706\n",
            "[0m 13s] Epoch 41 [5120/8158] loss=0.001969261560589075\n",
            "[0m 13s] Epoch 41 [7680/8158] loss=0.0019547032425180078\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7954874650754836\n",
            "[0m 13s] Epoch 42 [2560/8158] loss=0.0020112809375859796\n",
            "[0m 13s] Epoch 42 [5120/8158] loss=0.001996980485273525\n",
            "[0m 13s] Epoch 42 [7680/8158] loss=0.00196222411080574\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7955708480490595\n",
            "[0m 13s] Epoch 43 [2560/8158] loss=0.0019496252061799169\n",
            "[0m 14s] Epoch 43 [5120/8158] loss=0.0019633816147688775\n",
            "[0m 14s] Epoch 43 [7680/8158] loss=0.0019546392334935567\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.794484342635799\n",
            "[0m 14s] Epoch 44 [2560/8158] loss=0.001889556518290192\n",
            "[0m 14s] Epoch 44 [5120/8158] loss=0.0019776371773332357\n",
            "[0m 14s] Epoch 44 [7680/8158] loss=0.0019767370036182304\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7955638994679282\n",
            "[0m 14s] Epoch 45 [2560/8158] loss=0.001937953196465969\n",
            "[0m 14s] Epoch 45 [5120/8158] loss=0.001939124398631975\n",
            "[0m 14s] Epoch 45 [7680/8158] loss=0.0019553876753586035\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7955228396703341\n",
            "[0m 14s] Epoch 46 [2560/8158] loss=0.00188693794189021\n",
            "[0m 15s] Epoch 46 [5120/8158] loss=0.001916488097049296\n",
            "[0m 15s] Epoch 46 [7680/8158] loss=0.001951094220081965\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7957483527125051\n",
            "[0m 15s] Epoch 47 [2560/8158] loss=0.0019387608277611434\n",
            "[0m 15s] Epoch 47 [5120/8158] loss=0.0019282856199424713\n",
            "[0m 15s] Epoch 47 [7680/8158] loss=0.0019470070682776472\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7957268752799174\n",
            "[0m 15s] Epoch 48 [2560/8158] loss=0.0019321640254929661\n",
            "[0m 15s] Epoch 48 [5120/8158] loss=0.001938673894619569\n",
            "[0m 15s] Epoch 48 [7680/8158] loss=0.0019507543727134665\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7960812529176144\n",
            "[0m 16s] Epoch 49 [2560/8158] loss=0.0018375244457274676\n",
            "[0m 16s] Epoch 49 [5120/8158] loss=0.0019332786439917981\n",
            "[0m 16s] Epoch 49 [7680/8158] loss=0.0019507254861916106\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7959492298761193\n",
            "[0m 16s] Epoch 50 [2560/8158] loss=0.001998719817493111\n",
            "[0m 16s] Epoch 50 [5120/8158] loss=0.0019748558814171703\n",
            "[0m 16s] Epoch 50 [7680/8158] loss=0.0019424074213020503\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7962088541347533\n",
            "[0m 16s] Epoch 51 [2560/8158] loss=0.001951181935146451\n",
            "[0m 16s] Epoch 51 [5120/8158] loss=0.0019391005218494684\n",
            "[0m 16s] Epoch 51 [7680/8158] loss=0.0019454573707965513\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7961633725128028\n",
            "[0m 17s] Epoch 52 [2560/8158] loss=0.0018768501933664084\n",
            "[0m 17s] Epoch 52 [5120/8158] loss=0.0019445735029876231\n",
            "[0m 17s] Epoch 52 [7680/8158] loss=0.001955486973747611\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7965215402856625\n",
            "[0m 17s] Epoch 53 [2560/8158] loss=0.0019193393411114812\n",
            "[0m 17s] Epoch 53 [5120/8158] loss=0.0019474533735774457\n",
            "[0m 17s] Epoch 53 [7680/8158] loss=0.0019478603848256172\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.796538595893894\n",
            "[0m 17s] Epoch 54 [2560/8158] loss=0.0018772204755805432\n",
            "[0m 17s] Epoch 54 [5120/8158] loss=0.0019360726815648376\n",
            "[0m 17s] Epoch 54 [7680/8158] loss=0.0019418451508196692\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7961772696750653\n",
            "[0m 18s] Epoch 55 [2560/8158] loss=0.0019700477714650333\n",
            "[0m 18s] Epoch 55 [5120/8158] loss=0.0019356155244167894\n",
            "[0m 18s] Epoch 55 [7680/8158] loss=0.0019402728027974566\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7957957294020367\n",
            "[0m 18s] Epoch 56 [2560/8158] loss=0.0019500897265970706\n",
            "[0m 18s] Epoch 56 [5120/8158] loss=0.0019603684777393937\n",
            "[0m 18s] Epoch 56 [7680/8158] loss=0.0019471584392401079\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7956769718336105\n",
            "[0m 18s] Epoch 57 [2560/8158] loss=0.0019334304612129926\n",
            "[0m 18s] Epoch 57 [5120/8158] loss=0.0019463310425635427\n",
            "[0m 18s] Epoch 57 [7680/8158] loss=0.0019517199601978064\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7961308405193244\n",
            "[0m 19s] Epoch 58 [2560/8158] loss=0.001875607029069215\n",
            "[0m 19s] Epoch 58 [5120/8158] loss=0.001920565782347694\n",
            "[0m 19s] Epoch 58 [7680/8158] loss=0.001935409983464827\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7963111877841417\n",
            "[0m 19s] Epoch 59 [2560/8158] loss=0.0019444831414148211\n",
            "[0m 19s] Epoch 59 [5120/8158] loss=0.0019442539487499744\n",
            "[0m 19s] Epoch 59 [7680/8158] loss=0.0019402302568778396\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7956826570363543\n",
            "[0m 19s] Epoch 60 [2560/8158] loss=0.0019473879947327077\n",
            "[0m 19s] Epoch 60 [5120/8158] loss=0.001956848578993231\n",
            "[0m 19s] Epoch 60 [7680/8158] loss=0.0019521230560106536\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7962694962973539\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "8V8zZcZVeRly",
        "outputId": "e7d02d4a-5f50-4e68-eed6-f31ca660e7de"
      },
      "source": [
        "for i, N_LAYER in enumerate(N_LAYERs):\r\n",
        "\r\n",
        "  epoch = np.arange(1, len(acc_list) + 1, 1)\r\n",
        "  plt.plot(epoch[2:], aucs[i][2:], label = str(N_LAYER))\r\n",
        "\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylabel('AUC')\r\n",
        "plt.legend()\r\n",
        "plt.grid()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA_828nnh9sC",
        "outputId": "fe6aa625-1f9f-4198-e82d-fbdd8e8bb28a"
      },
      "source": [
        "#调参BATCH_SIZEs\r\n",
        "N_EPOCHS  = 60\r\n",
        "HIDDEN_SIZE = 128\r\n",
        "N_LAYER = 3\r\n",
        "\r\n",
        "BATCH_SIZEs = [64, 128, 256, 512, 1024, 2048]\r\n",
        "\r\n",
        "\r\n",
        "accs = []\r\n",
        "aucs = []\r\n",
        "\r\n",
        "for BATCH_SIZE in BATCH_SIZEs:\r\n",
        "  trainset = DayFeatureDataset(is_train_set = True)\r\n",
        "  trainloader = DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True)\r\n",
        "  devset = DayFeatureDataset(is_train_set = False)\r\n",
        "  devloader = DataLoader(devset, batch_size = BATCH_SIZE, shuffle = False)\r\n",
        "  \r\n",
        "  if __name__ == '__main__':\r\n",
        "    classifier = RNNClassifier(11, HIDDEN_SIZE, N_CLASS, N_LAYER, bidirectional = False)\r\n",
        "    if USE_GPU:\r\n",
        "      device = torch.device(\"cuda:0\")\r\n",
        "      classifier.to(device)\r\n",
        "\r\n",
        "    criterion = torch.nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.0005)\r\n",
        "\r\n",
        "    start = time.time()\r\n",
        "    print(\"Training for %d epochs...\" % N_EPOCHS)\r\n",
        "    acc_list = []\r\n",
        "    auc_list = []\r\n",
        "    for epoch in range(1, N_EPOCHS + 1):\r\n",
        "      # Train cycle\r\n",
        "      trainModel()\r\n",
        "      acc, auc = devModel_auc()\r\n",
        "      acc_list.append(acc)\r\n",
        "      auc_list.append(auc)\r\n",
        "    accs.append(acc_list)\r\n",
        "    aucs.append(auc_list)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [640/8158] loss=0.009990967344492674\n",
            "[0m 0s] Epoch 1 [1280/8158] loss=0.00996819008141756\n",
            "[0m 0s] Epoch 1 [1920/8158] loss=0.00988262997319301\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.009775035362690688\n",
            "[0m 0s] Epoch 1 [3200/8158] loss=0.00966526048257947\n",
            "[0m 0s] Epoch 1 [3840/8158] loss=0.009473436900104085\n",
            "[0m 0s] Epoch 1 [4480/8158] loss=0.00929659732750484\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.009118783136364073\n",
            "[0m 0s] Epoch 1 [5760/8158] loss=0.009065242929177151\n",
            "[0m 0s] Epoch 1 [6400/8158] loss=0.00900147550739348\n",
            "[0m 0s] Epoch 1 [7040/8158] loss=0.008929627359082754\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.00889941987503941\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1971/2658 74.15%\n",
            "Dec set: AUC  0.7873127752190856\n",
            "[0m 0s] Epoch 2 [640/8158] loss=0.008154264604672789\n",
            "[0m 0s] Epoch 2 [1280/8158] loss=0.008099221554584802\n",
            "[0m 0s] Epoch 2 [1920/8158] loss=0.00814382426130275\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.008190648804884404\n",
            "[0m 0s] Epoch 2 [3200/8158] loss=0.008178096134215593\n",
            "[0m 0s] Epoch 2 [3840/8158] loss=0.008227215055376292\n",
            "[0m 1s] Epoch 2 [4480/8158] loss=0.008218843928937401\n",
            "[0m 1s] Epoch 2 [5120/8158] loss=0.008113438601139934\n",
            "[0m 1s] Epoch 2 [5760/8158] loss=0.008072667211915055\n",
            "[0m 1s] Epoch 2 [6400/8158] loss=0.008048579888418316\n",
            "[0m 1s] Epoch 2 [7040/8158] loss=0.008049951603805477\n",
            "[0m 1s] Epoch 2 [7680/8158] loss=0.008070435836755981\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1963/2658 73.85%\n",
            "Dec set: AUC  0.789767519426022\n",
            "[0m 1s] Epoch 3 [640/8158] loss=0.007986227190122009\n",
            "[0m 1s] Epoch 3 [1280/8158] loss=0.007827672269195318\n",
            "[0m 1s] Epoch 3 [1920/8158] loss=0.007900752949838837\n",
            "[0m 1s] Epoch 3 [2560/8158] loss=0.00805787971476093\n",
            "[0m 1s] Epoch 3 [3200/8158] loss=0.00794379623606801\n",
            "[0m 1s] Epoch 3 [3840/8158] loss=0.007957949279807508\n",
            "[0m 1s] Epoch 3 [4480/8158] loss=0.007935256557539105\n",
            "[0m 1s] Epoch 3 [5120/8158] loss=0.007937003864208236\n",
            "[0m 1s] Epoch 3 [5760/8158] loss=0.007914933344970147\n",
            "[0m 2s] Epoch 3 [6400/8158] loss=0.007965379813686013\n",
            "[0m 2s] Epoch 3 [7040/8158] loss=0.008001030147583648\n",
            "[0m 2s] Epoch 3 [7680/8158] loss=0.007986515081332376\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7894213537478434\n",
            "[0m 2s] Epoch 4 [640/8158] loss=0.008167596114799381\n",
            "[0m 2s] Epoch 4 [1280/8158] loss=0.007964892243035138\n",
            "[0m 2s] Epoch 4 [1920/8158] loss=0.007925635778034727\n",
            "[0m 2s] Epoch 4 [2560/8158] loss=0.007860911812167614\n",
            "[0m 2s] Epoch 4 [3200/8158] loss=0.00786774754524231\n",
            "[0m 2s] Epoch 4 [3840/8158] loss=0.007925884914584458\n",
            "[0m 2s] Epoch 4 [4480/8158] loss=0.007953841172690903\n",
            "[0m 2s] Epoch 4 [5120/8158] loss=0.007963750418275595\n",
            "[0m 2s] Epoch 4 [5760/8158] loss=0.007960773616408309\n",
            "[0m 2s] Epoch 4 [6400/8158] loss=0.007956383884884417\n",
            "[0m 2s] Epoch 4 [7040/8158] loss=0.007987960745495829\n",
            "[0m 2s] Epoch 4 [7680/8158] loss=0.007988844253122806\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7898389003049164\n",
            "[0m 3s] Epoch 5 [640/8158] loss=0.007966670906171203\n",
            "[0m 3s] Epoch 5 [1280/8158] loss=0.008064761152490973\n",
            "[0m 3s] Epoch 5 [1920/8158] loss=0.008055544411763548\n",
            "[0m 3s] Epoch 5 [2560/8158] loss=0.008059409412089736\n",
            "[0m 3s] Epoch 5 [3200/8158] loss=0.007938185902312399\n",
            "[0m 3s] Epoch 5 [3840/8158] loss=0.007945468160323798\n",
            "[0m 3s] Epoch 5 [4480/8158] loss=0.007971908870552267\n",
            "[0m 3s] Epoch 5 [5120/8158] loss=0.007952498400118201\n",
            "[0m 3s] Epoch 5 [5760/8158] loss=0.007954313021360172\n",
            "[0m 3s] Epoch 5 [6400/8158] loss=0.007943800594657659\n",
            "[0m 3s] Epoch 5 [7040/8158] loss=0.007904389420185577\n",
            "[0m 3s] Epoch 5 [7680/8158] loss=0.007943921792320906\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dec set: AUC  0.7902924531460332\n",
            "[0m 3s] Epoch 6 [640/8158] loss=0.008424621168524027\n",
            "[0m 3s] Epoch 6 [1280/8158] loss=0.008435901300981642\n",
            "[0m 4s] Epoch 6 [1920/8158] loss=0.00823784035940965\n",
            "[0m 4s] Epoch 6 [2560/8158] loss=0.008189412183128297\n",
            "[0m 4s] Epoch 6 [3200/8158] loss=0.008088911483064294\n",
            "[0m 4s] Epoch 6 [3840/8158] loss=0.008088143014659484\n",
            "[0m 4s] Epoch 6 [4480/8158] loss=0.008154891198500991\n",
            "[0m 4s] Epoch 6 [5120/8158] loss=0.008079374453518539\n",
            "[0m 4s] Epoch 6 [5760/8158] loss=0.008026005658838485\n",
            "[0m 4s] Epoch 6 [6400/8158] loss=0.008010437306948006\n",
            "[0m 4s] Epoch 6 [7040/8158] loss=0.007962232222780585\n",
            "[0m 4s] Epoch 6 [7680/8158] loss=0.00789234929252416\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1977/2658 74.38%\n",
            "Dec set: AUC  0.7901635885505071\n",
            "[0m 4s] Epoch 7 [640/8158] loss=0.007929944153875112\n",
            "[0m 4s] Epoch 7 [1280/8158] loss=0.00798224057070911\n",
            "[0m 4s] Epoch 7 [1920/8158] loss=0.008084967297812303\n",
            "[0m 4s] Epoch 7 [2560/8158] loss=0.008081490581389517\n",
            "[0m 4s] Epoch 7 [3200/8158] loss=0.008125572660937906\n",
            "[0m 4s] Epoch 7 [3840/8158] loss=0.008083109402408202\n",
            "[0m 4s] Epoch 7 [4480/8158] loss=0.008099382862980876\n",
            "[0m 4s] Epoch 7 [5120/8158] loss=0.00808867317973636\n",
            "[0m 5s] Epoch 7 [5760/8158] loss=0.008031201926577422\n",
            "[0m 5s] Epoch 7 [6400/8158] loss=0.008043439448811114\n",
            "[0m 5s] Epoch 7 [7040/8158] loss=0.008000973772934891\n",
            "[0m 5s] Epoch 7 [7680/8158] loss=0.00793276927821959\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dec set: AUC  0.7915084548440138\n",
            "[0m 5s] Epoch 8 [640/8158] loss=0.008076062239706517\n",
            "[0m 5s] Epoch 8 [1280/8158] loss=0.007939284830354155\n",
            "[0m 5s] Epoch 8 [1920/8158] loss=0.007957414397969842\n",
            "[0m 5s] Epoch 8 [2560/8158] loss=0.00807571755722165\n",
            "[0m 5s] Epoch 8 [3200/8158] loss=0.008074103584513069\n",
            "[0m 5s] Epoch 8 [3840/8158] loss=0.007957479815619688\n",
            "[0m 5s] Epoch 8 [4480/8158] loss=0.007918794912153056\n",
            "[0m 5s] Epoch 8 [5120/8158] loss=0.007883087708614766\n",
            "[0m 5s] Epoch 8 [5760/8158] loss=0.007911967993196514\n",
            "[0m 5s] Epoch 8 [6400/8158] loss=0.00787773503921926\n",
            "[0m 5s] Epoch 8 [7040/8158] loss=0.007894382443787023\n",
            "[0m 5s] Epoch 8 [7680/8158] loss=0.007891397461450348\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7918299846436357\n",
            "[0m 6s] Epoch 9 [640/8158] loss=0.007904110802337527\n",
            "[0m 6s] Epoch 9 [1280/8158] loss=0.007717386330477893\n",
            "[0m 6s] Epoch 9 [1920/8158] loss=0.007702849463870128\n",
            "[0m 6s] Epoch 9 [2560/8158] loss=0.007735904038418085\n",
            "[0m 6s] Epoch 9 [3200/8158] loss=0.007801068127155304\n",
            "[0m 6s] Epoch 9 [3840/8158] loss=0.007861129466133814\n",
            "[0m 6s] Epoch 9 [4480/8158] loss=0.007846764867593135\n",
            "[0m 6s] Epoch 9 [5120/8158] loss=0.007882079953560606\n",
            "[0m 6s] Epoch 9 [5760/8158] loss=0.007876016013324261\n",
            "[0m 6s] Epoch 9 [6400/8158] loss=0.007924266704358161\n",
            "[0m 6s] Epoch 9 [7040/8158] loss=0.007918575892902234\n",
            "[0m 6s] Epoch 9 [7680/8158] loss=0.007920978377417972\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7926878185687567\n",
            "[0m 6s] Epoch 10 [640/8158] loss=0.007544740801677108\n",
            "[0m 6s] Epoch 10 [1280/8158] loss=0.007809941587038338\n",
            "[0m 6s] Epoch 10 [1920/8158] loss=0.007844957414393623\n",
            "[0m 6s] Epoch 10 [2560/8158] loss=0.007818263210356235\n",
            "[0m 6s] Epoch 10 [3200/8158] loss=0.007895308611914516\n",
            "[0m 6s] Epoch 10 [3840/8158] loss=0.007827707286924123\n",
            "[0m 7s] Epoch 10 [4480/8158] loss=0.007838789406897766\n",
            "[0m 7s] Epoch 10 [5120/8158] loss=0.007913635851582512\n",
            "[0m 7s] Epoch 10 [5760/8158] loss=0.007905552096457946\n",
            "[0m 7s] Epoch 10 [6400/8158] loss=0.007898935354314745\n",
            "[0m 7s] Epoch 10 [7040/8158] loss=0.007882912969216704\n",
            "[0m 7s] Epoch 10 [7680/8158] loss=0.007888044676898668\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7932752895189499\n",
            "[0m 7s] Epoch 11 [640/8158] loss=0.00806508306413889\n",
            "[0m 7s] Epoch 11 [1280/8158] loss=0.00787337685469538\n",
            "[0m 7s] Epoch 11 [1920/8158] loss=0.007897648898263773\n",
            "[0m 7s] Epoch 11 [2560/8158] loss=0.007887913309969009\n",
            "[0m 7s] Epoch 11 [3200/8158] loss=0.007807774934917688\n",
            "[0m 7s] Epoch 11 [3840/8158] loss=0.007798614795319736\n",
            "[0m 7s] Epoch 11 [4480/8158] loss=0.007849943970463106\n",
            "[0m 7s] Epoch 11 [5120/8158] loss=0.007836769899586215\n",
            "[0m 7s] Epoch 11 [5760/8158] loss=0.007912394238842858\n",
            "[0m 7s] Epoch 11 [6400/8158] loss=0.007899578674696386\n",
            "[0m 7s] Epoch 11 [7040/8158] loss=0.007880162672055038\n",
            "[0m 7s] Epoch 11 [7680/8158] loss=0.007872628308056543\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1967/2658 74.00%\n",
            "Dec set: AUC  0.7936890459408602\n",
            "[0m 8s] Epoch 12 [640/8158] loss=0.007824556715786458\n",
            "[0m 8s] Epoch 12 [1280/8158] loss=0.00788395565468818\n",
            "[0m 8s] Epoch 12 [1920/8158] loss=0.008005843016629417\n",
            "[0m 8s] Epoch 12 [2560/8158] loss=0.00795392997097224\n",
            "[0m 8s] Epoch 12 [3200/8158] loss=0.007929776143282653\n",
            "[0m 8s] Epoch 12 [3840/8158] loss=0.007929561853719255\n",
            "[0m 8s] Epoch 12 [4480/8158] loss=0.007898481436339873\n",
            "[0m 8s] Epoch 12 [5120/8158] loss=0.007890038215555251\n",
            "[0m 8s] Epoch 12 [5760/8158] loss=0.007898377694396509\n",
            "[0m 8s] Epoch 12 [6400/8158] loss=0.007884119139052927\n",
            "[0m 8s] Epoch 12 [7040/8158] loss=0.007907522169195793\n",
            "[0m 8s] Epoch 12 [7680/8158] loss=0.007860949340586862\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7941514424306894\n",
            "[0m 8s] Epoch 13 [640/8158] loss=0.00747093241661787\n",
            "[0m 8s] Epoch 13 [1280/8158] loss=0.007722665183246135\n",
            "[0m 8s] Epoch 13 [1920/8158] loss=0.007713695305089156\n",
            "[0m 8s] Epoch 13 [2560/8158] loss=0.00782120090443641\n",
            "[0m 9s] Epoch 13 [3200/8158] loss=0.00781681613996625\n",
            "[0m 9s] Epoch 13 [3840/8158] loss=0.007801035403584441\n",
            "[0m 9s] Epoch 13 [4480/8158] loss=0.007820150568815215\n",
            "[0m 9s] Epoch 13 [5120/8158] loss=0.007859363203169777\n",
            "[0m 9s] Epoch 13 [5760/8158] loss=0.007940959381974406\n",
            "[0m 9s] Epoch 13 [6400/8158] loss=0.007926590004935861\n",
            "[0m 9s] Epoch 13 [7040/8158] loss=0.007893265783786773\n",
            "[0m 9s] Epoch 13 [7680/8158] loss=0.007866569278606524\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7937452662791042\n",
            "[0m 9s] Epoch 14 [640/8158] loss=0.007276953384280205\n",
            "[0m 9s] Epoch 14 [1280/8158] loss=0.007597045693546534\n",
            "[0m 9s] Epoch 14 [1920/8158] loss=0.007718424421424667\n",
            "[0m 9s] Epoch 14 [2560/8158] loss=0.007771931716706604\n",
            "[0m 9s] Epoch 14 [3200/8158] loss=0.007841921327635646\n",
            "[0m 9s] Epoch 14 [3840/8158] loss=0.007806734655362865\n",
            "[0m 9s] Epoch 14 [4480/8158] loss=0.00784253893154008\n",
            "[0m 9s] Epoch 14 [5120/8158] loss=0.007884128240402789\n",
            "[0m 9s] Epoch 14 [5760/8158] loss=0.007848428877898389\n",
            "[0m 9s] Epoch 14 [6400/8158] loss=0.00788335773628205\n",
            "[0m 10s] Epoch 14 [7040/8158] loss=0.00787782318551432\n",
            "[0m 10s] Epoch 14 [7680/8158] loss=0.0078656084719114\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.793904451955931\n",
            "[0m 10s] Epoch 15 [640/8158] loss=0.007896291185170412\n",
            "[0m 10s] Epoch 15 [1280/8158] loss=0.008150084060616791\n",
            "[0m 10s] Epoch 15 [1920/8158] loss=0.007796386551732818\n",
            "[0m 10s] Epoch 15 [2560/8158] loss=0.0077896917471662165\n",
            "[0m 10s] Epoch 15 [3200/8158] loss=0.007810747353360057\n",
            "[0m 10s] Epoch 15 [3840/8158] loss=0.0077713286271318795\n",
            "[0m 10s] Epoch 15 [4480/8158] loss=0.007737607196239489\n",
            "[0m 10s] Epoch 15 [5120/8158] loss=0.007760190014960244\n",
            "[0m 10s] Epoch 15 [5760/8158] loss=0.007747846034665902\n",
            "[0m 10s] Epoch 15 [6400/8158] loss=0.007794364308938384\n",
            "[0m 10s] Epoch 15 [7040/8158] loss=0.007815532166172158\n",
            "[0m 10s] Epoch 15 [7680/8158] loss=0.007833725858169297\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7945190855414556\n",
            "[0m 10s] Epoch 16 [640/8158] loss=0.007666350854560733\n",
            "[0m 10s] Epoch 16 [1280/8158] loss=0.007667288859374821\n",
            "[0m 11s] Epoch 16 [1920/8158] loss=0.007693736410389344\n",
            "[0m 11s] Epoch 16 [2560/8158] loss=0.007695433718618005\n",
            "[0m 11s] Epoch 16 [3200/8158] loss=0.0077983186673372985\n",
            "[0m 11s] Epoch 16 [3840/8158] loss=0.007912989410882195\n",
            "[0m 11s] Epoch 16 [4480/8158] loss=0.007901230767103178\n",
            "[0m 11s] Epoch 16 [5120/8158] loss=0.007850840600440279\n",
            "[0m 11s] Epoch 16 [5760/8158] loss=0.007826182375558549\n",
            "[0m 11s] Epoch 16 [6400/8158] loss=0.007848182776942849\n",
            "[0m 11s] Epoch 16 [7040/8158] loss=0.007825786065818234\n",
            "[0m 11s] Epoch 16 [7680/8158] loss=0.007840665619975576\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.793603136210509\n",
            "[0m 11s] Epoch 17 [640/8158] loss=0.0077432393562048675\n",
            "[0m 11s] Epoch 17 [1280/8158] loss=0.007703652768395841\n",
            "[0m 11s] Epoch 17 [1920/8158] loss=0.007713971985504031\n",
            "[0m 11s] Epoch 17 [2560/8158] loss=0.007697631930932402\n",
            "[0m 11s] Epoch 17 [3200/8158] loss=0.007583749536424875\n",
            "[0m 11s] Epoch 17 [3840/8158] loss=0.007665657089091837\n",
            "[0m 11s] Epoch 17 [4480/8158] loss=0.00762703139334917\n",
            "[0m 12s] Epoch 17 [5120/8158] loss=0.007680037431418896\n",
            "[0m 12s] Epoch 17 [5760/8158] loss=0.007714997330266569\n",
            "[0m 12s] Epoch 17 [6400/8158] loss=0.007749603749252856\n",
            "[0m 12s] Epoch 17 [7040/8158] loss=0.007752625326710669\n",
            "[0m 12s] Epoch 17 [7680/8158] loss=0.0077822143988062935\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7946517402721445\n",
            "[0m 12s] Epoch 18 [640/8158] loss=0.007970498222857714\n",
            "[0m 12s] Epoch 18 [1280/8158] loss=0.007773537607863546\n",
            "[0m 12s] Epoch 18 [1920/8158] loss=0.00781352558794121\n",
            "[0m 12s] Epoch 18 [2560/8158] loss=0.007747413602191955\n",
            "[0m 12s] Epoch 18 [3200/8158] loss=0.007756456201896071\n",
            "[0m 12s] Epoch 18 [3840/8158] loss=0.007927924728331466\n",
            "[0m 12s] Epoch 18 [4480/8158] loss=0.007896281745550888\n",
            "[0m 12s] Epoch 18 [5120/8158] loss=0.007880054658744483\n",
            "[0m 12s] Epoch 18 [5760/8158] loss=0.007836384491788017\n",
            "[0m 12s] Epoch 18 [6400/8158] loss=0.007807204877026379\n",
            "[0m 13s] Epoch 18 [7040/8158] loss=0.007807405885647644\n",
            "[0m 13s] Epoch 18 [7680/8158] loss=0.007855087347949544\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7948358776721243\n",
            "[0m 13s] Epoch 19 [640/8158] loss=0.00756309675052762\n",
            "[0m 13s] Epoch 19 [1280/8158] loss=0.007467572647146881\n",
            "[0m 13s] Epoch 19 [1920/8158] loss=0.007454068399965763\n",
            "[0m 13s] Epoch 19 [2560/8158] loss=0.007488243980333209\n",
            "[0m 13s] Epoch 19 [3200/8158] loss=0.007589053427800536\n",
            "[0m 13s] Epoch 19 [3840/8158] loss=0.007695228257216513\n",
            "[0m 13s] Epoch 19 [4480/8158] loss=0.007741669618657657\n",
            "[0m 13s] Epoch 19 [5120/8158] loss=0.0078078451042529196\n",
            "[0m 13s] Epoch 19 [5760/8158] loss=0.007814855909802847\n",
            "[0m 13s] Epoch 19 [6400/8158] loss=0.007833324773237109\n",
            "[0m 13s] Epoch 19 [7040/8158] loss=0.007825468886982311\n",
            "[0m 13s] Epoch 19 [7680/8158] loss=0.00783456116138647\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7950067495990353\n",
            "[0m 14s] Epoch 20 [640/8158] loss=0.007688522851094603\n",
            "[0m 14s] Epoch 20 [1280/8158] loss=0.00768913805950433\n",
            "[0m 14s] Epoch 20 [1920/8158] loss=0.007813598308712245\n",
            "[0m 14s] Epoch 20 [2560/8158] loss=0.007816517143510282\n",
            "[0m 14s] Epoch 20 [3200/8158] loss=0.007840732596814632\n",
            "[0m 14s] Epoch 20 [3840/8158] loss=0.007836446872291465\n",
            "[0m 14s] Epoch 20 [4480/8158] loss=0.007805712926866753\n",
            "[0m 14s] Epoch 20 [5120/8158] loss=0.007804869586834684\n",
            "[0m 14s] Epoch 20 [5760/8158] loss=0.007848344675989615\n",
            "[0m 14s] Epoch 20 [6400/8158] loss=0.007796901888214052\n",
            "[0m 14s] Epoch 20 [7040/8158] loss=0.007805645304986022\n",
            "[0m 14s] Epoch 20 [7680/8158] loss=0.007801308894219498\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7935131205003989\n",
            "[0m 14s] Epoch 21 [640/8158] loss=0.007853221846744418\n",
            "[0m 14s] Epoch 21 [1280/8158] loss=0.00755414767190814\n",
            "[0m 14s] Epoch 21 [1920/8158] loss=0.007736234397937854\n",
            "[0m 14s] Epoch 21 [2560/8158] loss=0.007826812076382339\n",
            "[0m 15s] Epoch 21 [3200/8158] loss=0.00785121557302773\n",
            "[0m 15s] Epoch 21 [3840/8158] loss=0.007972074470793207\n",
            "[0m 15s] Epoch 21 [4480/8158] loss=0.008003753756306001\n",
            "[0m 15s] Epoch 21 [5120/8158] loss=0.00793558253790252\n",
            "[0m 15s] Epoch 21 [5760/8158] loss=0.007886777104188999\n",
            "[0m 15s] Epoch 21 [6400/8158] loss=0.007876422344706953\n",
            "[0m 15s] Epoch 21 [7040/8158] loss=0.007836815901100636\n",
            "[0m 15s] Epoch 21 [7680/8158] loss=0.007838741061277688\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7948380885843024\n",
            "[0m 15s] Epoch 22 [640/8158] loss=0.008035864308476448\n",
            "[0m 15s] Epoch 22 [1280/8158] loss=0.008095481456257403\n",
            "[0m 15s] Epoch 22 [1920/8158] loss=0.007836329486841957\n",
            "[0m 15s] Epoch 22 [2560/8158] loss=0.007803016086108982\n",
            "[0m 15s] Epoch 22 [3200/8158] loss=0.007788284989073873\n",
            "[0m 15s] Epoch 22 [3840/8158] loss=0.00784437357603262\n",
            "[0m 15s] Epoch 22 [4480/8158] loss=0.007869558894474592\n",
            "[0m 15s] Epoch 22 [5120/8158] loss=0.007856582471868023\n",
            "[0m 15s] Epoch 22 [5760/8158] loss=0.007878075648720066\n",
            "[0m 15s] Epoch 22 [6400/8158] loss=0.007858041366562247\n",
            "[0m 15s] Epoch 22 [7040/8158] loss=0.00784462270411578\n",
            "[0m 16s] Epoch 22 [7680/8158] loss=0.007825893489643932\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7953263843310759\n",
            "[0m 16s] Epoch 23 [640/8158] loss=0.007698683487251401\n",
            "[0m 16s] Epoch 23 [1280/8158] loss=0.007663175649940968\n",
            "[0m 16s] Epoch 23 [1920/8158] loss=0.007589554538329443\n",
            "[0m 16s] Epoch 23 [2560/8158] loss=0.007711573143023997\n",
            "[0m 16s] Epoch 23 [3200/8158] loss=0.007758167637512088\n",
            "[0m 16s] Epoch 23 [3840/8158] loss=0.007688627523990969\n",
            "[0m 16s] Epoch 23 [4480/8158] loss=0.007672604639083147\n",
            "[0m 16s] Epoch 23 [5120/8158] loss=0.007664921955438331\n",
            "[0m 16s] Epoch 23 [5760/8158] loss=0.0077153056032127805\n",
            "[0m 16s] Epoch 23 [6400/8158] loss=0.00776312209200114\n",
            "[0m 16s] Epoch 23 [7040/8158] loss=0.00779286571778357\n",
            "[0m 16s] Epoch 23 [7680/8158] loss=0.007811244088225066\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7952518450062127\n",
            "[0m 16s] Epoch 24 [640/8158] loss=0.007322155451402068\n",
            "[0m 16s] Epoch 24 [1280/8158] loss=0.007608200539834797\n",
            "[0m 16s] Epoch 24 [1920/8158] loss=0.007685012028863032\n",
            "[0m 17s] Epoch 24 [2560/8158] loss=0.00759714973391965\n",
            "[0m 17s] Epoch 24 [3200/8158] loss=0.007635751664638519\n",
            "[0m 17s] Epoch 24 [3840/8158] loss=0.007583856605924666\n",
            "[0m 17s] Epoch 24 [4480/8158] loss=0.0076108130559857405\n",
            "[0m 17s] Epoch 24 [5120/8158] loss=0.007775976980337873\n",
            "[0m 17s] Epoch 24 [5760/8158] loss=0.007828544307914045\n",
            "[0m 17s] Epoch 24 [6400/8158] loss=0.007850901507772505\n",
            "[0m 17s] Epoch 24 [7040/8158] loss=0.007868316536769272\n",
            "[0m 17s] Epoch 24 [7680/8158] loss=0.007831102768735339\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7950819206130922\n",
            "[0m 17s] Epoch 25 [640/8158] loss=0.0074181962758302685\n",
            "[0m 17s] Epoch 25 [1280/8158] loss=0.007517231069505215\n",
            "[0m 17s] Epoch 25 [1920/8158] loss=0.007799375553925832\n",
            "[0m 17s] Epoch 25 [2560/8158] loss=0.007800196518655866\n",
            "[0m 17s] Epoch 25 [3200/8158] loss=0.007924496540799736\n",
            "[0m 18s] Epoch 25 [3840/8158] loss=0.00791599062892298\n",
            "[0m 18s] Epoch 25 [4480/8158] loss=0.007967441994696855\n",
            "[0m 18s] Epoch 25 [5120/8158] loss=0.007919144327752292\n",
            "[0m 18s] Epoch 25 [5760/8158] loss=0.007868434949260619\n",
            "[0m 18s] Epoch 25 [6400/8158] loss=0.007814602674916387\n",
            "[0m 18s] Epoch 25 [7040/8158] loss=0.007794906342910094\n",
            "[0m 18s] Epoch 25 [7680/8158] loss=0.007808289292734116\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7951337191269804\n",
            "[0m 18s] Epoch 26 [640/8158] loss=0.007508754497393966\n",
            "[0m 18s] Epoch 26 [1280/8158] loss=0.007784110936336219\n",
            "[0m 18s] Epoch 26 [1920/8158] loss=0.007926705339923502\n",
            "[0m 18s] Epoch 26 [2560/8158] loss=0.00794521642383188\n",
            "[0m 18s] Epoch 26 [3200/8158] loss=0.007929854327812791\n",
            "[0m 18s] Epoch 26 [3840/8158] loss=0.00790557156627377\n",
            "[0m 18s] Epoch 26 [4480/8158] loss=0.007886160012068493\n",
            "[0m 18s] Epoch 26 [5120/8158] loss=0.007802772975992411\n",
            "[0m 18s] Epoch 26 [5760/8158] loss=0.007746913071928753\n",
            "[0m 18s] Epoch 26 [6400/8158] loss=0.007744689779356122\n",
            "[0m 18s] Epoch 26 [7040/8158] loss=0.007741934958506714\n",
            "[0m 18s] Epoch 26 [7680/8158] loss=0.007787623764791836\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7957230851447548\n",
            "[0m 19s] Epoch 27 [640/8158] loss=0.007308464217931032\n",
            "[0m 19s] Epoch 27 [1280/8158] loss=0.007468396541662514\n",
            "[0m 19s] Epoch 27 [1920/8158] loss=0.007470809000854691\n",
            "[0m 19s] Epoch 27 [2560/8158] loss=0.007660451263654977\n",
            "[0m 19s] Epoch 27 [3200/8158] loss=0.007754831956699491\n",
            "[0m 19s] Epoch 27 [3840/8158] loss=0.007754541685183843\n",
            "[0m 19s] Epoch 27 [4480/8158] loss=0.007777717443449157\n",
            "[0m 19s] Epoch 27 [5120/8158] loss=0.007781700167106465\n",
            "[0m 19s] Epoch 27 [5760/8158] loss=0.007820970047679213\n",
            "[0m 19s] Epoch 27 [6400/8158] loss=0.007831371938809752\n",
            "[0m 19s] Epoch 27 [7040/8158] loss=0.007818550938232378\n",
            "[0m 19s] Epoch 27 [7680/8158] loss=0.007813800258251529\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dec set: AUC  0.7939909933754754\n",
            "[0m 20s] Epoch 28 [640/8158] loss=0.007815874926745892\n",
            "[0m 20s] Epoch 28 [1280/8158] loss=0.007484925049357116\n",
            "[0m 20s] Epoch 28 [1920/8158] loss=0.007600180339068174\n",
            "[0m 20s] Epoch 28 [2560/8158] loss=0.0077451143530197445\n",
            "[0m 20s] Epoch 28 [3200/8158] loss=0.007787340488284826\n",
            "[0m 20s] Epoch 28 [3840/8158] loss=0.007740395508396129\n",
            "[0m 20s] Epoch 28 [4480/8158] loss=0.007765166933781334\n",
            "[0m 20s] Epoch 28 [5120/8158] loss=0.007837001222651453\n",
            "[0m 20s] Epoch 28 [5760/8158] loss=0.007804672642507487\n",
            "[0m 20s] Epoch 28 [6400/8158] loss=0.007828002851456404\n",
            "[0m 20s] Epoch 28 [7040/8158] loss=0.007783717822960832\n",
            "[0m 20s] Epoch 28 [7680/8158] loss=0.007783107300444196\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.794802713989452\n",
            "[0m 20s] Epoch 29 [640/8158] loss=0.008146780589595438\n",
            "[0m 20s] Epoch 29 [1280/8158] loss=0.008134172717109322\n",
            "[0m 20s] Epoch 29 [1920/8158] loss=0.008151600742712617\n",
            "[0m 21s] Epoch 29 [2560/8158] loss=0.008043594879563899\n",
            "[0m 21s] Epoch 29 [3200/8158] loss=0.008026555208489298\n",
            "[0m 21s] Epoch 29 [3840/8158] loss=0.007950517279095948\n",
            "[0m 21s] Epoch 29 [4480/8158] loss=0.007871651436601367\n",
            "[0m 21s] Epoch 29 [5120/8158] loss=0.007835861190687864\n",
            "[0m 21s] Epoch 29 [5760/8158] loss=0.007808087962783045\n",
            "[0m 21s] Epoch 29 [6400/8158] loss=0.007761994041502475\n",
            "[0m 21s] Epoch 29 [7040/8158] loss=0.0078004367522556675\n",
            "[0m 21s] Epoch 29 [7680/8158] loss=0.0077714324812404815\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7954470369670833\n",
            "[0m 21s] Epoch 30 [640/8158] loss=0.007993488758802413\n",
            "[0m 21s] Epoch 30 [1280/8158] loss=0.00798315331339836\n",
            "[0m 21s] Epoch 30 [1920/8158] loss=0.008020321109021704\n",
            "[0m 21s] Epoch 30 [2560/8158] loss=0.00795306273503229\n",
            "[0m 21s] Epoch 30 [3200/8158] loss=0.007874803710728883\n",
            "[0m 21s] Epoch 30 [3840/8158] loss=0.007948268360147873\n",
            "[0m 21s] Epoch 30 [4480/8158] loss=0.00790796993699457\n",
            "[0m 21s] Epoch 30 [5120/8158] loss=0.007910846255254001\n",
            "[0m 22s] Epoch 30 [5760/8158] loss=0.00785521845229798\n",
            "[0m 22s] Epoch 30 [6400/8158] loss=0.007828519674949347\n",
            "[0m 22s] Epoch 30 [7040/8158] loss=0.007805845099077983\n",
            "[0m 22s] Epoch 30 [7680/8158] loss=0.007805456438412269\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7956529676442479\n",
            "[0m 22s] Epoch 31 [640/8158] loss=0.008310583606362342\n",
            "[0m 22s] Epoch 31 [1280/8158] loss=0.008035467099398374\n",
            "[0m 22s] Epoch 31 [1920/8158] loss=0.008047652244567872\n",
            "[0m 22s] Epoch 31 [2560/8158] loss=0.007938284380361437\n",
            "[0m 22s] Epoch 31 [3200/8158] loss=0.00787265295162797\n",
            "[0m 22s] Epoch 31 [3840/8158] loss=0.007809973151112596\n",
            "[0m 22s] Epoch 31 [4480/8158] loss=0.007837491888286812\n",
            "[0m 22s] Epoch 31 [5120/8158] loss=0.00785024223732762\n",
            "[0m 22s] Epoch 31 [5760/8158] loss=0.007816434310128292\n",
            "[0m 22s] Epoch 31 [6400/8158] loss=0.007791273039765656\n",
            "[0m 22s] Epoch 31 [7040/8158] loss=0.007833936526863412\n",
            "[0m 23s] Epoch 31 [7680/8158] loss=0.007819001310660193\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7952341577087875\n",
            "[0m 23s] Epoch 32 [640/8158] loss=0.007889794791117311\n",
            "[0m 23s] Epoch 32 [1280/8158] loss=0.007892104936763645\n",
            "[0m 23s] Epoch 32 [1920/8158] loss=0.007765567178527514\n",
            "[0m 23s] Epoch 32 [2560/8158] loss=0.0077233717544004325\n",
            "[0m 23s] Epoch 32 [3200/8158] loss=0.007659337744116783\n",
            "[0m 23s] Epoch 32 [3840/8158] loss=0.007680975552648306\n",
            "[0m 23s] Epoch 32 [4480/8158] loss=0.007769567166854228\n",
            "[0m 23s] Epoch 32 [5120/8158] loss=0.007774953340413049\n",
            "[0m 23s] Epoch 32 [5760/8158] loss=0.007786739612412122\n",
            "[0m 23s] Epoch 32 [6400/8158] loss=0.007770724231377244\n",
            "[0m 23s] Epoch 32 [7040/8158] loss=0.007776243074543097\n",
            "[0m 23s] Epoch 32 [7680/8158] loss=0.007783183901725958\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7958399476455996\n",
            "[0m 23s] Epoch 33 [640/8158] loss=0.007455177139490843\n",
            "[0m 23s] Epoch 33 [1280/8158] loss=0.007659677974879741\n",
            "[0m 24s] Epoch 33 [1920/8158] loss=0.0078116906496385734\n",
            "[0m 24s] Epoch 33 [2560/8158] loss=0.007909575931262226\n",
            "[0m 24s] Epoch 33 [3200/8158] loss=0.007936090072616935\n",
            "[0m 24s] Epoch 33 [3840/8158] loss=0.007858365292971332\n",
            "[0m 24s] Epoch 33 [4480/8158] loss=0.007828940038702317\n",
            "[0m 24s] Epoch 33 [5120/8158] loss=0.007831910031381995\n",
            "[0m 24s] Epoch 33 [5760/8158] loss=0.007829230159728064\n",
            "[0m 24s] Epoch 33 [6400/8158] loss=0.007784351371228695\n",
            "[0m 24s] Epoch 33 [7040/8158] loss=0.007796975919468836\n",
            "[0m 24s] Epoch 33 [7680/8158] loss=0.007799115295832356\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7958134166994619\n",
            "[0m 24s] Epoch 34 [640/8158] loss=0.008119660150259733\n",
            "[0m 24s] Epoch 34 [1280/8158] loss=0.008113979827612638\n",
            "[0m 24s] Epoch 34 [1920/8158] loss=0.007901229988783598\n",
            "[0m 24s] Epoch 34 [2560/8158] loss=0.00792615864193067\n",
            "[0m 24s] Epoch 34 [3200/8158] loss=0.007820168184116483\n",
            "[0m 24s] Epoch 34 [3840/8158] loss=0.007840586601135631\n",
            "[0m 24s] Epoch 34 [4480/8158] loss=0.007784062717109919\n",
            "[0m 24s] Epoch 34 [5120/8158] loss=0.007847824855707586\n",
            "[0m 24s] Epoch 34 [5760/8158] loss=0.007823581284739905\n",
            "[0m 25s] Epoch 34 [6400/8158] loss=0.007855037348344923\n",
            "[0m 25s] Epoch 34 [7040/8158] loss=0.007859318809245121\n",
            "[0m 25s] Epoch 34 [7680/8158] loss=0.007817242158732066\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dec set: AUC  0.795836789199631\n",
            "[0m 25s] Epoch 35 [640/8158] loss=0.007140534557402134\n",
            "[0m 25s] Epoch 35 [1280/8158] loss=0.007866774825379253\n",
            "[0m 25s] Epoch 35 [1920/8158] loss=0.007968888571485876\n",
            "[0m 25s] Epoch 35 [2560/8158] loss=0.007956613251008094\n",
            "[0m 25s] Epoch 35 [3200/8158] loss=0.008005296997725963\n",
            "[0m 25s] Epoch 35 [3840/8158] loss=0.007987091961937646\n",
            "[0m 25s] Epoch 35 [4480/8158] loss=0.007949418414916311\n",
            "[0m 25s] Epoch 35 [5120/8158] loss=0.007892654795432464\n",
            "[0m 25s] Epoch 35 [5760/8158] loss=0.007842192276277476\n",
            "[0m 25s] Epoch 35 [6400/8158] loss=0.007825688943266868\n",
            "[0m 25s] Epoch 35 [7040/8158] loss=0.007825694724240086\n",
            "[0m 25s] Epoch 35 [7680/8158] loss=0.007820547379863759\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dec set: AUC  0.7958355258212432\n",
            "[0m 26s] Epoch 36 [640/8158] loss=0.008055664505809546\n",
            "[0m 26s] Epoch 36 [1280/8158] loss=0.007836344861425459\n",
            "[0m 26s] Epoch 36 [1920/8158] loss=0.007896474764371912\n",
            "[0m 26s] Epoch 36 [2560/8158] loss=0.00787674329476431\n",
            "[0m 26s] Epoch 36 [3200/8158] loss=0.007848968878388405\n",
            "[0m 26s] Epoch 36 [3840/8158] loss=0.0077427601364130775\n",
            "[0m 26s] Epoch 36 [4480/8158] loss=0.007669582663636122\n",
            "[0m 26s] Epoch 36 [5120/8158] loss=0.007698565995087847\n",
            "[0m 26s] Epoch 36 [5760/8158] loss=0.0077565328631964\n",
            "[0m 26s] Epoch 36 [6400/8158] loss=0.007736393320374191\n",
            "[0m 26s] Epoch 36 [7040/8158] loss=0.007755797483365644\n",
            "[0m 26s] Epoch 36 [7680/8158] loss=0.007754292734898627\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7947566006783079\n",
            "[0m 26s] Epoch 37 [640/8158] loss=0.007917187782004476\n",
            "[0m 26s] Epoch 37 [1280/8158] loss=0.00816901985090226\n",
            "[0m 26s] Epoch 37 [1920/8158] loss=0.008030364119137327\n",
            "[0m 26s] Epoch 37 [2560/8158] loss=0.008038589695934207\n",
            "[0m 26s] Epoch 37 [3200/8158] loss=0.007895472887903453\n",
            "[0m 26s] Epoch 37 [3840/8158] loss=0.00787733489026626\n",
            "[0m 26s] Epoch 37 [4480/8158] loss=0.007796478444444282\n",
            "[0m 27s] Epoch 37 [5120/8158] loss=0.0077231741743162274\n",
            "[0m 27s] Epoch 37 [5760/8158] loss=0.007737545782907141\n",
            "[0m 27s] Epoch 37 [6400/8158] loss=0.007779872086830437\n",
            "[0m 27s] Epoch 37 [7040/8158] loss=0.007791648974472826\n",
            "[0m 27s] Epoch 37 [7680/8158] loss=0.007802591185706357\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7956485458198915\n",
            "[0m 27s] Epoch 38 [640/8158] loss=0.007986806379631162\n",
            "[0m 27s] Epoch 38 [1280/8158] loss=0.00789715563878417\n",
            "[0m 27s] Epoch 38 [1920/8158] loss=0.007888001312191287\n",
            "[0m 27s] Epoch 38 [2560/8158] loss=0.007991122163366527\n",
            "[0m 27s] Epoch 38 [3200/8158] loss=0.007905421443283558\n",
            "[0m 27s] Epoch 38 [3840/8158] loss=0.007835226521516839\n",
            "[0m 27s] Epoch 38 [4480/8158] loss=0.007832203419612986\n",
            "[0m 27s] Epoch 38 [5120/8158] loss=0.007782823487650603\n",
            "[0m 27s] Epoch 38 [5760/8158] loss=0.007789267361578014\n",
            "[0m 27s] Epoch 38 [6400/8158] loss=0.0078003084147349\n",
            "[0m 27s] Epoch 38 [7040/8158] loss=0.007792940202423117\n",
            "[0m 27s] Epoch 38 [7680/8158] loss=0.007770986723092695\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.795038334058723\n",
            "[0m 28s] Epoch 39 [640/8158] loss=0.007335102930665016\n",
            "[0m 28s] Epoch 39 [1280/8158] loss=0.00776737870182842\n",
            "[0m 28s] Epoch 39 [1920/8158] loss=0.007942501998816928\n",
            "[0m 28s] Epoch 39 [2560/8158] loss=0.00785941774956882\n",
            "[0m 28s] Epoch 39 [3200/8158] loss=0.007860167203471064\n",
            "[0m 28s] Epoch 39 [3840/8158] loss=0.007830652594566345\n",
            "[0m 28s] Epoch 39 [4480/8158] loss=0.007785059199003237\n",
            "[0m 28s] Epoch 39 [5120/8158] loss=0.007832728425273671\n",
            "[0m 28s] Epoch 39 [5760/8158] loss=0.007866960712191132\n",
            "[0m 28s] Epoch 39 [6400/8158] loss=0.007864968148060142\n",
            "[0m 28s] Epoch 39 [7040/8158] loss=0.007767905976453966\n",
            "[0m 28s] Epoch 39 [7680/8158] loss=0.0078031051166666055\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1974/2658 74.27%\n",
            "Dec set: AUC  0.7952891146686444\n",
            "[0m 28s] Epoch 40 [640/8158] loss=0.007656577741727233\n",
            "[0m 28s] Epoch 40 [1280/8158] loss=0.007409318769350648\n",
            "[0m 28s] Epoch 40 [1920/8158] loss=0.0076643323215345545\n",
            "[0m 28s] Epoch 40 [2560/8158] loss=0.007787960499990731\n",
            "[0m 28s] Epoch 40 [3200/8158] loss=0.007742298124358058\n",
            "[0m 29s] Epoch 40 [3840/8158] loss=0.007820742391049861\n",
            "[0m 29s] Epoch 40 [4480/8158] loss=0.00781688967586628\n",
            "[0m 29s] Epoch 40 [5120/8158] loss=0.007795297261327505\n",
            "[0m 29s] Epoch 40 [5760/8158] loss=0.007803465643276771\n",
            "[0m 29s] Epoch 40 [6400/8158] loss=0.007798991235904396\n",
            "[0m 29s] Epoch 40 [7040/8158] loss=0.007752373945814642\n",
            "[0m 29s] Epoch 40 [7680/8158] loss=0.0077914869179949164\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7952347893979813\n",
            "[0m 29s] Epoch 41 [640/8158] loss=0.008105080155655742\n",
            "[0m 29s] Epoch 41 [1280/8158] loss=0.007784522185102105\n",
            "[0m 29s] Epoch 41 [1920/8158] loss=0.0075903723016381265\n",
            "[0m 29s] Epoch 41 [2560/8158] loss=0.0077118247514590624\n",
            "[0m 29s] Epoch 41 [3200/8158] loss=0.007709319917485118\n",
            "[0m 29s] Epoch 41 [3840/8158] loss=0.007663691113702953\n",
            "[0m 29s] Epoch 41 [4480/8158] loss=0.007667777534308178\n",
            "[0m 29s] Epoch 41 [5120/8158] loss=0.007742215786129237\n",
            "[0m 29s] Epoch 41 [5760/8158] loss=0.0077932489259789385\n",
            "[0m 29s] Epoch 41 [6400/8158] loss=0.007811876107007265\n",
            "[0m 29s] Epoch 41 [7040/8158] loss=0.007832951026714661\n",
            "[0m 29s] Epoch 41 [7680/8158] loss=0.0078030402190051975\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7946915366913511\n",
            "[0m 30s] Epoch 42 [640/8158] loss=0.008228004351258278\n",
            "[0m 30s] Epoch 42 [1280/8158] loss=0.007757081277668476\n",
            "[0m 30s] Epoch 42 [1920/8158] loss=0.008028432928646604\n",
            "[0m 30s] Epoch 42 [2560/8158] loss=0.007978457794524729\n",
            "[0m 30s] Epoch 42 [3200/8158] loss=0.007936254935339093\n",
            "[0m 30s] Epoch 42 [3840/8158] loss=0.007962952693924307\n",
            "[0m 30s] Epoch 42 [4480/8158] loss=0.00798774394871933\n",
            "[0m 30s] Epoch 42 [5120/8158] loss=0.0079344789206516\n",
            "[0m 30s] Epoch 42 [5760/8158] loss=0.007900716648954484\n",
            "[0m 30s] Epoch 42 [6400/8158] loss=0.007819326343014837\n",
            "[0m 30s] Epoch 42 [7040/8158] loss=0.007820430808615955\n",
            "[0m 30s] Epoch 42 [7680/8158] loss=0.007782271104709556\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7953990285883579\n",
            "[0m 30s] Epoch 43 [640/8158] loss=0.007538651907816529\n",
            "[0m 30s] Epoch 43 [1280/8158] loss=0.007571807317435742\n",
            "[0m 30s] Epoch 43 [1920/8158] loss=0.007727371497700612\n",
            "[0m 30s] Epoch 43 [2560/8158] loss=0.007830471033230424\n",
            "[0m 31s] Epoch 43 [3200/8158] loss=0.007859855256974697\n",
            "[0m 31s] Epoch 43 [3840/8158] loss=0.007808077762213846\n",
            "[0m 31s] Epoch 43 [4480/8158] loss=0.007830113312229514\n",
            "[0m 31s] Epoch 43 [5120/8158] loss=0.007755312620429322\n",
            "[0m 31s] Epoch 43 [5760/8158] loss=0.007718600105080339\n",
            "[0m 31s] Epoch 43 [6400/8158] loss=0.007791908532381058\n",
            "[0m 31s] Epoch 43 [7040/8158] loss=0.007822373145344582\n",
            "[0m 31s] Epoch 43 [7680/8158] loss=0.007818537065759301\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2012/2658 75.70%\n",
            "Dec set: AUC  0.7949221032470719\n",
            "[0m 31s] Epoch 44 [640/8158] loss=0.007817638898268342\n",
            "[0m 31s] Epoch 44 [1280/8158] loss=0.007950855512171983\n",
            "[0m 31s] Epoch 44 [1920/8158] loss=0.007956653165941437\n",
            "[0m 31s] Epoch 44 [2560/8158] loss=0.00795694817788899\n",
            "[0m 31s] Epoch 44 [3200/8158] loss=0.007861870443448424\n",
            "[0m 31s] Epoch 44 [3840/8158] loss=0.007878488255664706\n",
            "[0m 31s] Epoch 44 [4480/8158] loss=0.007812271646357009\n",
            "[0m 31s] Epoch 44 [5120/8158] loss=0.00785833858535625\n",
            "[0m 31s] Epoch 44 [5760/8158] loss=0.007847375246799654\n",
            "[0m 31s] Epoch 44 [6400/8158] loss=0.007816262114793061\n",
            "[0m 31s] Epoch 44 [7040/8158] loss=0.007794845345514742\n",
            "[0m 32s] Epoch 44 [7680/8158] loss=0.007772106770426035\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7951482479784366\n",
            "[0m 32s] Epoch 45 [640/8158] loss=0.0075915450230240825\n",
            "[0m 32s] Epoch 45 [1280/8158] loss=0.007888893177732826\n",
            "[0m 32s] Epoch 45 [1920/8158] loss=0.007865327782928944\n",
            "[0m 32s] Epoch 45 [2560/8158] loss=0.007909455266781152\n",
            "[0m 32s] Epoch 45 [3200/8158] loss=0.007946263561025261\n",
            "[0m 32s] Epoch 45 [3840/8158] loss=0.007831256176965931\n",
            "[0m 32s] Epoch 45 [4480/8158] loss=0.007889843991558466\n",
            "[0m 32s] Epoch 45 [5120/8158] loss=0.007842217403231188\n",
            "[0m 32s] Epoch 45 [5760/8158] loss=0.007850255377383696\n",
            "[0m 32s] Epoch 45 [6400/8158] loss=0.007873525046743452\n",
            "[0m 32s] Epoch 45 [7040/8158] loss=0.007822425739670342\n",
            "[0m 32s] Epoch 45 [7680/8158] loss=0.007830684445798397\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dec set: AUC  0.7935683933048525\n",
            "[0m 32s] Epoch 46 [640/8158] loss=0.0075078485533595085\n",
            "[0m 32s] Epoch 46 [1280/8158] loss=0.007752572768367827\n",
            "[0m 32s] Epoch 46 [1920/8158] loss=0.007784108305349946\n",
            "[0m 33s] Epoch 46 [2560/8158] loss=0.007700710522476584\n",
            "[0m 33s] Epoch 46 [3200/8158] loss=0.007755354875698685\n",
            "[0m 33s] Epoch 46 [3840/8158] loss=0.007816990418359638\n",
            "[0m 33s] Epoch 46 [4480/8158] loss=0.007802091319380062\n",
            "[0m 33s] Epoch 46 [5120/8158] loss=0.007788959745084867\n",
            "[0m 33s] Epoch 46 [5760/8158] loss=0.007791610517435604\n",
            "[0m 33s] Epoch 46 [6400/8158] loss=0.007802209844812751\n",
            "[0m 33s] Epoch 46 [7040/8158] loss=0.007767196346751668\n",
            "[0m 33s] Epoch 46 [7680/8158] loss=0.007775656180456281\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7934508991148139\n",
            "[0m 33s] Epoch 47 [640/8158] loss=0.007651230273768306\n",
            "[0m 33s] Epoch 47 [1280/8158] loss=0.007589639676734805\n",
            "[0m 33s] Epoch 47 [1920/8158] loss=0.00768007398582995\n",
            "[0m 33s] Epoch 47 [2560/8158] loss=0.007711200742051005\n",
            "[0m 33s] Epoch 47 [3200/8158] loss=0.007757558850571513\n",
            "[0m 33s] Epoch 47 [3840/8158] loss=0.0077345899771898985\n",
            "[0m 33s] Epoch 47 [4480/8158] loss=0.007734223701325911\n",
            "[0m 33s] Epoch 47 [5120/8158] loss=0.007752826978685334\n",
            "[0m 33s] Epoch 47 [5760/8158] loss=0.007766975493480762\n",
            "[0m 33s] Epoch 47 [6400/8158] loss=0.0077985550416633485\n",
            "[0m 34s] Epoch 47 [7040/8158] loss=0.00778954080729322\n",
            "[0m 34s] Epoch 47 [7680/8158] loss=0.007782342541031539\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7928729035025271\n",
            "[0m 34s] Epoch 48 [640/8158] loss=0.007345807645469904\n",
            "[0m 34s] Epoch 48 [1280/8158] loss=0.007580722984857857\n",
            "[0m 34s] Epoch 48 [1920/8158] loss=0.007612676965072751\n",
            "[0m 34s] Epoch 48 [2560/8158] loss=0.007566429034341127\n",
            "[0m 34s] Epoch 48 [3200/8158] loss=0.007630196763202548\n",
            "[0m 34s] Epoch 48 [3840/8158] loss=0.007657629818034669\n",
            "[0m 34s] Epoch 48 [4480/8158] loss=0.00770917431052242\n",
            "[0m 34s] Epoch 48 [5120/8158] loss=0.007727113086730242\n",
            "[0m 34s] Epoch 48 [5760/8158] loss=0.007728782259962625\n",
            "[0m 34s] Epoch 48 [6400/8158] loss=0.007723005195148289\n",
            "[0m 34s] Epoch 48 [7040/8158] loss=0.007708027230745012\n",
            "[0m 34s] Epoch 48 [7680/8158] loss=0.007743977561282615\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.794745861962014\n",
            "[0m 34s] Epoch 49 [640/8158] loss=0.007660400308668614\n",
            "[0m 34s] Epoch 49 [1280/8158] loss=0.007872553123161197\n",
            "[0m 35s] Epoch 49 [1920/8158] loss=0.007713680357361833\n",
            "[0m 35s] Epoch 49 [2560/8158] loss=0.007584331883117556\n",
            "[0m 35s] Epoch 49 [3200/8158] loss=0.007573689725250006\n",
            "[0m 35s] Epoch 49 [3840/8158] loss=0.007597780561385056\n",
            "[0m 35s] Epoch 49 [4480/8158] loss=0.0076898424021367515\n",
            "[0m 35s] Epoch 49 [5120/8158] loss=0.007722116686636582\n",
            "[0m 35s] Epoch 49 [5760/8158] loss=0.007743163851814137\n",
            "[0m 35s] Epoch 49 [6400/8158] loss=0.007764169797301292\n",
            "[0m 35s] Epoch 49 [7040/8158] loss=0.007751646235754544\n",
            "[0m 35s] Epoch 49 [7680/8158] loss=0.007729145186021924\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7937932746578298\n",
            "[0m 35s] Epoch 50 [640/8158] loss=0.008109356928616761\n",
            "[0m 35s] Epoch 50 [1280/8158] loss=0.007873163023032249\n",
            "[0m 35s] Epoch 50 [1920/8158] loss=0.007798902969807386\n",
            "[0m 35s] Epoch 50 [2560/8158] loss=0.007750573358498514\n",
            "[0m 35s] Epoch 50 [3200/8158] loss=0.0077944888640195135\n",
            "[0m 36s] Epoch 50 [3840/8158] loss=0.007766340354767938\n",
            "[0m 36s] Epoch 50 [4480/8158] loss=0.007688623839723212\n",
            "[0m 36s] Epoch 50 [5120/8158] loss=0.007760570076061413\n",
            "[0m 36s] Epoch 50 [5760/8158] loss=0.007738292672567897\n",
            "[0m 36s] Epoch 50 [6400/8158] loss=0.0077159238373860715\n",
            "[0m 36s] Epoch 50 [7040/8158] loss=0.00774105829610066\n",
            "[0m 36s] Epoch 50 [7680/8158] loss=0.007779681550649305\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7935867122914715\n",
            "[0m 36s] Epoch 51 [640/8158] loss=0.0074476170353591446\n",
            "[0m 36s] Epoch 51 [1280/8158] loss=0.007739788945764303\n",
            "[0m 36s] Epoch 51 [1920/8158] loss=0.007749585434794426\n",
            "[0m 36s] Epoch 51 [2560/8158] loss=0.007756311597768217\n",
            "[0m 36s] Epoch 51 [3200/8158] loss=0.007738486789166927\n",
            "[0m 36s] Epoch 51 [3840/8158] loss=0.007700147161570688\n",
            "[0m 36s] Epoch 51 [4480/8158] loss=0.007705673515530569\n",
            "[0m 36s] Epoch 51 [5120/8158] loss=0.007722519472008571\n",
            "[0m 36s] Epoch 51 [5760/8158] loss=0.007750262521828214\n",
            "[0m 36s] Epoch 51 [6400/8158] loss=0.0077639746246859435\n",
            "[0m 36s] Epoch 51 [7040/8158] loss=0.007766824279149825\n",
            "[0m 36s] Epoch 51 [7680/8158] loss=0.007764340149393926\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7938381245905866\n",
            "[0m 37s] Epoch 52 [640/8158] loss=0.007387285586446524\n",
            "[0m 37s] Epoch 52 [1280/8158] loss=0.007519063563086093\n",
            "[0m 37s] Epoch 52 [1920/8158] loss=0.00744977560825646\n",
            "[0m 37s] Epoch 52 [2560/8158] loss=0.007568323204759508\n",
            "[0m 37s] Epoch 52 [3200/8158] loss=0.007541070440784097\n",
            "[0m 37s] Epoch 52 [3840/8158] loss=0.0075617917037258545\n",
            "[0m 37s] Epoch 52 [4480/8158] loss=0.007587328480024423\n",
            "[0m 37s] Epoch 52 [5120/8158] loss=0.007608286087634042\n",
            "[0m 37s] Epoch 52 [5760/8158] loss=0.007651430709908406\n",
            "[0m 37s] Epoch 52 [6400/8158] loss=0.007623714855872095\n",
            "[0m 37s] Epoch 52 [7040/8158] loss=0.007680960380556909\n",
            "[0m 37s] Epoch 52 [7680/8158] loss=0.007720833139804502\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7949461074364347\n",
            "[0m 37s] Epoch 53 [640/8158] loss=0.008027168177068233\n",
            "[0m 38s] Epoch 53 [1280/8158] loss=0.007900412706658245\n",
            "[0m 38s] Epoch 53 [1920/8158] loss=0.007912538262705009\n",
            "[0m 38s] Epoch 53 [2560/8158] loss=0.007846044062171131\n",
            "[0m 38s] Epoch 53 [3200/8158] loss=0.007909374749287964\n",
            "[0m 38s] Epoch 53 [3840/8158] loss=0.007825972512364388\n",
            "[0m 38s] Epoch 53 [4480/8158] loss=0.00775983352214098\n",
            "[0m 38s] Epoch 53 [5120/8158] loss=0.007742236490594223\n",
            "[0m 38s] Epoch 53 [5760/8158] loss=0.00775822133032812\n",
            "[0m 38s] Epoch 53 [6400/8158] loss=0.007744441186077893\n",
            "[0m 38s] Epoch 53 [7040/8158] loss=0.007754211601885883\n",
            "[0m 38s] Epoch 53 [7680/8158] loss=0.007759805120682964\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7951305606810116\n",
            "[0m 38s] Epoch 54 [640/8158] loss=0.007605817401781678\n",
            "[0m 38s] Epoch 54 [1280/8158] loss=0.007562614581547678\n",
            "[0m 38s] Epoch 54 [1920/8158] loss=0.007606120097140471\n",
            "[0m 38s] Epoch 54 [2560/8158] loss=0.0076786142424680294\n",
            "[0m 38s] Epoch 54 [3200/8158] loss=0.007766045518219471\n",
            "[0m 38s] Epoch 54 [3840/8158] loss=0.007722823438234628\n",
            "[0m 39s] Epoch 54 [4480/8158] loss=0.0077453555539250376\n",
            "[0m 39s] Epoch 54 [5120/8158] loss=0.007706918905023485\n",
            "[0m 39s] Epoch 54 [5760/8158] loss=0.007793987221601937\n",
            "[0m 39s] Epoch 54 [6400/8158] loss=0.007781759286299348\n",
            "[0m 39s] Epoch 54 [7040/8158] loss=0.007715486980636011\n",
            "[0m 39s] Epoch 54 [7680/8158] loss=0.007742476742714643\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2009/2658 75.58%\n",
            "Dec set: AUC  0.7943573731078539\n",
            "[0m 39s] Epoch 55 [640/8158] loss=0.0074908867944031956\n",
            "[0m 39s] Epoch 55 [1280/8158] loss=0.007742635533213615\n",
            "[0m 39s] Epoch 55 [1920/8158] loss=0.0077517527000357704\n",
            "[0m 39s] Epoch 55 [2560/8158] loss=0.007746191718615592\n",
            "[0m 39s] Epoch 55 [3200/8158] loss=0.007804649360477925\n",
            "[0m 39s] Epoch 55 [3840/8158] loss=0.007791473949328065\n",
            "[0m 39s] Epoch 55 [4480/8158] loss=0.007850690059629933\n",
            "[0m 39s] Epoch 55 [5120/8158] loss=0.007789666904136539\n",
            "[0m 39s] Epoch 55 [5760/8158] loss=0.007803175318986178\n",
            "[0m 39s] Epoch 55 [6400/8158] loss=0.007733726506121457\n",
            "[0m 39s] Epoch 55 [7040/8158] loss=0.007713743769140406\n",
            "[0m 39s] Epoch 55 [7680/8158] loss=0.007757337022727976\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7933454070194568\n",
            "[0m 40s] Epoch 56 [640/8158] loss=0.007677921745926142\n",
            "[0m 40s] Epoch 56 [1280/8158] loss=0.007801145338453353\n",
            "[0m 40s] Epoch 56 [1920/8158] loss=0.00793917130989333\n",
            "[0m 40s] Epoch 56 [2560/8158] loss=0.007827694714069366\n",
            "[0m 40s] Epoch 56 [3200/8158] loss=0.007882468421012163\n",
            "[0m 40s] Epoch 56 [3840/8158] loss=0.007814015595552821\n",
            "[0m 40s] Epoch 56 [4480/8158] loss=0.0077833465300500395\n",
            "[0m 40s] Epoch 56 [5120/8158] loss=0.007734788866946474\n",
            "[0m 40s] Epoch 56 [5760/8158] loss=0.007757438553704156\n",
            "[0m 40s] Epoch 56 [6400/8158] loss=0.007777576078660786\n",
            "[0m 40s] Epoch 56 [7040/8158] loss=0.00773243231868202\n",
            "[0m 40s] Epoch 56 [7680/8158] loss=0.0077036500403968\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7937920112794422\n",
            "[0m 40s] Epoch 57 [640/8158] loss=0.007427781028673053\n",
            "[0m 40s] Epoch 57 [1280/8158] loss=0.007791705289855599\n",
            "[0m 41s] Epoch 57 [1920/8158] loss=0.0076563091793408\n",
            "[0m 41s] Epoch 57 [2560/8158] loss=0.007608231320045888\n",
            "[0m 41s] Epoch 57 [3200/8158] loss=0.007537574199959636\n",
            "[0m 41s] Epoch 57 [3840/8158] loss=0.007640097104012966\n",
            "[0m 41s] Epoch 57 [4480/8158] loss=0.007607469328546099\n",
            "[0m 41s] Epoch 57 [5120/8158] loss=0.007619036606047303\n",
            "[0m 41s] Epoch 57 [5760/8158] loss=0.0076479170833610825\n",
            "[0m 41s] Epoch 57 [6400/8158] loss=0.0076391368499025706\n",
            "[0m 41s] Epoch 57 [7040/8158] loss=0.007685090051117269\n",
            "[0m 41s] Epoch 57 [7680/8158] loss=0.00772075691105177\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7934919589124081\n",
            "[0m 41s] Epoch 58 [640/8158] loss=0.00813553249463439\n",
            "[0m 41s] Epoch 58 [1280/8158] loss=0.007871880824677647\n",
            "[0m 41s] Epoch 58 [1920/8158] loss=0.00787692794886728\n",
            "[0m 41s] Epoch 58 [2560/8158] loss=0.007648599392268807\n",
            "[0m 41s] Epoch 58 [3200/8158] loss=0.00762286463752389\n",
            "[0m 41s] Epoch 58 [3840/8158] loss=0.0076100987925504645\n",
            "[0m 41s] Epoch 58 [4480/8158] loss=0.007548883996371712\n",
            "[0m 41s] Epoch 58 [5120/8158] loss=0.007619788247393444\n",
            "[0m 42s] Epoch 58 [5760/8158] loss=0.007638991023931238\n",
            "[0m 42s] Epoch 58 [6400/8158] loss=0.007679227609187365\n",
            "[0m 42s] Epoch 58 [7040/8158] loss=0.007709852449426597\n",
            "[0m 42s] Epoch 58 [7680/8158] loss=0.007736951912132402\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7939126639154497\n",
            "[0m 42s] Epoch 59 [640/8158] loss=0.007568727945908904\n",
            "[0m 42s] Epoch 59 [1280/8158] loss=0.0076181907439604405\n",
            "[0m 42s] Epoch 59 [1920/8158] loss=0.007596637572472294\n",
            "[0m 42s] Epoch 59 [2560/8158] loss=0.007722971145994961\n",
            "[0m 42s] Epoch 59 [3200/8158] loss=0.0077435410767793655\n",
            "[0m 42s] Epoch 59 [3840/8158] loss=0.007822959839055935\n",
            "[0m 42s] Epoch 59 [4480/8158] loss=0.007854772206129772\n",
            "[0m 42s] Epoch 59 [5120/8158] loss=0.007838725694455206\n",
            "[0m 42s] Epoch 59 [5760/8158] loss=0.007794462082286676\n",
            "[0m 43s] Epoch 59 [6400/8158] loss=0.007782460916787386\n",
            "[0m 43s] Epoch 59 [7040/8158] loss=0.007775785464962775\n",
            "[0m 43s] Epoch 59 [7680/8158] loss=0.0077064418621982135\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.794020682767582\n",
            "[0m 43s] Epoch 60 [640/8158] loss=0.007899545785039663\n",
            "[0m 43s] Epoch 60 [1280/8158] loss=0.007984638307243585\n",
            "[0m 43s] Epoch 60 [1920/8158] loss=0.007937181383992235\n",
            "[0m 43s] Epoch 60 [2560/8158] loss=0.007763209112454206\n",
            "[0m 43s] Epoch 60 [3200/8158] loss=0.007745341612026096\n",
            "[0m 43s] Epoch 60 [3840/8158] loss=0.007700931925016145\n",
            "[0m 43s] Epoch 60 [4480/8158] loss=0.007698778522067837\n",
            "[0m 43s] Epoch 60 [5120/8158] loss=0.007652096217498183\n",
            "[0m 43s] Epoch 60 [5760/8158] loss=0.007725189502040545\n",
            "[0m 43s] Epoch 60 [6400/8158] loss=0.007738515739329159\n",
            "[0m 43s] Epoch 60 [7040/8158] loss=0.007678193831816315\n",
            "[0m 43s] Epoch 60 [7680/8158] loss=0.007692655835611125\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7934338435065824\n",
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [1280/8158] loss=0.005148096662014723\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.005016655125655234\n",
            "[0m 0s] Epoch 1 [3840/8158] loss=0.005001893018682798\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.004928454046603292\n",
            "[0m 0s] Epoch 1 [6400/8158] loss=0.004836015254259109\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.00475983761716634\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1873/2658 70.47%\n",
            "Dec set: AUC  0.7628872491641172\n",
            "[0m 0s] Epoch 2 [1280/8158] loss=0.004244337091222405\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.0042227109428495165\n",
            "[0m 0s] Epoch 2 [3840/8158] loss=0.00413007988439252\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.00410991219105199\n",
            "[0m 0s] Epoch 2 [6400/8158] loss=0.004146248772740364\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.004127693851478398\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1966/2658 73.97%\n",
            "Dec set: AUC  0.7834657880291109\n",
            "[0m 0s] Epoch 3 [1280/8158] loss=0.00395870553329587\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.003984487731941044\n",
            "[0m 0s] Epoch 3 [3840/8158] loss=0.004005039964492123\n",
            "[0m 1s] Epoch 3 [5120/8158] loss=0.00403782312059775\n",
            "[0m 1s] Epoch 3 [6400/8158] loss=0.004045433569699526\n",
            "[0m 1s] Epoch 3 [7680/8158] loss=0.004031438198095808\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1957/2658 73.63%\n",
            "Dec set: AUC  0.7892609046926294\n",
            "[0m 1s] Epoch 4 [1280/8158] loss=0.0041700989007949826\n",
            "[0m 1s] Epoch 4 [2560/8158] loss=0.004087093833368272\n",
            "[0m 1s] Epoch 4 [3840/8158] loss=0.003980261933368941\n",
            "[0m 1s] Epoch 4 [5120/8158] loss=0.0040091403585392985\n",
            "[0m 1s] Epoch 4 [6400/8158] loss=0.004004313563928008\n",
            "[0m 1s] Epoch 4 [7680/8158] loss=0.00402669693576172\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1972/2658 74.19%\n",
            "Dec set: AUC  0.7892482709087544\n",
            "[0m 1s] Epoch 5 [1280/8158] loss=0.0038936817087233066\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.003981440875213594\n",
            "[0m 1s] Epoch 5 [3840/8158] loss=0.0040006907812009255\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.0039933316176757215\n",
            "[0m 1s] Epoch 5 [6400/8158] loss=0.004006268107332289\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.003965606055377672\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1971/2658 74.15%\n",
            "Dec set: AUC  0.7891415154350097\n",
            "[0m 2s] Epoch 6 [1280/8158] loss=0.003965401859022677\n",
            "[0m 2s] Epoch 6 [2560/8158] loss=0.0039769315044395626\n",
            "[0m 2s] Epoch 6 [3840/8158] loss=0.004019481789631148\n",
            "[0m 2s] Epoch 6 [5120/8158] loss=0.0039984930423088375\n",
            "[0m 2s] Epoch 6 [6400/8158] loss=0.003987363143824041\n",
            "[0m 2s] Epoch 6 [7680/8158] loss=0.00399172049947083\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7892226874964072\n",
            "[0m 2s] Epoch 7 [1280/8158] loss=0.003931613452732563\n",
            "[0m 2s] Epoch 7 [2560/8158] loss=0.004000485618598759\n",
            "[0m 2s] Epoch 7 [3840/8158] loss=0.003950602336165805\n",
            "[0m 2s] Epoch 7 [5120/8158] loss=0.003959117137128487\n",
            "[0m 2s] Epoch 7 [6400/8158] loss=0.003990240907296538\n",
            "[0m 2s] Epoch 7 [7680/8158] loss=0.003980690042953938\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1974/2658 74.27%\n",
            "Dec set: AUC  0.7900833640229\n",
            "[0m 2s] Epoch 8 [1280/8158] loss=0.003980143344961107\n",
            "[0m 2s] Epoch 8 [2560/8158] loss=0.003983417281415313\n",
            "[0m 3s] Epoch 8 [3840/8158] loss=0.003956683344828586\n",
            "[0m 3s] Epoch 8 [5120/8158] loss=0.003909645037492737\n",
            "[0m 3s] Epoch 8 [6400/8158] loss=0.003922297470271587\n",
            "[0m 3s] Epoch 8 [7680/8158] loss=0.003953432245180011\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dec set: AUC  0.7902956115920021\n",
            "[0m 3s] Epoch 9 [1280/8158] loss=0.0038734640227630736\n",
            "[0m 3s] Epoch 9 [2560/8158] loss=0.0038549652555957435\n",
            "[0m 3s] Epoch 9 [3840/8158] loss=0.0038860964744041365\n",
            "[0m 3s] Epoch 9 [5120/8158] loss=0.0038853418081998825\n",
            "[0m 3s] Epoch 9 [6400/8158] loss=0.00393361761700362\n",
            "[0m 3s] Epoch 9 [7680/8158] loss=0.003965831447082261\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.790811069974107\n",
            "[0m 3s] Epoch 10 [1280/8158] loss=0.004060247913002968\n",
            "[0m 3s] Epoch 10 [2560/8158] loss=0.003948768856935203\n",
            "[0m 4s] Epoch 10 [3840/8158] loss=0.003924675968786081\n",
            "[0m 4s] Epoch 10 [5120/8158] loss=0.003927795076742768\n",
            "[0m 4s] Epoch 10 [6400/8158] loss=0.003956302036531269\n",
            "[0m 4s] Epoch 10 [7680/8158] loss=0.003951519289209197\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7911307047061475\n",
            "[0m 4s] Epoch 11 [1280/8158] loss=0.004267733846791089\n",
            "[0m 4s] Epoch 11 [2560/8158] loss=0.004071513668168336\n",
            "[0m 4s] Epoch 11 [3840/8158] loss=0.004003072010042767\n",
            "[0m 4s] Epoch 11 [5120/8158] loss=0.004002360458252951\n",
            "[0m 4s] Epoch 11 [6400/8158] loss=0.003944542328827083\n",
            "[0m 4s] Epoch 11 [7680/8158] loss=0.0039485912071540955\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7917459699808662\n",
            "[0m 4s] Epoch 12 [1280/8158] loss=0.0036903050960972903\n",
            "[0m 4s] Epoch 12 [2560/8158] loss=0.003851167601533234\n",
            "[0m 4s] Epoch 12 [3840/8158] loss=0.0039089144595588245\n",
            "[0m 5s] Epoch 12 [5120/8158] loss=0.003932757582515478\n",
            "[0m 5s] Epoch 12 [6400/8158] loss=0.003919555908069015\n",
            "[0m 5s] Epoch 12 [7680/8158] loss=0.0039198343564445775\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7917766069067633\n",
            "[0m 5s] Epoch 13 [1280/8158] loss=0.0041623533004894854\n",
            "[0m 5s] Epoch 13 [2560/8158] loss=0.0040343551081605256\n",
            "[0m 5s] Epoch 13 [3840/8158] loss=0.003972873596164088\n",
            "[0m 5s] Epoch 13 [5120/8158] loss=0.0039649624668527395\n",
            "[0m 5s] Epoch 13 [6400/8158] loss=0.003923671608790755\n",
            "[0m 5s] Epoch 13 [7680/8158] loss=0.003929636829222242\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7919727464014246\n",
            "[0m 5s] Epoch 14 [1280/8158] loss=0.003930094535462558\n",
            "[0m 5s] Epoch 14 [2560/8158] loss=0.0039141102693974975\n",
            "[0m 5s] Epoch 14 [3840/8158] loss=0.003956550150178373\n",
            "[0m 5s] Epoch 14 [5120/8158] loss=0.003934983606450259\n",
            "[0m 5s] Epoch 14 [6400/8158] loss=0.003919932991266251\n",
            "[0m 5s] Epoch 14 [7680/8158] loss=0.003945260002122571\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7927137178257005\n",
            "[0m 6s] Epoch 15 [1280/8158] loss=0.003955155494622886\n",
            "[0m 6s] Epoch 15 [2560/8158] loss=0.003914344869554043\n",
            "[0m 6s] Epoch 15 [3840/8158] loss=0.003908536055435737\n",
            "[0m 6s] Epoch 15 [5120/8158] loss=0.003876800887519494\n",
            "[0m 6s] Epoch 15 [6400/8158] loss=0.0038999945623800158\n",
            "[0m 6s] Epoch 15 [7680/8158] loss=0.003939876860628525\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7931426347882609\n",
            "[0m 6s] Epoch 16 [1280/8158] loss=0.003908051317557693\n",
            "[0m 6s] Epoch 16 [2560/8158] loss=0.003889080975204706\n",
            "[0m 6s] Epoch 16 [3840/8158] loss=0.003912652873744567\n",
            "[0m 6s] Epoch 16 [5120/8158] loss=0.0039034386281855404\n",
            "[0m 6s] Epoch 16 [6400/8158] loss=0.003923136019147932\n",
            "[0m 6s] Epoch 16 [7680/8158] loss=0.0039203303128791354\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7934521624932013\n",
            "[0m 6s] Epoch 17 [1280/8158] loss=0.003915957291610539\n",
            "[0m 6s] Epoch 17 [2560/8158] loss=0.003886700072325766\n",
            "[0m 7s] Epoch 17 [3840/8158] loss=0.003960273236346742\n",
            "[0m 7s] Epoch 17 [5120/8158] loss=0.003953737783012912\n",
            "[0m 7s] Epoch 17 [6400/8158] loss=0.003928991444408894\n",
            "[0m 7s] Epoch 17 [7680/8158] loss=0.0039311144462165735\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dec set: AUC  0.7937579000629794\n",
            "[0m 7s] Epoch 18 [1280/8158] loss=0.0039081819588318465\n",
            "[0m 7s] Epoch 18 [2560/8158] loss=0.003993944195099175\n",
            "[0m 7s] Epoch 18 [3840/8158] loss=0.00400677490979433\n",
            "[0m 7s] Epoch 18 [5120/8158] loss=0.003976458904799074\n",
            "[0m 7s] Epoch 18 [6400/8158] loss=0.003947096737101674\n",
            "[0m 7s] Epoch 18 [7680/8158] loss=0.003915716373982529\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7937547416170105\n",
            "[0m 7s] Epoch 19 [1280/8158] loss=0.003936937195248902\n",
            "[0m 7s] Epoch 19 [2560/8158] loss=0.00393364104675129\n",
            "[0m 8s] Epoch 19 [3840/8158] loss=0.003920163842849433\n",
            "[0m 8s] Epoch 19 [5120/8158] loss=0.0039022300858050584\n",
            "[0m 8s] Epoch 19 [6400/8158] loss=0.0038935828860849142\n",
            "[0m 8s] Epoch 19 [7680/8158] loss=0.003910649490232269\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.794225350066359\n",
            "[0m 8s] Epoch 20 [1280/8158] loss=0.0038298138184472918\n",
            "[0m 8s] Epoch 20 [2560/8158] loss=0.0038974368828348814\n",
            "[0m 8s] Epoch 20 [3840/8158] loss=0.003947059569569925\n",
            "[0m 8s] Epoch 20 [5120/8158] loss=0.003926821920322255\n",
            "[0m 8s] Epoch 20 [6400/8158] loss=0.003922641943208873\n",
            "[0m 8s] Epoch 20 [7680/8158] loss=0.003932919741297762\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.79407753479502\n",
            "[0m 8s] Epoch 21 [1280/8158] loss=0.004032830684445798\n",
            "[0m 8s] Epoch 21 [2560/8158] loss=0.0040404948638752105\n",
            "[0m 8s] Epoch 21 [3840/8158] loss=0.004031831763374309\n",
            "[0m 8s] Epoch 21 [5120/8158] loss=0.003993655391968787\n",
            "[0m 9s] Epoch 21 [6400/8158] loss=0.003994934498332441\n",
            "[0m 9s] Epoch 21 [7680/8158] loss=0.003944917923460404\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7947603908134705\n",
            "[0m 9s] Epoch 22 [1280/8158] loss=0.003952211956493557\n",
            "[0m 9s] Epoch 22 [2560/8158] loss=0.0038834166480228305\n",
            "[0m 9s] Epoch 22 [3840/8158] loss=0.0038374822276333966\n",
            "[0m 9s] Epoch 22 [5120/8158] loss=0.0038866816787049173\n",
            "[0m 9s] Epoch 22 [6400/8158] loss=0.003910067728720605\n",
            "[0m 9s] Epoch 22 [7680/8158] loss=0.00391538767532135\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7946169973664877\n",
            "[0m 9s] Epoch 23 [1280/8158] loss=0.003924026573076844\n",
            "[0m 9s] Epoch 23 [2560/8158] loss=0.003952836396638304\n",
            "[0m 9s] Epoch 23 [3840/8158] loss=0.003954778929861884\n",
            "[0m 9s] Epoch 23 [5120/8158] loss=0.003954233846161515\n",
            "[0m 9s] Epoch 23 [6400/8158] loss=0.0038986057695001362\n",
            "[0m 9s] Epoch 23 [7680/8158] loss=0.003914239773682008\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7949448440580472\n",
            "[0m 10s] Epoch 24 [1280/8158] loss=0.0038742860313504936\n",
            "[0m 10s] Epoch 24 [2560/8158] loss=0.0038713588495738805\n",
            "[0m 10s] Epoch 24 [3840/8158] loss=0.0038837433249379197\n",
            "[0m 10s] Epoch 24 [5120/8158] loss=0.0038936064520385116\n",
            "[0m 10s] Epoch 24 [6400/8158] loss=0.003941407715901732\n",
            "[0m 10s] Epoch 24 [7680/8158] loss=0.003912332374602557\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7949600045986972\n",
            "[0m 10s] Epoch 25 [1280/8158] loss=0.003738708281889558\n",
            "[0m 10s] Epoch 25 [2560/8158] loss=0.003920588770415634\n",
            "[0m 10s] Epoch 25 [3840/8158] loss=0.003957258475323518\n",
            "[0m 10s] Epoch 25 [5120/8158] loss=0.003945437341462821\n",
            "[0m 10s] Epoch 25 [6400/8158] loss=0.003906220863573253\n",
            "[0m 10s] Epoch 25 [7680/8158] loss=0.003918285586405545\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7952594252765378\n",
            "[0m 10s] Epoch 26 [1280/8158] loss=0.0038501511793583632\n",
            "[0m 10s] Epoch 26 [2560/8158] loss=0.00394168149214238\n",
            "[0m 10s] Epoch 26 [3840/8158] loss=0.00396534432657063\n",
            "[0m 11s] Epoch 26 [5120/8158] loss=0.003935195918893441\n",
            "[0m 11s] Epoch 26 [6400/8158] loss=0.003943343358114362\n",
            "[0m 11s] Epoch 26 [7680/8158] loss=0.003912028934185703\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dec set: AUC  0.7931830628966614\n",
            "[0m 11s] Epoch 27 [1280/8158] loss=0.003745331452228129\n",
            "[0m 11s] Epoch 27 [2560/8158] loss=0.0038368995068594814\n",
            "[0m 11s] Epoch 27 [3840/8158] loss=0.0038429494481533764\n",
            "[0m 11s] Epoch 27 [5120/8158] loss=0.0038789582380559296\n",
            "[0m 11s] Epoch 27 [6400/8158] loss=0.0039052234310656786\n",
            "[0m 11s] Epoch 27 [7680/8158] loss=0.003918208783337225\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7953548103447949\n",
            "[0m 11s] Epoch 28 [1280/8158] loss=0.0038822891423478723\n",
            "[0m 11s] Epoch 28 [2560/8158] loss=0.0038879915606230496\n",
            "[0m 11s] Epoch 28 [3840/8158] loss=0.003961340532017251\n",
            "[0m 11s] Epoch 28 [5120/8158] loss=0.003990458720363676\n",
            "[0m 11s] Epoch 28 [6400/8158] loss=0.003957537161186337\n",
            "[0m 11s] Epoch 28 [7680/8158] loss=0.003919274491878847\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.795077498788736\n",
            "[0m 12s] Epoch 29 [1280/8158] loss=0.004112042183987796\n",
            "[0m 12s] Epoch 29 [2560/8158] loss=0.004060084722004831\n",
            "[0m 12s] Epoch 29 [3840/8158] loss=0.0039743926065663494\n",
            "[0m 12s] Epoch 29 [5120/8158] loss=0.0039469319162890315\n",
            "[0m 12s] Epoch 29 [6400/8158] loss=0.003913481272757053\n",
            "[0m 12s] Epoch 29 [7680/8158] loss=0.003909399822199096\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7954066088586829\n",
            "[0m 12s] Epoch 30 [1280/8158] loss=0.0038458220195025207\n",
            "[0m 12s] Epoch 30 [2560/8158] loss=0.003795723069924861\n",
            "[0m 12s] Epoch 30 [3840/8158] loss=0.0038287597553183636\n",
            "[0m 12s] Epoch 30 [5120/8158] loss=0.003868577565299347\n",
            "[0m 12s] Epoch 30 [6400/8158] loss=0.0038660567393526434\n",
            "[0m 12s] Epoch 30 [7680/8158] loss=0.0039102513692341745\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7955885353464847\n",
            "[0m 12s] Epoch 31 [1280/8158] loss=0.003951822593808174\n",
            "[0m 12s] Epoch 31 [2560/8158] loss=0.0038774017011746764\n",
            "[0m 13s] Epoch 31 [3840/8158] loss=0.003949350281618536\n",
            "[0m 13s] Epoch 31 [5120/8158] loss=0.003934490622486919\n",
            "[0m 13s] Epoch 31 [6400/8158] loss=0.003914860915392638\n",
            "[0m 13s] Epoch 31 [7680/8158] loss=0.0039035403014471134\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7953598638583449\n",
            "[0m 13s] Epoch 32 [1280/8158] loss=0.003943579364567995\n",
            "[0m 13s] Epoch 32 [2560/8158] loss=0.003900298324879259\n",
            "[0m 13s] Epoch 32 [3840/8158] loss=0.003915515068608026\n",
            "[0m 13s] Epoch 32 [5120/8158] loss=0.0038984038052149116\n",
            "[0m 13s] Epoch 32 [6400/8158] loss=0.003937292220070958\n",
            "[0m 13s] Epoch 32 [7680/8158] loss=0.003919814779267957\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7954773580483837\n",
            "[0m 13s] Epoch 33 [1280/8158] loss=0.0037986177951097487\n",
            "[0m 13s] Epoch 33 [2560/8158] loss=0.0039021716685965655\n",
            "[0m 13s] Epoch 33 [3840/8158] loss=0.0039031055833523474\n",
            "[0m 13s] Epoch 33 [5120/8158] loss=0.003934108058456331\n",
            "[0m 13s] Epoch 33 [6400/8158] loss=0.003911436819471419\n",
            "[0m 14s] Epoch 33 [7680/8158] loss=0.003901850905579825\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7954514587914396\n",
            "[0m 14s] Epoch 34 [1280/8158] loss=0.003829205664806068\n",
            "[0m 14s] Epoch 34 [2560/8158] loss=0.0038371406379155816\n",
            "[0m 14s] Epoch 34 [3840/8158] loss=0.0039272896867866315\n",
            "[0m 14s] Epoch 34 [5120/8158] loss=0.003920458850916475\n",
            "[0m 14s] Epoch 34 [6400/8158] loss=0.0039204480219632385\n",
            "[0m 14s] Epoch 34 [7680/8158] loss=0.003915073776928087\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7957142414960421\n",
            "[0m 14s] Epoch 35 [1280/8158] loss=0.00398775793146342\n",
            "[0m 14s] Epoch 35 [2560/8158] loss=0.003910169412847608\n",
            "[0m 14s] Epoch 35 [3840/8158] loss=0.003937163405741254\n",
            "[0m 14s] Epoch 35 [5120/8158] loss=0.003942955628735945\n",
            "[0m 14s] Epoch 35 [6400/8158] loss=0.00392395157366991\n",
            "[0m 14s] Epoch 35 [7680/8158] loss=0.003897159449600925\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.7958386842672122\n",
            "[0m 14s] Epoch 36 [1280/8158] loss=0.003840691177174449\n",
            "[0m 15s] Epoch 36 [2560/8158] loss=0.0038331807125359775\n",
            "[0m 15s] Epoch 36 [3840/8158] loss=0.0038665652197475233\n",
            "[0m 15s] Epoch 36 [5120/8158] loss=0.0039012759807519615\n",
            "[0m 15s] Epoch 36 [6400/8158] loss=0.0038837602082639933\n",
            "[0m 15s] Epoch 36 [7680/8158] loss=0.003908341424539686\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2009/2658 75.58%\n",
            "Dec set: AUC  0.7958721637944811\n",
            "[0m 15s] Epoch 37 [1280/8158] loss=0.003786631068214774\n",
            "[0m 15s] Epoch 37 [2560/8158] loss=0.003865418676286936\n",
            "[0m 15s] Epoch 37 [3840/8158] loss=0.00387141447669516\n",
            "[0m 15s] Epoch 37 [5120/8158] loss=0.0039016028575133534\n",
            "[0m 15s] Epoch 37 [6400/8158] loss=0.003919367138296366\n",
            "[0m 15s] Epoch 37 [7680/8158] loss=0.0038931622247522077\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7945809910824437\n",
            "[0m 15s] Epoch 38 [1280/8158] loss=0.0038532624021172523\n",
            "[0m 15s] Epoch 38 [2560/8158] loss=0.0038625673041678965\n",
            "[0m 16s] Epoch 38 [3840/8158] loss=0.00390852353690813\n",
            "[0m 16s] Epoch 38 [5120/8158] loss=0.003906499617733062\n",
            "[0m 16s] Epoch 38 [6400/8158] loss=0.003911720630712807\n",
            "[0m 16s] Epoch 38 [7680/8158] loss=0.00390866967694213\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.796020610755014\n",
            "[0m 16s] Epoch 39 [1280/8158] loss=0.004037825763225556\n",
            "[0m 16s] Epoch 39 [2560/8158] loss=0.003936876589432359\n",
            "[0m 16s] Epoch 39 [3840/8158] loss=0.0038755124046777685\n",
            "[0m 16s] Epoch 39 [5120/8158] loss=0.0038662918785121294\n",
            "[0m 16s] Epoch 39 [6400/8158] loss=0.0038713620975613593\n",
            "[0m 16s] Epoch 39 [7680/8158] loss=0.003881919321914514\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.796162740823609\n",
            "[0m 16s] Epoch 40 [1280/8158] loss=0.003815494058653712\n",
            "[0m 16s] Epoch 40 [2560/8158] loss=0.0038573584402911364\n",
            "[0m 17s] Epoch 40 [3840/8158] loss=0.003867361011604468\n",
            "[0m 17s] Epoch 40 [5120/8158] loss=0.0038600827800109984\n",
            "[0m 17s] Epoch 40 [6400/8158] loss=0.00386424754280597\n",
            "[0m 17s] Epoch 40 [7680/8158] loss=0.003888988820835948\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7960863064311645\n",
            "[0m 17s] Epoch 41 [1280/8158] loss=0.003882488119415939\n",
            "[0m 17s] Epoch 41 [2560/8158] loss=0.003900911461096257\n",
            "[0m 17s] Epoch 41 [3840/8158] loss=0.003906577119293312\n",
            "[0m 17s] Epoch 41 [5120/8158] loss=0.003915625601075589\n",
            "[0m 17s] Epoch 41 [6400/8158] loss=0.003882540464401245\n",
            "[0m 17s] Epoch 41 [7680/8158] loss=0.003911206968283901\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dec set: AUC  0.7962916054191352\n",
            "[0m 17s] Epoch 42 [1280/8158] loss=0.0038717435440048576\n",
            "[0m 17s] Epoch 42 [2560/8158] loss=0.003921816509682685\n",
            "[0m 17s] Epoch 42 [3840/8158] loss=0.0038696691083411378\n",
            "[0m 17s] Epoch 42 [5120/8158] loss=0.003878377779619768\n",
            "[0m 17s] Epoch 42 [6400/8158] loss=0.0038642733730375766\n",
            "[0m 18s] Epoch 42 [7680/8158] loss=0.0038899751923357445\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7960079769711388\n",
            "[0m 18s] Epoch 43 [1280/8158] loss=0.004010383668355644\n",
            "[0m 18s] Epoch 43 [2560/8158] loss=0.004034554609097541\n",
            "[0m 18s] Epoch 43 [3840/8158] loss=0.003949773878169556\n",
            "[0m 18s] Epoch 43 [5120/8158] loss=0.003937603835947811\n",
            "[0m 18s] Epoch 43 [6400/8158] loss=0.003918102774769068\n",
            "[0m 18s] Epoch 43 [7680/8158] loss=0.003901990627249082\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7958146800778493\n",
            "[0m 18s] Epoch 44 [1280/8158] loss=0.0039678267668932675\n",
            "[0m 18s] Epoch 44 [2560/8158] loss=0.003927052335347981\n",
            "[0m 18s] Epoch 44 [3840/8158] loss=0.003970355191268027\n",
            "[0m 18s] Epoch 44 [5120/8158] loss=0.003921154636191204\n",
            "[0m 18s] Epoch 44 [6400/8158] loss=0.003908198061399162\n",
            "[0m 18s] Epoch 44 [7680/8158] loss=0.003899977670516819\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7958329990644682\n",
            "[0m 18s] Epoch 45 [1280/8158] loss=0.0038463742239400744\n",
            "[0m 19s] Epoch 45 [2560/8158] loss=0.003885553649161011\n",
            "[0m 19s] Epoch 45 [3840/8158] loss=0.0038809220306575297\n",
            "[0m 19s] Epoch 45 [5120/8158] loss=0.0038571028970181944\n",
            "[0m 19s] Epoch 45 [6400/8158] loss=0.0038786498876288533\n",
            "[0m 19s] Epoch 45 [7680/8158] loss=0.003891866762811939\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7956087494006849\n",
            "[0m 19s] Epoch 46 [1280/8158] loss=0.0037814893294125795\n",
            "[0m 19s] Epoch 46 [2560/8158] loss=0.0038509227451868355\n",
            "[0m 19s] Epoch 46 [3840/8158] loss=0.003836376266553998\n",
            "[0m 19s] Epoch 46 [5120/8158] loss=0.0038873339188285174\n",
            "[0m 19s] Epoch 46 [6400/8158] loss=0.0038761205179616808\n",
            "[0m 19s] Epoch 46 [7680/8158] loss=0.003884282599513729\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7954426151427271\n",
            "[0m 19s] Epoch 47 [1280/8158] loss=0.003988103219307959\n",
            "[0m 19s] Epoch 47 [2560/8158] loss=0.003915460233110934\n",
            "[0m 19s] Epoch 47 [3840/8158] loss=0.003896711370907724\n",
            "[0m 19s] Epoch 47 [5120/8158] loss=0.003867154911858961\n",
            "[0m 20s] Epoch 47 [6400/8158] loss=0.003883010377176106\n",
            "[0m 20s] Epoch 47 [7680/8158] loss=0.0038920241330439847\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7957186633203985\n",
            "[0m 20s] Epoch 48 [1280/8158] loss=0.0038515666034072637\n",
            "[0m 20s] Epoch 48 [2560/8158] loss=0.003786753653548658\n",
            "[0m 20s] Epoch 48 [3840/8158] loss=0.0038906102534383537\n",
            "[0m 20s] Epoch 48 [5120/8158] loss=0.0038950071029830724\n",
            "[0m 20s] Epoch 48 [6400/8158] loss=0.0038823738554492593\n",
            "[0m 20s] Epoch 48 [7680/8158] loss=0.0038950558363770446\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dec set: AUC  0.7952467914926626\n",
            "[0m 20s] Epoch 49 [1280/8158] loss=0.003769525815732777\n",
            "[0m 20s] Epoch 49 [2560/8158] loss=0.0038063730346038936\n",
            "[0m 20s] Epoch 49 [3840/8158] loss=0.0038109210009376207\n",
            "[0m 20s] Epoch 49 [5120/8158] loss=0.0038309542229399084\n",
            "[0m 20s] Epoch 49 [6400/8158] loss=0.0038664851477369666\n",
            "[0m 20s] Epoch 49 [7680/8158] loss=0.0038916833892775077\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dec set: AUC  0.7959069067001379\n",
            "[0m 21s] Epoch 50 [1280/8158] loss=0.0039055298315361143\n",
            "[0m 21s] Epoch 50 [2560/8158] loss=0.0038818250526674093\n",
            "[0m 21s] Epoch 50 [3840/8158] loss=0.003901179685878257\n",
            "[0m 21s] Epoch 50 [5120/8158] loss=0.0038943336461670698\n",
            "[0m 21s] Epoch 50 [6400/8158] loss=0.003904520599171519\n",
            "[0m 21s] Epoch 50 [7680/8158] loss=0.0038848011987283825\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.794881043449478\n",
            "[0m 21s] Epoch 51 [1280/8158] loss=0.003768058307468891\n",
            "[0m 21s] Epoch 51 [2560/8158] loss=0.0038616751087829472\n",
            "[0m 21s] Epoch 51 [3840/8158] loss=0.0038665936794131995\n",
            "[0m 21s] Epoch 51 [5120/8158] loss=0.0038664605759549885\n",
            "[0m 21s] Epoch 51 [6400/8158] loss=0.0038687730813398956\n",
            "[0m 21s] Epoch 51 [7680/8158] loss=0.0038749048641572395\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dec set: AUC  0.7956535993334416\n",
            "[0m 22s] Epoch 52 [1280/8158] loss=0.003771512513048947\n",
            "[0m 22s] Epoch 52 [2560/8158] loss=0.003811393503565341\n",
            "[0m 22s] Epoch 52 [3840/8158] loss=0.003784340457059443\n",
            "[0m 22s] Epoch 52 [5120/8158] loss=0.0038460303738247605\n",
            "[0m 22s] Epoch 52 [6400/8158] loss=0.0038834567880257965\n",
            "[0m 22s] Epoch 52 [7680/8158] loss=0.0038780145812779667\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7949941158151602\n",
            "[0m 22s] Epoch 53 [1280/8158] loss=0.003986277664080262\n",
            "[0m 22s] Epoch 53 [2560/8158] loss=0.00394453062908724\n",
            "[0m 22s] Epoch 53 [3840/8158] loss=0.00395697852751861\n",
            "[0m 22s] Epoch 53 [5120/8158] loss=0.003931703825946898\n",
            "[0m 22s] Epoch 53 [6400/8158] loss=0.0038640533853322267\n",
            "[0m 22s] Epoch 53 [7680/8158] loss=0.003887884241218368\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dec set: AUC  0.7959290158219192\n",
            "[0m 22s] Epoch 54 [1280/8158] loss=0.003845491469837725\n",
            "[0m 22s] Epoch 54 [2560/8158] loss=0.003825693635735661\n",
            "[0m 22s] Epoch 54 [3840/8158] loss=0.003941265625568727\n",
            "[0m 23s] Epoch 54 [5120/8158] loss=0.0039035606780089436\n",
            "[0m 23s] Epoch 54 [6400/8158] loss=0.003889500196091831\n",
            "[0m 23s] Epoch 54 [7680/8158] loss=0.0038781296888676783\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7944199103380358\n",
            "[0m 23s] Epoch 55 [1280/8158] loss=0.004030200303532183\n",
            "[0m 23s] Epoch 55 [2560/8158] loss=0.003953374177217484\n",
            "[0m 23s] Epoch 55 [3840/8158] loss=0.0038649762514978646\n",
            "[0m 23s] Epoch 55 [5120/8158] loss=0.0038755255693104116\n",
            "[0m 23s] Epoch 55 [6400/8158] loss=0.0038746820064261556\n",
            "[0m 23s] Epoch 55 [7680/8158] loss=0.0038762148818932475\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7948526174357589\n",
            "[0m 23s] Epoch 56 [1280/8158] loss=0.0038412756286561487\n",
            "[0m 23s] Epoch 56 [2560/8158] loss=0.003896770847495645\n",
            "[0m 23s] Epoch 56 [3840/8158] loss=0.0038610225620989997\n",
            "[0m 23s] Epoch 56 [5120/8158] loss=0.0038285572081804276\n",
            "[0m 23s] Epoch 56 [6400/8158] loss=0.003883680561557412\n",
            "[0m 23s] Epoch 56 [7680/8158] loss=0.0038760461960919202\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7951918345328058\n",
            "[0m 24s] Epoch 57 [1280/8158] loss=0.0036511521320790052\n",
            "[0m 24s] Epoch 57 [2560/8158] loss=0.0037736578728072346\n",
            "[0m 24s] Epoch 57 [3840/8158] loss=0.0038272322543586295\n",
            "[0m 24s] Epoch 57 [5120/8158] loss=0.0038001192500814796\n",
            "[0m 24s] Epoch 57 [6400/8158] loss=0.0038887101830914618\n",
            "[0m 24s] Epoch 57 [7680/8158] loss=0.003876797517295927\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7922917494442714\n",
            "[0m 24s] Epoch 58 [1280/8158] loss=0.003969901520758867\n",
            "[0m 24s] Epoch 58 [2560/8158] loss=0.003941564110573381\n",
            "[0m 24s] Epoch 58 [3840/8158] loss=0.003900059941224754\n",
            "[0m 24s] Epoch 58 [5120/8158] loss=0.0038813495018985124\n",
            "[0m 24s] Epoch 58 [6400/8158] loss=0.003880392326973379\n",
            "[0m 24s] Epoch 58 [7680/8158] loss=0.003880074596963823\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7949151546659408\n",
            "[0m 24s] Epoch 59 [1280/8158] loss=0.0037334106164053082\n",
            "[0m 24s] Epoch 59 [2560/8158] loss=0.0038117442047223447\n",
            "[0m 25s] Epoch 59 [3840/8158] loss=0.0038403091719374062\n",
            "[0m 25s] Epoch 59 [5120/8158] loss=0.00380379386479035\n",
            "[0m 25s] Epoch 59 [6400/8158] loss=0.003852857816964388\n",
            "[0m 25s] Epoch 59 [7680/8158] loss=0.0038622760485547284\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7948538808141462\n",
            "[0m 25s] Epoch 60 [1280/8158] loss=0.0038919309852644803\n",
            "[0m 25s] Epoch 60 [2560/8158] loss=0.003838743641972542\n",
            "[0m 25s] Epoch 60 [3840/8158] loss=0.0038669835698480408\n",
            "[0m 25s] Epoch 60 [5120/8158] loss=0.003878262417856604\n",
            "[0m 25s] Epoch 60 [6400/8158] loss=0.0038449930446222426\n",
            "[0m 25s] Epoch 60 [7680/8158] loss=0.0038680808036588134\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7938583386447867\n",
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.00251942474860698\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.0025036953040398656\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.0024736915482208134\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7531333363233289\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.002294837520457804\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.0022360087372362615\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.0022056975945209463\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1920/2658 72.23%\n",
            "Dec set: AUC  0.7629459962591365\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.0021433352725580336\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.002081438235472888\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.002057656137427936\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1970/2658 74.12%\n",
            "Dec set: AUC  0.7869918771086574\n",
            "[0m 0s] Epoch 4 [2560/8158] loss=0.002003069594502449\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.0020277867384720593\n",
            "[0m 1s] Epoch 4 [7680/8158] loss=0.002025194920133799\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1968/2658 74.04%\n",
            "Dec set: AUC  0.7876235663024136\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.002048235107213259\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.0020263726415578275\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.0020204573559264344\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7888916823588792\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.002003842731937766\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.002009893610375002\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.0019963105985273915\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1963/2658 73.85%\n",
            "Dec set: AUC  0.7896215992222642\n",
            "[0m 1s] Epoch 7 [2560/8158] loss=0.0020129055948927997\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.002001852134708315\n",
            "[0m 1s] Epoch 7 [7680/8158] loss=0.0020063821730824808\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7896421291210615\n",
            "[0m 2s] Epoch 8 [2560/8158] loss=0.001926050242036581\n",
            "[0m 2s] Epoch 8 [5120/8158] loss=0.0019815771898720413\n",
            "[0m 2s] Epoch 8 [7680/8158] loss=0.0019955683732405306\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7893569214500805\n",
            "[0m 2s] Epoch 9 [2560/8158] loss=0.0019672795897349715\n",
            "[0m 2s] Epoch 9 [5120/8158] loss=0.0019648298737592997\n",
            "[0m 2s] Epoch 9 [7680/8158] loss=0.001981835494128366\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7895590619920824\n",
            "[0m 2s] Epoch 10 [2560/8158] loss=0.0020000773947685957\n",
            "[0m 2s] Epoch 10 [5120/8158] loss=0.0019746609730646016\n",
            "[0m 2s] Epoch 10 [7680/8158] loss=0.001981379557400942\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7895413746946573\n",
            "[0m 2s] Epoch 11 [2560/8158] loss=0.001966929784975946\n",
            "[0m 3s] Epoch 11 [5120/8158] loss=0.001968964177649468\n",
            "[0m 3s] Epoch 11 [7680/8158] loss=0.0019886836991645394\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dec set: AUC  0.7897826799666721\n",
            "[0m 3s] Epoch 12 [2560/8158] loss=0.002045017294585705\n",
            "[0m 3s] Epoch 12 [5120/8158] loss=0.002011878212215379\n",
            "[0m 3s] Epoch 12 [7680/8158] loss=0.0019908477008963627\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7901465329422757\n",
            "[0m 3s] Epoch 13 [2560/8158] loss=0.001978612446691841\n",
            "[0m 3s] Epoch 13 [5120/8158] loss=0.0019739645067602396\n",
            "[0m 3s] Epoch 13 [7680/8158] loss=0.0019907596016613145\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dec set: AUC  0.7903158256462023\n",
            "[0m 3s] Epoch 14 [2560/8158] loss=0.0019107896136119963\n",
            "[0m 3s] Epoch 14 [5120/8158] loss=0.0019688682223204523\n",
            "[0m 3s] Epoch 14 [7680/8158] loss=0.001986046612728387\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.790634828689049\n",
            "[0m 4s] Epoch 15 [2560/8158] loss=0.002013461128808558\n",
            "[0m 4s] Epoch 15 [5120/8158] loss=0.0019874554593116043\n",
            "[0m 4s] Epoch 15 [7680/8158] loss=0.0019762747377778094\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7909907855497308\n",
            "[0m 4s] Epoch 16 [2560/8158] loss=0.001966929866466671\n",
            "[0m 4s] Epoch 16 [5120/8158] loss=0.0019627140078227966\n",
            "[0m 4s] Epoch 16 [7680/8158] loss=0.001976405910681933\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7912172461256921\n",
            "[0m 4s] Epoch 17 [2560/8158] loss=0.001977886201348156\n",
            "[0m 4s] Epoch 17 [5120/8158] loss=0.001977868762332946\n",
            "[0m 4s] Epoch 17 [7680/8158] loss=0.00196616070655485\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dec set: AUC  0.7916006814663022\n",
            "[0m 4s] Epoch 18 [2560/8158] loss=0.0019262826070189476\n",
            "[0m 4s] Epoch 18 [5120/8158] loss=0.0019491745333652943\n",
            "[0m 4s] Epoch 18 [7680/8158] loss=0.001976022671442479\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dec set: AUC  0.7917440749132848\n",
            "[0m 5s] Epoch 19 [2560/8158] loss=0.001933488470967859\n",
            "[0m 5s] Epoch 19 [5120/8158] loss=0.001946403761394322\n",
            "[0m 5s] Epoch 19 [7680/8158] loss=0.0019663055737813313\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7921887841056893\n",
            "[0m 5s] Epoch 20 [2560/8158] loss=0.0019446849124506115\n",
            "[0m 5s] Epoch 20 [5120/8158] loss=0.001977189397439361\n",
            "[0m 5s] Epoch 20 [7680/8158] loss=0.0019711078066999713\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1979/2658 74.45%\n",
            "Dec set: AUC  0.7923985049180162\n",
            "[0m 5s] Epoch 21 [2560/8158] loss=0.0019492956693284214\n",
            "[0m 5s] Epoch 21 [5120/8158] loss=0.001976117753656581\n",
            "[0m 5s] Epoch 21 [7680/8158] loss=0.001969735122596224\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7924281943101228\n",
            "[0m 5s] Epoch 22 [2560/8158] loss=0.0019610566087067127\n",
            "[0m 5s] Epoch 22 [5120/8158] loss=0.0019765966571867466\n",
            "[0m 6s] Epoch 22 [7680/8158] loss=0.0019616273464635016\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7929802906654655\n",
            "[0m 6s] Epoch 23 [2560/8158] loss=0.001992918213363737\n",
            "[0m 6s] Epoch 23 [5120/8158] loss=0.0019768602505791932\n",
            "[0m 6s] Epoch 23 [7680/8158] loss=0.0019697705089735487\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7931451615450359\n",
            "[0m 6s] Epoch 24 [2560/8158] loss=0.0019669350935146213\n",
            "[0m 6s] Epoch 24 [5120/8158] loss=0.0019410377542953938\n",
            "[0m 6s] Epoch 24 [7680/8158] loss=0.001966702472418547\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7934925906016019\n",
            "[0m 6s] Epoch 25 [2560/8158] loss=0.0018933829152956605\n",
            "[0m 6s] Epoch 25 [5120/8158] loss=0.0019312990247271956\n",
            "[0m 6s] Epoch 25 [7680/8158] loss=0.001949978720707198\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7937541099278169\n",
            "[0m 6s] Epoch 26 [2560/8158] loss=0.001941504853311926\n",
            "[0m 7s] Epoch 26 [5120/8158] loss=0.001973944081692025\n",
            "[0m 7s] Epoch 26 [7680/8158] loss=0.001960909448098391\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.794004890537738\n",
            "[0m 7s] Epoch 27 [2560/8158] loss=0.0020208112779073416\n",
            "[0m 7s] Epoch 27 [5120/8158] loss=0.0019563720386940988\n",
            "[0m 7s] Epoch 27 [7680/8158] loss=0.0019719743402674796\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7939316145912624\n",
            "[0m 7s] Epoch 28 [2560/8158] loss=0.0020123222726397215\n",
            "[0m 7s] Epoch 28 [5120/8158] loss=0.001967045123456046\n",
            "[0m 7s] Epoch 28 [7680/8158] loss=0.001955187014148881\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.7941583910118208\n",
            "[0m 7s] Epoch 29 [2560/8158] loss=0.0019835014594718815\n",
            "[0m 7s] Epoch 29 [5120/8158] loss=0.001988619432086125\n",
            "[0m 8s] Epoch 29 [7680/8158] loss=0.0019624247138078014\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.793994783510638\n",
            "[0m 8s] Epoch 30 [2560/8158] loss=0.0019438505405560136\n",
            "[0m 8s] Epoch 30 [5120/8158] loss=0.001959215465467423\n",
            "[0m 8s] Epoch 30 [7680/8158] loss=0.0019526886520907284\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7943700068917291\n",
            "[0m 8s] Epoch 31 [2560/8158] loss=0.0019328603870235384\n",
            "[0m 8s] Epoch 31 [5120/8158] loss=0.001969774399185553\n",
            "[0m 8s] Epoch 31 [7680/8158] loss=0.001965113457602759\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7940427918893634\n",
            "[0m 8s] Epoch 32 [2560/8158] loss=0.0020169851719401776\n",
            "[0m 8s] Epoch 32 [5120/8158] loss=0.0019602075102739037\n",
            "[0m 8s] Epoch 32 [7680/8158] loss=0.0019582130946218966\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7946972218940948\n",
            "[0m 9s] Epoch 33 [2560/8158] loss=0.0019846383831463752\n",
            "[0m 9s] Epoch 33 [5120/8158] loss=0.0019407283049076795\n",
            "[0m 9s] Epoch 33 [7680/8158] loss=0.0019563783619863293\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1976/2658 74.34%\n",
            "Dec set: AUC  0.7949189448011031\n",
            "[0m 9s] Epoch 34 [2560/8158] loss=0.0019344357075169682\n",
            "[0m 9s] Epoch 34 [5120/8158] loss=0.0019575467624235897\n",
            "[0m 9s] Epoch 34 [7680/8158] loss=0.0019638523537044723\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7950143298693604\n",
            "[0m 9s] Epoch 35 [2560/8158] loss=0.0019434162881225348\n",
            "[0m 9s] Epoch 35 [5120/8158] loss=0.001959198183612898\n",
            "[0m 9s] Epoch 35 [7680/8158] loss=0.0019651831205313402\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7950383340587232\n",
            "[0m 9s] Epoch 36 [2560/8158] loss=0.0019066583714447915\n",
            "[0m 9s] Epoch 36 [5120/8158] loss=0.001957976701669395\n",
            "[0m 9s] Epoch 36 [7680/8158] loss=0.001951229382151117\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7950863424374486\n",
            "[0m 10s] Epoch 37 [2560/8158] loss=0.0019322704756632447\n",
            "[0m 10s] Epoch 37 [5120/8158] loss=0.001940463186474517\n",
            "[0m 10s] Epoch 37 [7680/8158] loss=0.001940647535957396\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7952474231818565\n",
            "[0m 10s] Epoch 38 [2560/8158] loss=0.001980655430816114\n",
            "[0m 10s] Epoch 38 [5120/8158] loss=0.001983563747489825\n",
            "[0m 10s] Epoch 38 [7680/8158] loss=0.001948094367980957\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7950737086535734\n",
            "[0m 10s] Epoch 39 [2560/8158] loss=0.0018921923590824008\n",
            "[0m 10s] Epoch 39 [5120/8158] loss=0.0019293276884127409\n",
            "[0m 10s] Epoch 39 [7680/8158] loss=0.001942431414499879\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.795320699128332\n",
            "[0m 10s] Epoch 40 [2560/8158] loss=0.0019518725341185928\n",
            "[0m 10s] Epoch 40 [5120/8158] loss=0.0019750228500925006\n",
            "[0m 11s] Epoch 40 [7680/8158] loss=0.0019606624147854744\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.795416715885783\n",
            "[0m 11s] Epoch 41 [2560/8158] loss=0.001987510174512863\n",
            "[0m 11s] Epoch 41 [5120/8158] loss=0.0019638665486127137\n",
            "[0m 11s] Epoch 41 [7680/8158] loss=0.0019617878482677043\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dec set: AUC  0.7950288587208167\n",
            "[0m 11s] Epoch 42 [2560/8158] loss=0.0019347157911397516\n",
            "[0m 11s] Epoch 42 [5120/8158] loss=0.001941288565285504\n",
            "[0m 11s] Epoch 42 [7680/8158] loss=0.001955580373760313\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7955942205492283\n",
            "[0m 11s] Epoch 43 [2560/8158] loss=0.0018950806115753949\n",
            "[0m 11s] Epoch 43 [5120/8158] loss=0.0019259365799371153\n",
            "[0m 11s] Epoch 43 [7680/8158] loss=0.0019557545815284054\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7955184178459779\n",
            "[0m 11s] Epoch 44 [2560/8158] loss=0.001935884275007993\n",
            "[0m 12s] Epoch 44 [5120/8158] loss=0.001982951891841367\n",
            "[0m 12s] Epoch 44 [7680/8158] loss=0.001958022111405929\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7951539331811804\n",
            "[0m 12s] Epoch 45 [2560/8158] loss=0.001883110124617815\n",
            "[0m 12s] Epoch 45 [5120/8158] loss=0.0019218835106585175\n",
            "[0m 12s] Epoch 45 [7680/8158] loss=0.0019476966156313817\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7956908689958732\n",
            "[0m 12s] Epoch 46 [2560/8158] loss=0.0019296235754154623\n",
            "[0m 12s] Epoch 46 [5120/8158] loss=0.0019556074985302985\n",
            "[0m 12s] Epoch 46 [7680/8158] loss=0.0019563247992967566\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7956921323742607\n",
            "[0m 12s] Epoch 47 [2560/8158] loss=0.0019673384726047517\n",
            "[0m 12s] Epoch 47 [5120/8158] loss=0.0019557408755645155\n",
            "[0m 12s] Epoch 47 [7680/8158] loss=0.0019476745122422774\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dec set: AUC  0.7956776035228043\n",
            "[0m 13s] Epoch 48 [2560/8158] loss=0.0019372376729734242\n",
            "[0m 13s] Epoch 48 [5120/8158] loss=0.001942034560488537\n",
            "[0m 13s] Epoch 48 [7680/8158] loss=0.001941205320569376\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7957660400099302\n",
            "[0m 13s] Epoch 49 [2560/8158] loss=0.0019324550754390656\n",
            "[0m 13s] Epoch 49 [5120/8158] loss=0.001939172059064731\n",
            "[0m 13s] Epoch 49 [7680/8158] loss=0.0019432388517695168\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7956036958871349\n",
            "[0m 13s] Epoch 50 [2560/8158] loss=0.0019770574872381985\n",
            "[0m 13s] Epoch 50 [5120/8158] loss=0.001968053536256775\n",
            "[0m 13s] Epoch 50 [7680/8158] loss=0.0019491136306896805\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7958633201457685\n",
            "[0m 13s] Epoch 51 [2560/8158] loss=0.0019236072897911073\n",
            "[0m 14s] Epoch 51 [5120/8158] loss=0.0019245908362790943\n",
            "[0m 14s] Epoch 51 [7680/8158] loss=0.0019372623491411408\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7955146277108153\n",
            "[0m 14s] Epoch 52 [2560/8158] loss=0.0018864125129766762\n",
            "[0m 14s] Epoch 52 [5120/8158] loss=0.0019454024732112885\n",
            "[0m 14s] Epoch 52 [7680/8158] loss=0.001970356594150265\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7960951500798771\n",
            "[0m 14s] Epoch 53 [2560/8158] loss=0.002046208793763071\n",
            "[0m 14s] Epoch 53 [5120/8158] loss=0.001973566791275516\n",
            "[0m 14s] Epoch 53 [7680/8158] loss=0.0019509748322889208\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7957388773745986\n",
            "[0m 14s] Epoch 54 [2560/8158] loss=0.001911113690584898\n",
            "[0m 14s] Epoch 54 [5120/8158] loss=0.001932933006901294\n",
            "[0m 14s] Epoch 54 [7680/8158] loss=0.0019465362575526038\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.7956637063605417\n",
            "[0m 15s] Epoch 55 [2560/8158] loss=0.0019395904266275466\n",
            "[0m 15s] Epoch 55 [5120/8158] loss=0.001954005379229784\n",
            "[0m 15s] Epoch 55 [7680/8158] loss=0.0019513705473703643\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7960218741334013\n",
            "[0m 15s] Epoch 56 [2560/8158] loss=0.0019263975555077195\n",
            "[0m 15s] Epoch 56 [5120/8158] loss=0.0019320083200000226\n",
            "[0m 15s] Epoch 56 [7680/8158] loss=0.0019498995233637592\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7959621795045914\n",
            "[0m 15s] Epoch 57 [2560/8158] loss=0.0019410365377552808\n",
            "[0m 15s] Epoch 57 [5120/8158] loss=0.001952065184013918\n",
            "[0m 15s] Epoch 57 [7680/8158] loss=0.001950537401717156\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.7963086610273665\n",
            "[0m 15s] Epoch 58 [2560/8158] loss=0.0019690679269842803\n",
            "[0m 15s] Epoch 58 [5120/8158] loss=0.0019522232993040233\n",
            "[0m 16s] Epoch 58 [7680/8158] loss=0.0019449497495467464\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7962019055536219\n",
            "[0m 16s] Epoch 59 [2560/8158] loss=0.0018807679414749145\n",
            "[0m 16s] Epoch 59 [5120/8158] loss=0.0019249644596129657\n",
            "[0m 16s] Epoch 59 [7680/8158] loss=0.0019483356038108468\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7955651628463156\n",
            "[0m 16s] Epoch 60 [2560/8158] loss=0.0019370434805750848\n",
            "[0m 16s] Epoch 60 [5120/8158] loss=0.001938015053747222\n",
            "[0m 16s] Epoch 60 [7680/8158] loss=0.0019447073456831277\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7961324197423088\n",
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.0012758064665831625\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7499271978204196\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.0012060445849783718\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7536860643678654\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.0011522321030497551\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1858/2658 69.90%\n",
            "Dec set: AUC  0.7527290552393249\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.0010790970991365612\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1921/2658 72.27%\n",
            "Dec set: AUC  0.766851098854937\n",
            "[0m 0s] Epoch 5 [5120/8158] loss=0.0010164283798076213\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1898/2658 71.41%\n",
            "Dec set: AUC  0.7849831054725129\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.0010309892590157689\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1917/2658 72.12%\n",
            "Dec set: AUC  0.7872805590702041\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.0010016199434176088\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1929/2658 72.57%\n",
            "Dec set: AUC  0.7881655556306564\n",
            "[0m 1s] Epoch 8 [5120/8158] loss=0.001002903445623815\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1962/2658 73.81%\n",
            "Dec set: AUC  0.7884169679297712\n",
            "[0m 1s] Epoch 9 [5120/8158] loss=0.001008854666724801\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dec set: AUC  0.7885502543496539\n",
            "[0m 1s] Epoch 10 [5120/8158] loss=0.0009864446765277535\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1977/2658 74.38%\n",
            "Dec set: AUC  0.7889475868525264\n",
            "[0m 2s] Epoch 11 [5120/8158] loss=0.0010101850260980428\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7888180905678064\n",
            "[0m 2s] Epoch 12 [5120/8158] loss=0.0010096433165017516\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dec set: AUC  0.788894840804848\n",
            "[0m 2s] Epoch 13 [5120/8158] loss=0.0010027968906797468\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1975/2658 74.30%\n",
            "Dec set: AUC  0.7888897872912978\n",
            "[0m 2s] Epoch 14 [5120/8158] loss=0.0010082724271342157\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7889652741499517\n",
            "[0m 2s] Epoch 15 [5120/8158] loss=0.0010009906836785376\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7890840317183777\n",
            "[0m 2s] Epoch 16 [5120/8158] loss=0.0009956107824109495\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7892021575976103\n",
            "[0m 3s] Epoch 17 [5120/8158] loss=0.0009946169855538755\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7892950159090925\n",
            "[0m 3s] Epoch 18 [5120/8158] loss=0.0009812462492845952\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7894630452346314\n",
            "[0m 3s] Epoch 19 [5120/8158] loss=0.0009840763232205063\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dec set: AUC  0.7896891899659961\n",
            "[0m 3s] Epoch 20 [5120/8158] loss=0.0009981945564504712\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dec set: AUC  0.7897630976016656\n",
            "[0m 3s] Epoch 21 [5120/8158] loss=0.0009830385562963784\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7899879789546429\n",
            "[0m 4s] Epoch 22 [5120/8158] loss=0.0009915048140101135\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7902678172674766\n",
            "[0m 4s] Epoch 23 [5120/8158] loss=0.0009972922853194177\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1979/2658 74.45%\n",
            "Dec set: AUC  0.7904206860523657\n",
            "[0m 4s] Epoch 24 [5120/8158] loss=0.0009961219853721558\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dec set: AUC  0.7906379871350178\n",
            "[0m 4s] Epoch 25 [5120/8158] loss=0.0009908495820127427\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7908818191638078\n",
            "[0m 4s] Epoch 26 [5120/8158] loss=0.000988661526935175\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1979/2658 74.45%\n",
            "Dec set: AUC  0.7911382849764728\n",
            "[0m 5s] Epoch 27 [5120/8158] loss=0.0009828430193010718\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7913915923431689\n",
            "[0m 5s] Epoch 28 [5120/8158] loss=0.000981774175306782\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7916461630882526\n",
            "[0m 5s] Epoch 29 [5120/8158] loss=0.000973316578892991\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7918931535630112\n",
            "[0m 5s] Epoch 30 [5120/8158] loss=0.000995584751944989\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7921426707945449\n",
            "[0m 5s] Epoch 31 [5120/8158] loss=0.0009804838104173542\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7922645868089399\n",
            "[0m 6s] Epoch 32 [5120/8158] loss=0.0009884961356874556\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7925090505269234\n",
            "[0m 6s] Epoch 33 [5120/8158] loss=0.000984859006712213\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dec set: AUC  0.7925949602572743\n",
            "[0m 6s] Epoch 34 [5120/8158] loss=0.0009867564600426703\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7927914155965325\n",
            "[0m 6s] Epoch 35 [5120/8158] loss=0.000987638474907726\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7929272287731901\n",
            "[0m 6s] Epoch 36 [5120/8158] loss=0.0009642042161431164\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dec set: AUC  0.7931438981666483\n",
            "[0m 6s] Epoch 37 [5120/8158] loss=0.000984694465296343\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7932904500595999\n",
            "[0m 7s] Epoch 38 [5120/8158] loss=0.0009599429788067937\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dec set: AUC  0.7935146997233832\n",
            "[0m 7s] Epoch 39 [5120/8158] loss=0.000985110818874091\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7934237364794823\n",
            "[0m 7s] Epoch 40 [5120/8158] loss=0.0009882392128929495\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.7938589703339803\n",
            "[0m 7s] Epoch 41 [5120/8158] loss=0.00098339740652591\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dec set: AUC  0.7938558118880117\n",
            "[0m 7s] Epoch 42 [5120/8158] loss=0.000974868843331933\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7939865715511192\n",
            "[0m 8s] Epoch 43 [5120/8158] loss=0.0009800875792279839\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.79407248128147\n",
            "[0m 8s] Epoch 44 [5120/8158] loss=0.0009573548566550016\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7941508107414956\n",
            "[0m 8s] Epoch 45 [5120/8158] loss=0.0009735505736898631\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7943321055401037\n",
            "[0m 8s] Epoch 46 [5120/8158] loss=0.0009903114521875977\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2014/2658 75.77%\n",
            "Dec set: AUC  0.7941691297281147\n",
            "[0m 8s] Epoch 47 [5120/8158] loss=0.0009838802507147193\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dec set: AUC  0.7945986783798688\n",
            "[0m 9s] Epoch 48 [5120/8158] loss=0.0009711908642202616\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7947142775023261\n",
            "[0m 9s] Epoch 49 [5120/8158] loss=0.0009868217341136188\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7946207875016502\n",
            "[0m 9s] Epoch 50 [5120/8158] loss=0.000986969011137262\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7948943089225468\n",
            "[0m 9s] Epoch 51 [5120/8158] loss=0.0009738225489854813\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dec set: AUC  0.7949890623016101\n",
            "[0m 9s] Epoch 52 [5120/8158] loss=0.0009739046276081353\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dec set: AUC  0.7948658829088278\n",
            "[0m 9s] Epoch 53 [5120/8158] loss=0.0009889926412142813\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1974/2658 74.27%\n",
            "Dec set: AUC  0.7950598114913108\n",
            "[0m 10s] Epoch 54 [5120/8158] loss=0.0009712541999761015\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7950793938563174\n",
            "[0m 10s] Epoch 55 [5120/8158] loss=0.0009777308208867908\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dec set: AUC  0.7952284725060437\n",
            "[0m 10s] Epoch 56 [5120/8158] loss=0.000984524458181113\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7954445102103083\n",
            "[0m 10s] Epoch 57 [5120/8158] loss=0.0009764018177520484\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7954242961561081\n",
            "[0m 11s] Epoch 58 [5120/8158] loss=0.0009779157873708755\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7954887284538712\n",
            "[0m 11s] Epoch 59 [5120/8158] loss=0.000987836322747171\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.795479253115965\n",
            "[0m 11s] Epoch 60 [5120/8158] loss=0.000985266937641427\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dec set: AUC  0.7953661807502825\n",
            "Training for 60 epochs...\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.750454974141803\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.750428759040262\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7531068053771911\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7522736073306268\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1778/2658 66.89%\n",
            "Dec set: AUC  0.7515244239468319\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1907/2658 71.75%\n",
            "Dec set: AUC  0.752288767871277\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1915/2658 72.05%\n",
            "Dec set: AUC  0.7638120421437763\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1962/2658 73.81%\n",
            "Dec set: AUC  0.7711421635481224\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1954/2658 73.51%\n",
            "Dec set: AUC  0.7844458538132233\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1932/2658 72.69%\n",
            "Dec set: AUC  0.7863545027121577\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1976/2658 74.34%\n",
            "Dec set: AUC  0.7878775053583036\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1962/2658 73.81%\n",
            "Dec set: AUC  0.7879185651558978\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1976/2658 74.34%\n",
            "Dec set: AUC  0.7869375518379945\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dec set: AUC  0.7871472726503215\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1964/2658 73.89%\n",
            "Dec set: AUC  0.7885985785729761\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7886361640800047\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1977/2658 74.38%\n",
            "Dec set: AUC  0.7884018073891212\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7885325670522287\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1974/2658 74.27%\n",
            "Dec set: AUC  0.7885616247551414\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dec set: AUC  0.7886304788772609\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1972/2658 74.19%\n",
            "Dec set: AUC  0.7886105806676577\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7886772238775988\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7887258639455181\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7887650286755309\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7887782941485997\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7888263025273252\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7889267411091325\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1976/2658 74.34%\n",
            "Dec set: AUC  0.7889570621904327\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dec set: AUC  0.7890947704346717\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7891693097595348\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7892798553684421\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7894238805046185\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7895413746946572\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7896001217896766\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dec set: AUC  0.7897188793581027\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7898439538184663\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.790059991522731\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7900656767254748\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7903208791597525\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7904781697689977\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7906891539597121\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7908704487583201\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.7909601486238335\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7910795378814534\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dec set: AUC  0.791293048828943\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dec set: AUC  0.7913720099781625\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dec set: AUC  0.7914313887623756\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7916051032906587\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1972/2658 74.19%\n",
            "Dec set: AUC  0.7916240539664712\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dec set: AUC  0.7918615691033235\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.7919812742055402\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7920434955911253\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7921350905242198\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dec set: AUC  0.792248794579096\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7923612352555847\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7924850463375608\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7925690610003304\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7925918018113055\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dec set: AUC  0.7926177010682496\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dec set: AUC  0.7928261585021892\n",
            "Training for 60 epochs...\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7489752422054292\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.749035252678836\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7488249001773151\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7500579574835273\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7524612190211724\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7542368973448208\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7548695340723676\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7541048743033256\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dec set: AUC  0.7534112795685814\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1784/2658 67.12%\n",
            "Dec set: AUC  0.7537435480844973\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1844/2658 69.38%\n",
            "Dec set: AUC  0.7538509352474358\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1912/2658 71.93%\n",
            "Dec set: AUC  0.7548673231601895\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1899/2658 71.44%\n",
            "Dec set: AUC  0.759489392990903\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1912/2658 71.93%\n",
            "Dec set: AUC  0.7646913535014848\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1926/2658 72.46%\n",
            "Dec set: AUC  0.7688289177205875\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1940/2658 72.99%\n",
            "Dec set: AUC  0.7747137342496196\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1966/2658 73.97%\n",
            "Dec set: AUC  0.781294356425574\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1960/2658 73.74%\n",
            "Dec set: AUC  0.7858005112892334\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1958/2658 73.66%\n",
            "Dec set: AUC  0.7860058102772043\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1938/2658 72.91%\n",
            "Dec set: AUC  0.7845794560777029\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1976/2658 74.34%\n",
            "Dec set: AUC  0.7876804183298517\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1976/2658 74.34%\n",
            "Dec set: AUC  0.7877296900869646\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dec set: AUC  0.7871861215357374\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dec set: AUC  0.7865168468349528\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1969/2658 74.08%\n",
            "Dec set: AUC  0.7871940176506594\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7871346388664463\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7874947017068874\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1979/2658 74.45%\n",
            "Dec set: AUC  0.788091647994987\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dec set: AUC  0.7881826112388879\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dec set: AUC  0.7883417969157143\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7884068609026713\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7883828567133084\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7885079311736722\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7885256184710975\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7885287769170662\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dec set: AUC  0.7885584663091728\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dec set: AUC  0.7885793120525668\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dec set: AUC  0.7885647832011103\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dec set: AUC  0.7886626950261425\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dec set: AUC  0.7887612385403684\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.788765028675531\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7888124053650627\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dec set: AUC  0.7888755742844382\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dec set: AUC  0.7889242143523575\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dec set: AUC  0.7889880149609267\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dec set: AUC  0.7890284430693273\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7890972971914467\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7891547809080784\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dec set: AUC  0.7892110012463228\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dec set: AUC  0.7892558511790796\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7893240736120052\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dec set: AUC  0.7894523065183376\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7894883128023817\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dec set: AUC  0.7895609570596638\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dec set: AUC  0.7896500252359833\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dec set: AUC  0.7897675194260221\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7898092109128101\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dec set: AUC  0.7898951206431607\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dec set: AUC  0.7899778719275428\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dec set: AUC  0.7900637816578935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "KQMzAdKfjT7S",
        "outputId": "96fa1089-108e-4add-db1a-4879860f61e0"
      },
      "source": [
        "for i, BATCH_SIZE in enumerate(BATCH_SIZEs):\r\n",
        "\r\n",
        "  epoch = np.arange(1, len(acc_list) + 1, 1)\r\n",
        "  plt.plot(epoch, aucs[i], label = str(BATCH_SIZE))\r\n",
        "\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylabel('AUC')\r\n",
        "plt.legend()\r\n",
        "plt.grid()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5TzoNdXmliR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df84ac7-dcad-4b74-8b28-b04df4463ed9"
      },
      "source": [
        "#调参 learning rate\r\n",
        "N_EPOCHS  = 60\r\n",
        "HIDDEN_SIZE = 128\r\n",
        "N_LAYER = 3\r\n",
        "BATCH_SIZE = 256\r\n",
        "\r\n",
        "\r\n",
        "LRs = [0.00005, 0.0001, 0.0005, 0.001, 0.01, 0.1]\r\n",
        "accs = []\r\n",
        "aucs = []\r\n",
        "\r\n",
        "for LR in LRs:\r\n",
        "  trainset = DayFeatureDataset(is_train_set = True)\r\n",
        "  trainloader = DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True)\r\n",
        "  devset = DayFeatureDataset(is_train_set = False)\r\n",
        "  devloader = DataLoader(devset, batch_size = BATCH_SIZE, shuffle = False)\r\n",
        "  \r\n",
        "  if __name__ == '__main__':\r\n",
        "    classifier = RNNClassifier(11, HIDDEN_SIZE, N_CLASS, N_LAYER, bidirectional = False)\r\n",
        "    if USE_GPU:\r\n",
        "      device = torch.device(\"cuda:0\")\r\n",
        "      classifier.to(device)\r\n",
        "\r\n",
        "    criterion = torch.nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "    optimizer = torch.optim.Adam(classifier.parameters(), lr=LR)\r\n",
        "\r\n",
        "    start = time.time()\r\n",
        "    print(\"Training for %d epochs...\" % N_EPOCHS)\r\n",
        "    acc_list = []\r\n",
        "    auc_list = []\r\n",
        "    for epoch in range(1, N_EPOCHS + 1):\r\n",
        "      # Train cycle\r\n",
        "      trainModel()\r\n",
        "      acc, auc = devModel_auc()\r\n",
        "      acc_list.append(acc)\r\n",
        "      auc_list.append(auc)\r\n",
        "    accs.append(acc_list)\r\n",
        "    aucs.append(auc_list)\r\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.0026864829706028104\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.002666939632035792\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.0026475111721083523\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.7534390738931067\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.002588431490585208\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.0025673875119537116\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.002542268120062848\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.7522170711477857\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.0025100577622652054\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.0024886950268410146\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.0024770861258730293\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.7502897874176356\n",
            "[0m 0s] Epoch 4 [2560/8158] loss=0.002439174056053162\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.002453998092096299\n",
            "[0m 0s] Epoch 4 [7680/8158] loss=0.0024557991263767084\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.749936673158326\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.00242119908798486\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.002440392202697694\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.002440326373713712\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.7501214422474998\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.0024064484285190703\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.0024146018899045885\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.0024096178744609158\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.7506419541431546\n",
            "[0m 1s] Epoch 7 [2560/8158] loss=0.00237522954121232\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.002394411968998611\n",
            "[0m 1s] Epoch 7 [7680/8158] loss=0.0023834900309642154\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.7509925416456893\n",
            "[0m 1s] Epoch 8 [2560/8158] loss=0.002397639467380941\n",
            "[0m 1s] Epoch 8 [5120/8158] loss=0.002348889212589711\n",
            "[0m 2s] Epoch 8 [7680/8158] loss=0.002333364845253527\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.750843147151366\n",
            "[0m 2s] Epoch 9 [2560/8158] loss=0.0022735787788406014\n",
            "[0m 2s] Epoch 9 [5120/8158] loss=0.0022921003517694773\n",
            "[0m 2s] Epoch 9 [7680/8158] loss=0.002284667865994076\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1774/2658 66.74%\n",
            "Dev set: AUC  0.7508883129287196\n",
            "[0m 2s] Epoch 10 [2560/8158] loss=0.002281699958257377\n",
            "[0m 2s] Epoch 10 [5120/8158] loss=0.002223459165543318\n",
            "[0m 2s] Epoch 10 [7680/8158] loss=0.002221704282176991\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1780/2658 66.97%\n",
            "Dev set: AUC  0.7509173706316323\n",
            "[0m 2s] Epoch 11 [2560/8158] loss=0.002221593703143299\n",
            "[0m 2s] Epoch 11 [5120/8158] loss=0.0021957248449325562\n",
            "[0m 2s] Epoch 11 [7680/8158] loss=0.0021742809292239445\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1773/2658 66.70%\n",
            "Dev set: AUC  0.7517401458064996\n",
            "[0m 3s] Epoch 12 [2560/8158] loss=0.00214696500916034\n",
            "[0m 3s] Epoch 12 [5120/8158] loss=0.0021417398238554597\n",
            "[0m 3s] Epoch 12 [7680/8158] loss=0.0021452710342903933\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1908/2658 71.78%\n",
            "Dev set: AUC  0.7542008910607767\n",
            "[0m 3s] Epoch 13 [2560/8158] loss=0.0021723709767684342\n",
            "[0m 3s] Epoch 13 [5120/8158] loss=0.0021439319360069932\n",
            "[0m 3s] Epoch 13 [7680/8158] loss=0.0021296977841605744\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1909/2658 71.82%\n",
            "Dev set: AUC  0.7584578445374993\n",
            "[0m 3s] Epoch 14 [2560/8158] loss=0.002151997247710824\n",
            "[0m 3s] Epoch 14 [5120/8158] loss=0.0021261446876451374\n",
            "[0m 3s] Epoch 14 [7680/8158] loss=0.002120843374480804\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1915/2658 72.05%\n",
            "Dev set: AUC  0.7627135346358344\n",
            "[0m 3s] Epoch 15 [2560/8158] loss=0.002094639849383384\n",
            "[0m 3s] Epoch 15 [5120/8158] loss=0.0020820297009777277\n",
            "[0m 3s] Epoch 15 [7680/8158] loss=0.0020941752824001013\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1934/2658 72.76%\n",
            "Dev set: AUC  0.7662326751342498\n",
            "[0m 3s] Epoch 16 [2560/8158] loss=0.002096083969809115\n",
            "[0m 4s] Epoch 16 [5120/8158] loss=0.0021018079831264915\n",
            "[0m 4s] Epoch 16 [7680/8158] loss=0.0020808492166300616\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1953/2658 73.48%\n",
            "Dev set: AUC  0.7686428852530263\n",
            "[0m 4s] Epoch 17 [2560/8158] loss=0.0020422374946065247\n",
            "[0m 4s] Epoch 17 [5120/8158] loss=0.002082667272770777\n",
            "[0m 4s] Epoch 17 [7680/8158] loss=0.0020634158126388987\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1962/2658 73.81%\n",
            "Dev set: AUC  0.7710821530747156\n",
            "[0m 4s] Epoch 18 [2560/8158] loss=0.002065732656046748\n",
            "[0m 4s] Epoch 18 [5120/8158] loss=0.00204562260187231\n",
            "[0m 4s] Epoch 18 [7680/8158] loss=0.00204467373356844\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1953/2658 73.48%\n",
            "Dev set: AUC  0.7757654967572235\n",
            "[0m 4s] Epoch 19 [2560/8158] loss=0.0020168061833828687\n",
            "[0m 4s] Epoch 19 [5120/8158] loss=0.0020217707380652426\n",
            "[0m 5s] Epoch 19 [7680/8158] loss=0.0020303898917821547\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1964/2658 73.89%\n",
            "Dev set: AUC  0.7797242929344932\n",
            "[0m 5s] Epoch 20 [2560/8158] loss=0.002031135733705014\n",
            "[0m 5s] Epoch 20 [5120/8158] loss=0.0020269614760763943\n",
            "[0m 5s] Epoch 20 [7680/8158] loss=0.0020275965759841102\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1949/2658 73.33%\n",
            "Dev set: AUC  0.7835763336380182\n",
            "[0m 5s] Epoch 21 [2560/8158] loss=0.0019762133830226957\n",
            "[0m 5s] Epoch 21 [5120/8158] loss=0.0020126888470258565\n",
            "[0m 5s] Epoch 21 [7680/8158] loss=0.002016159143143644\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1955/2658 73.55%\n",
            "Dev set: AUC  0.7853027402045536\n",
            "[0m 5s] Epoch 22 [2560/8158] loss=0.0020095812971703706\n",
            "[0m 5s] Epoch 22 [5120/8158] loss=0.002037339727394283\n",
            "[0m 5s] Epoch 22 [7680/8158] loss=0.0020193528228749833\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1969/2658 74.08%\n",
            "Dev set: AUC  0.7857632416268018\n",
            "[0m 5s] Epoch 23 [2560/8158] loss=0.002024487650487572\n",
            "[0m 6s] Epoch 23 [5120/8158] loss=0.0020336866786237807\n",
            "[0m 6s] Epoch 23 [7680/8158] loss=0.0020202387667571505\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1966/2658 73.97%\n",
            "Dev set: AUC  0.7870853671093333\n",
            "[0m 6s] Epoch 24 [2560/8158] loss=0.002048163895960897\n",
            "[0m 6s] Epoch 24 [5120/8158] loss=0.0020374226907733826\n",
            "[0m 6s] Epoch 24 [7680/8158] loss=0.0020164889089452722\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1975/2658 74.30%\n",
            "Dev set: AUC  0.7872420260293849\n",
            "[0m 6s] Epoch 25 [2560/8158] loss=0.00201321947388351\n",
            "[0m 6s] Epoch 25 [5120/8158] loss=0.00202562062186189\n",
            "[0m 6s] Epoch 25 [7680/8158] loss=0.0020129570427040258\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1974/2658 74.27%\n",
            "Dev set: AUC  0.7879599407980888\n",
            "[0m 6s] Epoch 26 [2560/8158] loss=0.001995775697287172\n",
            "[0m 6s] Epoch 26 [5120/8158] loss=0.0020149678515736015\n",
            "[0m 6s] Epoch 26 [7680/8158] loss=0.0020184642441260317\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1975/2658 74.30%\n",
            "Dev set: AUC  0.7877739083305275\n",
            "[0m 6s] Epoch 27 [2560/8158] loss=0.0020316164474934338\n",
            "[0m 7s] Epoch 27 [5120/8158] loss=0.0020124133792705835\n",
            "[0m 7s] Epoch 27 [7680/8158] loss=0.0020173902856186034\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dev set: AUC  0.7884548692813967\n",
            "[0m 7s] Epoch 28 [2560/8158] loss=0.002066650171764195\n",
            "[0m 7s] Epoch 28 [5120/8158] loss=0.0020383913477417083\n",
            "[0m 7s] Epoch 28 [7680/8158] loss=0.0020108058233745396\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dev set: AUC  0.7884119144162213\n",
            "[0m 7s] Epoch 29 [2560/8158] loss=0.002009551180526614\n",
            "[0m 7s] Epoch 29 [5120/8158] loss=0.002007780596613884\n",
            "[0m 7s] Epoch 29 [7680/8158] loss=0.0020068631158210335\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1979/2658 74.45%\n",
            "Dev set: AUC  0.7885303561400506\n",
            "[0m 7s] Epoch 30 [2560/8158] loss=0.002004794660024345\n",
            "[0m 7s] Epoch 30 [5120/8158] loss=0.0019927816931158302\n",
            "[0m 7s] Epoch 30 [7680/8158] loss=0.0019981084081033868\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dev set: AUC  0.7889008418521886\n",
            "[0m 7s] Epoch 31 [2560/8158] loss=0.001985120272729546\n",
            "[0m 8s] Epoch 31 [5120/8158] loss=0.0019846828887239098\n",
            "[0m 8s] Epoch 31 [7680/8158] loss=0.001997657283209264\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dev set: AUC  0.7889172657712262\n",
            "[0m 8s] Epoch 32 [2560/8158] loss=0.0020702061825431882\n",
            "[0m 8s] Epoch 32 [5120/8158] loss=0.002016106195515022\n",
            "[0m 8s] Epoch 32 [7680/8158] loss=0.0019974596604394415\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dev set: AUC  0.7891554125972723\n",
            "[0m 8s] Epoch 33 [2560/8158] loss=0.002001941401977092\n",
            "[0m 8s] Epoch 33 [5120/8158] loss=0.002026715304236859\n",
            "[0m 8s] Epoch 33 [7680/8158] loss=0.001997990576395144\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7892252142531824\n",
            "[0m 8s] Epoch 34 [2560/8158] loss=0.0020109609118662774\n",
            "[0m 9s] Epoch 34 [5120/8158] loss=0.0019993465510196986\n",
            "[0m 9s] Epoch 34 [7680/8158] loss=0.0019978515920229256\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dev set: AUC  0.789275433544086\n",
            "[0m 9s] Epoch 35 [2560/8158] loss=0.002010079484898597\n",
            "[0m 9s] Epoch 35 [5120/8158] loss=0.002025822253199294\n",
            "[0m 9s] Epoch 35 [7680/8158] loss=0.0020044817007146775\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dev set: AUC  0.7892744860102954\n",
            "[0m 9s] Epoch 36 [2560/8158] loss=0.0020039840135723354\n",
            "[0m 9s] Epoch 36 [5120/8158] loss=0.001993699767626822\n",
            "[0m 9s] Epoch 36 [7680/8158] loss=0.0019962636831526953\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dev set: AUC  0.7894510431399502\n",
            "[0m 9s] Epoch 37 [2560/8158] loss=0.001992110745050013\n",
            "[0m 9s] Epoch 37 [5120/8158] loss=0.00200043338118121\n",
            "[0m 9s] Epoch 37 [7680/8158] loss=0.001994555835456898\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dev set: AUC  0.7895192655728759\n",
            "[0m 10s] Epoch 38 [2560/8158] loss=0.001983464404474944\n",
            "[0m 10s] Epoch 38 [5120/8158] loss=0.002013494318816811\n",
            "[0m 10s] Epoch 38 [7680/8158] loss=0.0020022961931924024\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7894472530047876\n",
            "[0m 10s] Epoch 39 [2560/8158] loss=0.0020313226152211426\n",
            "[0m 10s] Epoch 39 [5120/8158] loss=0.0020091265672817827\n",
            "[0m 10s] Epoch 39 [7680/8158] loss=0.0019990482251159847\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dev set: AUC  0.7895502183433698\n",
            "[0m 10s] Epoch 40 [2560/8158] loss=0.0019650312256999313\n",
            "[0m 10s] Epoch 40 [5120/8158] loss=0.001990026992280036\n",
            "[0m 10s] Epoch 40 [7680/8158] loss=0.0019961902561287086\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dev set: AUC  0.7895274775323947\n",
            "[0m 10s] Epoch 41 [2560/8158] loss=0.002041056309826672\n",
            "[0m 10s] Epoch 41 [5120/8158] loss=0.0019697842944879085\n",
            "[0m 10s] Epoch 41 [7680/8158] loss=0.00200296330343311\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dev set: AUC  0.7895505341879667\n",
            "[0m 11s] Epoch 42 [2560/8158] loss=0.0019862514454871416\n",
            "[0m 11s] Epoch 42 [5120/8158] loss=0.00199165900121443\n",
            "[0m 11s] Epoch 42 [7680/8158] loss=0.001992826540178309\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.7895596936812761\n",
            "[0m 11s] Epoch 43 [2560/8158] loss=0.001968475547619164\n",
            "[0m 11s] Epoch 43 [5120/8158] loss=0.0019811766629572957\n",
            "[0m 11s] Epoch 43 [7680/8158] loss=0.001994157889081786\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dev set: AUC  0.7895944365869328\n",
            "[0m 11s] Epoch 44 [2560/8158] loss=0.002015832590404898\n",
            "[0m 11s] Epoch 44 [5120/8158] loss=0.0019993745139800013\n",
            "[0m 11s] Epoch 44 [7680/8158] loss=0.0019896025691802303\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dev set: AUC  0.7894535698967251\n",
            "[0m 11s] Epoch 45 [2560/8158] loss=0.001994360564276576\n",
            "[0m 11s] Epoch 45 [5120/8158] loss=0.001996479759691283\n",
            "[0m 11s] Epoch 45 [7680/8158] loss=0.0019956368138082325\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dev set: AUC  0.7896089654383892\n",
            "[0m 12s] Epoch 46 [2560/8158] loss=0.0019868108443915843\n",
            "[0m 12s] Epoch 46 [5120/8158] loss=0.0020038424176163972\n",
            "[0m 12s] Epoch 46 [7680/8158] loss=0.0019988977699540557\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dev set: AUC  0.7895401113162699\n",
            "[0m 12s] Epoch 47 [2560/8158] loss=0.002070030686445534\n",
            "[0m 12s] Epoch 47 [5120/8158] loss=0.002008249278878793\n",
            "[0m 12s] Epoch 47 [7680/8158] loss=0.002000995387788862\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7897037188174527\n",
            "[0m 12s] Epoch 48 [2560/8158] loss=0.0019909753929823635\n",
            "[0m 12s] Epoch 48 [5120/8158] loss=0.0020054062828421594\n",
            "[0m 12s] Epoch 48 [7680/8158] loss=0.0019907664546432594\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dev set: AUC  0.7897384617231091\n",
            "[0m 13s] Epoch 49 [2560/8158] loss=0.00198517725802958\n",
            "[0m 13s] Epoch 49 [5120/8158] loss=0.00198956792592071\n",
            "[0m 13s] Epoch 49 [7680/8158] loss=0.0019942634156905116\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7895698007083762\n",
            "[0m 13s] Epoch 50 [2560/8158] loss=0.002001790201757103\n",
            "[0m 13s] Epoch 50 [5120/8158] loss=0.0019851839519105853\n",
            "[0m 13s] Epoch 50 [7680/8158] loss=0.0019866696403672297\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dev set: AUC  0.7897090881755995\n",
            "[0m 13s] Epoch 51 [2560/8158] loss=0.0019509955192916096\n",
            "[0m 13s] Epoch 51 [5120/8158] loss=0.0019699664670042695\n",
            "[0m 13s] Epoch 51 [7680/8158] loss=0.001985566585790366\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dev set: AUC  0.7897368825001247\n",
            "[0m 13s] Epoch 52 [2560/8158] loss=0.0019524937495589256\n",
            "[0m 13s] Epoch 52 [5120/8158] loss=0.001987493160413578\n",
            "[0m 13s] Epoch 52 [7680/8158] loss=0.001992588338907808\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7897353032771404\n",
            "[0m 14s] Epoch 53 [2560/8158] loss=0.0019941765116527677\n",
            "[0m 14s] Epoch 53 [5120/8158] loss=0.0020048305159434676\n",
            "[0m 14s] Epoch 53 [7680/8158] loss=0.0019966429099440575\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7898180545615223\n",
            "[0m 14s] Epoch 54 [2560/8158] loss=0.001996386470273137\n",
            "[0m 14s] Epoch 54 [5120/8158] loss=0.001996562484418973\n",
            "[0m 14s] Epoch 54 [7680/8158] loss=0.0019981507949220637\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7897706778719907\n",
            "[0m 14s] Epoch 55 [2560/8158] loss=0.0020194931072182952\n",
            "[0m 14s] Epoch 55 [5120/8158] loss=0.0019848519470542668\n",
            "[0m 14s] Epoch 55 [7680/8158] loss=0.0019881137219878533\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7897510955069843\n",
            "[0m 14s] Epoch 56 [2560/8158] loss=0.002013923181220889\n",
            "[0m 14s] Epoch 56 [5120/8158] loss=0.0020114368293434382\n",
            "[0m 14s] Epoch 56 [7680/8158] loss=0.001992897402184705\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7898066841560348\n",
            "[0m 15s] Epoch 57 [2560/8158] loss=0.002013140916824341\n",
            "[0m 15s] Epoch 57 [5120/8158] loss=0.0019987786770798268\n",
            "[0m 15s] Epoch 57 [7680/8158] loss=0.0019822424316468337\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7897612025340842\n",
            "[0m 15s] Epoch 58 [2560/8158] loss=0.0019501571077853441\n",
            "[0m 15s] Epoch 58 [5120/8158] loss=0.0019877811020705847\n",
            "[0m 15s] Epoch 58 [7680/8158] loss=0.001990105751125763\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7897927869937722\n",
            "[0m 15s] Epoch 59 [2560/8158] loss=0.0019848408992402256\n",
            "[0m 15s] Epoch 59 [5120/8158] loss=0.0019811113888863474\n",
            "[0m 15s] Epoch 59 [7680/8158] loss=0.0019879747182130814\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dev set: AUC  0.7898256348318474\n",
            "[0m 15s] Epoch 60 [2560/8158] loss=0.0019129330292344093\n",
            "[0m 15s] Epoch 60 [5120/8158] loss=0.0019568041316233575\n",
            "[0m 15s] Epoch 60 [7680/8158] loss=0.0019968393840827047\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dev set: AUC  0.7898509023995979\n",
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.002630117884837091\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.0026001038146205246\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.0025695119751617313\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.7480930882463487\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.0024658009875565766\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.0024590295855887235\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.002460714903039237\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.7489793481851885\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.0024063905933871866\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.0024274372146464883\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.0024230150350679955\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.7514574648922938\n",
            "[0m 0s] Epoch 4 [2560/8158] loss=0.002375850989483297\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.00237969730515033\n",
            "[0m 0s] Epoch 4 [7680/8158] loss=0.002359892001065115\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.7520304069910305\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.0022901300340890884\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.0022905662772245705\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.0022789503059660395\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1784/2658 67.12%\n",
            "Dev set: AUC  0.752049989356037\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.0021543387323617936\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.002172345214057714\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.0021713996694112817\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1899/2658 71.44%\n",
            "Dev set: AUC  0.7546127524151056\n",
            "[0m 1s] Epoch 7 [2560/8158] loss=0.002117821655701846\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.00213294139248319\n",
            "[0m 1s] Epoch 7 [7680/8158] loss=0.002126906331007679\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1920/2658 72.23%\n",
            "Dev set: AUC  0.7623585253089434\n",
            "[0m 1s] Epoch 8 [2560/8158] loss=0.002109924959950149\n",
            "[0m 1s] Epoch 8 [5120/8158] loss=0.0021001170098315924\n",
            "[0m 1s] Epoch 8 [7680/8158] loss=0.0020896156473706167\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1953/2658 73.48%\n",
            "Dev set: AUC  0.7681890165673126\n",
            "[0m 2s] Epoch 9 [2560/8158] loss=0.002032851125113666\n",
            "[0m 2s] Epoch 9 [5120/8158] loss=0.0020405557297635823\n",
            "[0m 2s] Epoch 9 [7680/8158] loss=0.002055642509367317\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1936/2658 72.84%\n",
            "Dev set: AUC  0.7757743404059361\n",
            "[0m 2s] Epoch 10 [2560/8158] loss=0.0020391806843690573\n",
            "[0m 2s] Epoch 10 [5120/8158] loss=0.0020382922783028336\n",
            "[0m 2s] Epoch 10 [7680/8158] loss=0.0020252350446147223\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1960/2658 73.74%\n",
            "Dev set: AUC  0.782843574173261\n",
            "[0m 2s] Epoch 11 [2560/8158] loss=0.002023192960768938\n",
            "[0m 2s] Epoch 11 [5120/8158] loss=0.0020116612780839206\n",
            "[0m 2s] Epoch 11 [7680/8158] loss=0.002032972480325649\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1945/2658 73.18%\n",
            "Dev set: AUC  0.7875869283291758\n",
            "[0m 2s] Epoch 12 [2560/8158] loss=0.001971202541608363\n",
            "[0m 3s] Epoch 12 [5120/8158] loss=0.0019786036282312125\n",
            "[0m 3s] Epoch 12 [7680/8158] loss=0.0020080560197432836\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1956/2658 73.59%\n",
            "Dev set: AUC  0.7880341642783552\n",
            "[0m 3s] Epoch 13 [2560/8158] loss=0.0020588232669979333\n",
            "[0m 3s] Epoch 13 [5120/8158] loss=0.002016320131951943\n",
            "[0m 3s] Epoch 13 [7680/8158] loss=0.0020032454941732187\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1979/2658 74.45%\n",
            "Dev set: AUC  0.7878111779929592\n",
            "[0m 3s] Epoch 14 [2560/8158] loss=0.0020222417660988867\n",
            "[0m 3s] Epoch 14 [5120/8158] loss=0.0020085983094759287\n",
            "[0m 3s] Epoch 14 [7680/8158] loss=0.002003581162231664\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dev set: AUC  0.7882299879284196\n",
            "[0m 3s] Epoch 15 [2560/8158] loss=0.002046448620967567\n",
            "[0m 3s] Epoch 15 [5120/8158] loss=0.0020286351267714054\n",
            "[0m 3s] Epoch 15 [7680/8158] loss=0.0020157435908913614\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dev set: AUC  0.788552149417235\n",
            "[0m 4s] Epoch 16 [2560/8158] loss=0.0020332076703198252\n",
            "[0m 4s] Epoch 16 [5120/8158] loss=0.002006641187472269\n",
            "[0m 4s] Epoch 16 [7680/8158] loss=0.0020020985316174726\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1954/2658 73.51%\n",
            "Dev set: AUC  0.7890151775962583\n",
            "[0m 4s] Epoch 17 [2560/8158] loss=0.0019914135336875916\n",
            "[0m 4s] Epoch 17 [5120/8158] loss=0.0020243862876668573\n",
            "[0m 4s] Epoch 17 [7680/8158] loss=0.0020114704268053172\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dev set: AUC  0.7886544830666237\n",
            "[0m 4s] Epoch 18 [2560/8158] loss=0.0019841518835164605\n",
            "[0m 4s] Epoch 18 [5120/8158] loss=0.0019800351990852507\n",
            "[0m 4s] Epoch 18 [7680/8158] loss=0.0020057854048597316\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dev set: AUC  0.7890556057046588\n",
            "[0m 4s] Epoch 19 [2560/8158] loss=0.002033682190813124\n",
            "[0m 4s] Epoch 19 [5120/8158] loss=0.0020211736555211246\n",
            "[0m 4s] Epoch 19 [7680/8158] loss=0.002000248762002836\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.788948850230914\n",
            "[0m 5s] Epoch 20 [2560/8158] loss=0.001994714781176299\n",
            "[0m 5s] Epoch 20 [5120/8158] loss=0.001974009064724669\n",
            "[0m 5s] Epoch 20 [7680/8158] loss=0.0020032233519790073\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1965/2658 73.93%\n",
            "Dev set: AUC  0.7892558511790795\n",
            "[0m 5s] Epoch 21 [2560/8158] loss=0.0020058729918673635\n",
            "[0m 5s] Epoch 21 [5120/8158] loss=0.001996929501183331\n",
            "[0m 5s] Epoch 21 [7680/8158] loss=0.0019943410336660844\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dev set: AUC  0.7891775217190536\n",
            "[0m 5s] Epoch 22 [2560/8158] loss=0.001959421846549958\n",
            "[0m 5s] Epoch 22 [5120/8158] loss=0.001982024312019348\n",
            "[0m 5s] Epoch 22 [7680/8158] loss=0.001997592331220706\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dev set: AUC  0.7892255300977792\n",
            "[0m 5s] Epoch 23 [2560/8158] loss=0.001986360899172723\n",
            "[0m 5s] Epoch 23 [5120/8158] loss=0.0019876100530382248\n",
            "[0m 6s] Epoch 23 [7680/8158] loss=0.0019844681839458643\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dev set: AUC  0.7893145982740986\n",
            "[0m 6s] Epoch 24 [2560/8158] loss=0.0019887187285348774\n",
            "[0m 6s] Epoch 24 [5120/8158] loss=0.001982469583163038\n",
            "[0m 6s] Epoch 24 [7680/8158] loss=0.0019979111268185077\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dev set: AUC  0.789332285571524\n",
            "[0m 6s] Epoch 25 [2560/8158] loss=0.0019546481082215903\n",
            "[0m 6s] Epoch 25 [5120/8158] loss=0.002008025423856452\n",
            "[0m 6s] Epoch 25 [7680/8158] loss=0.0020020031680663426\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dev set: AUC  0.7891781534082474\n",
            "[0m 6s] Epoch 26 [2560/8158] loss=0.0019942230195738376\n",
            "[0m 6s] Epoch 26 [5120/8158] loss=0.0019907078822143376\n",
            "[0m 6s] Epoch 26 [7680/8158] loss=0.00199703600568076\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7894093516531622\n",
            "[0m 6s] Epoch 27 [2560/8158] loss=0.0020309150917455555\n",
            "[0m 6s] Epoch 27 [5120/8158] loss=0.0020035379042383284\n",
            "[0m 7s] Epoch 27 [7680/8158] loss=0.0019900512920382122\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dev set: AUC  0.7894504114507563\n",
            "[0m 7s] Epoch 28 [2560/8158] loss=0.00204531941562891\n",
            "[0m 7s] Epoch 28 [5120/8158] loss=0.001994730450678617\n",
            "[0m 7s] Epoch 28 [7680/8158] loss=0.0019972858135588467\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1964/2658 73.89%\n",
            "Dev set: AUC  0.7894396727344625\n",
            "[0m 7s] Epoch 29 [2560/8158] loss=0.002013269963208586\n",
            "[0m 7s] Epoch 29 [5120/8158] loss=0.0019980999059043825\n",
            "[0m 7s] Epoch 29 [7680/8158] loss=0.0019893326214514674\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dev set: AUC  0.7895148437485195\n",
            "[0m 7s] Epoch 30 [2560/8158] loss=0.002003582345787436\n",
            "[0m 7s] Epoch 30 [5120/8158] loss=0.001974907540716231\n",
            "[0m 7s] Epoch 30 [7680/8158] loss=0.001982043986208737\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7895281092215884\n",
            "[0m 7s] Epoch 31 [2560/8158] loss=0.0019652985502034426\n",
            "[0m 7s] Epoch 31 [5120/8158] loss=0.001982677774503827\n",
            "[0m 8s] Epoch 31 [7680/8158] loss=0.001989659449706475\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dev set: AUC  0.7895571669245011\n",
            "[0m 8s] Epoch 32 [2560/8158] loss=0.001985464687459171\n",
            "[0m 8s] Epoch 32 [5120/8158] loss=0.0019858819898217915\n",
            "[0m 8s] Epoch 32 [7680/8158] loss=0.0019906773154313366\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7895622204380512\n",
            "[0m 8s] Epoch 33 [2560/8158] loss=0.0019348364905454219\n",
            "[0m 8s] Epoch 33 [5120/8158] loss=0.001976376079255715\n",
            "[0m 8s] Epoch 33 [7680/8158] loss=0.0019779747701250015\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7895660105732137\n",
            "[0m 8s] Epoch 34 [2560/8158] loss=0.0019696983858011664\n",
            "[0m 8s] Epoch 34 [5120/8158] loss=0.001987817935878411\n",
            "[0m 8s] Epoch 34 [7680/8158] loss=0.0019849458243697883\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7896329696277518\n",
            "[0m 9s] Epoch 35 [2560/8158] loss=0.0020150097319856287\n",
            "[0m 9s] Epoch 35 [5120/8158] loss=0.00199645240791142\n",
            "[0m 9s] Epoch 35 [7680/8158] loss=0.0019951326850180823\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dev set: AUC  0.7896595005738896\n",
            "[0m 9s] Epoch 36 [2560/8158] loss=0.001909535436425358\n",
            "[0m 9s] Epoch 36 [5120/8158] loss=0.001966213347623125\n",
            "[0m 9s] Epoch 36 [7680/8158] loss=0.001984953375843664\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dev set: AUC  0.78969992868229\n",
            "[0m 9s] Epoch 37 [2560/8158] loss=0.0019610516494140027\n",
            "[0m 9s] Epoch 37 [5120/8158] loss=0.0019805111922323706\n",
            "[0m 9s] Epoch 37 [7680/8158] loss=0.0019877207038613656\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.7897056138850338\n",
            "[0m 9s] Epoch 38 [2560/8158] loss=0.0019395240349695086\n",
            "[0m 9s] Epoch 38 [5120/8158] loss=0.0019611196126788854\n",
            "[0m 10s] Epoch 38 [7680/8158] loss=0.0019800465980855126\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7897766789193313\n",
            "[0m 10s] Epoch 39 [2560/8158] loss=0.002001325774472207\n",
            "[0m 10s] Epoch 39 [5120/8158] loss=0.0019885492860339583\n",
            "[0m 10s] Epoch 39 [7680/8158] loss=0.0019887741616306204\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7898136327371662\n",
            "[0m 10s] Epoch 40 [2560/8158] loss=0.0019958976190537214\n",
            "[0m 10s] Epoch 40 [5120/8158] loss=0.001989729783963412\n",
            "[0m 10s] Epoch 40 [7680/8158] loss=0.001987779850605875\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7898389003049163\n",
            "[0m 10s] Epoch 41 [2560/8158] loss=0.001958375936374068\n",
            "[0m 10s] Epoch 41 [5120/8158] loss=0.0019962578604463487\n",
            "[0m 10s] Epoch 41 [7680/8158] loss=0.0019908583645398418\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7898344784805601\n",
            "[0m 10s] Epoch 42 [2560/8158] loss=0.0020254936767742037\n",
            "[0m 11s] Epoch 42 [5120/8158] loss=0.0019888674549292775\n",
            "[0m 11s] Epoch 42 [7680/8158] loss=0.001980437815655023\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7898853294606574\n",
            "[0m 11s] Epoch 43 [2560/8158] loss=0.001972329546697438\n",
            "[0m 11s] Epoch 43 [5120/8158] loss=0.001999579189578071\n",
            "[0m 11s] Epoch 43 [7680/8158] loss=0.001984552569532146\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dev set: AUC  0.7899570261841488\n",
            "[0m 11s] Epoch 44 [2560/8158] loss=0.001960891834460199\n",
            "[0m 11s] Epoch 44 [5120/8158] loss=0.002002754737623036\n",
            "[0m 11s] Epoch 44 [7680/8158] loss=0.0019881555112078787\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dev set: AUC  0.7899942958465803\n",
            "[0m 11s] Epoch 45 [2560/8158] loss=0.0018866625265218317\n",
            "[0m 11s] Epoch 45 [5120/8158] loss=0.001963104720925912\n",
            "[0m 11s] Epoch 45 [7680/8158] loss=0.001969910168554634\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7900713619282187\n",
            "[0m 11s] Epoch 46 [2560/8158] loss=0.0019471397274173797\n",
            "[0m 12s] Epoch 46 [5120/8158] loss=0.0019787819590419533\n",
            "[0m 12s] Epoch 46 [7680/8158] loss=0.001986057493680467\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7900915759824187\n",
            "[0m 12s] Epoch 47 [2560/8158] loss=0.0019039766513742507\n",
            "[0m 12s] Epoch 47 [5120/8158] loss=0.0019474639208056032\n",
            "[0m 12s] Epoch 47 [7680/8158] loss=0.0019767455601443847\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7901572716585694\n",
            "[0m 12s] Epoch 48 [2560/8158] loss=0.0019583491259254513\n",
            "[0m 12s] Epoch 48 [5120/8158] loss=0.001982309465529397\n",
            "[0m 12s] Epoch 48 [7680/8158] loss=0.0019819331044952076\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7902494982808579\n",
            "[0m 12s] Epoch 49 [2560/8158] loss=0.0020014239591546355\n",
            "[0m 12s] Epoch 49 [5120/8158] loss=0.0019881560641806574\n",
            "[0m 12s] Epoch 49 [7680/8158] loss=0.0019763974201244612\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.790280451051352\n",
            "[0m 13s] Epoch 50 [2560/8158] loss=0.0018963200389407575\n",
            "[0m 13s] Epoch 50 [5120/8158] loss=0.001967371441423893\n",
            "[0m 13s] Epoch 50 [7680/8158] loss=0.001975715214697023\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7904175276063969\n",
            "[0m 13s] Epoch 51 [2560/8158] loss=0.0019028562470339238\n",
            "[0m 13s] Epoch 51 [5120/8158] loss=0.001931169000454247\n",
            "[0m 13s] Epoch 51 [7680/8158] loss=0.001970328491491576\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7904036304441344\n",
            "[0m 13s] Epoch 52 [2560/8158] loss=0.0019966084510087967\n",
            "[0m 13s] Epoch 52 [5120/8158] loss=0.002001305180601776\n",
            "[0m 13s] Epoch 52 [7680/8158] loss=0.001969136049350103\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7905463922019232\n",
            "[0m 13s] Epoch 53 [2560/8158] loss=0.0019616193952970208\n",
            "[0m 13s] Epoch 53 [5120/8158] loss=0.001976406475296244\n",
            "[0m 14s] Epoch 53 [7680/8158] loss=0.0019748550568086404\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7906278801079177\n",
            "[0m 14s] Epoch 54 [2560/8158] loss=0.001957168336957693\n",
            "[0m 14s] Epoch 54 [5120/8158] loss=0.001954798906808719\n",
            "[0m 14s] Epoch 54 [7680/8158] loss=0.0019744857718857626\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dev set: AUC  0.7906768360204338\n",
            "[0m 14s] Epoch 55 [2560/8158] loss=0.0019554371712729333\n",
            "[0m 14s] Epoch 55 [5120/8158] loss=0.0019597728503867986\n",
            "[0m 14s] Epoch 55 [7680/8158] loss=0.0019680012444344658\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1977/2658 74.38%\n",
            "Dev set: AUC  0.7907220017977875\n",
            "[0m 14s] Epoch 56 [2560/8158] loss=0.001969736651517451\n",
            "[0m 14s] Epoch 56 [5120/8158] loss=0.0019796990149188788\n",
            "[0m 14s] Epoch 56 [7680/8158] loss=0.0019810198301759858\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7907940143658756\n",
            "[0m 15s] Epoch 57 [2560/8158] loss=0.002070924220606685\n",
            "[0m 15s] Epoch 57 [5120/8158] loss=0.0019868128059897573\n",
            "[0m 15s] Epoch 57 [7680/8158] loss=0.00198412499933814\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7908508663933138\n",
            "[0m 15s] Epoch 58 [2560/8158] loss=0.0019651658716611565\n",
            "[0m 15s] Epoch 58 [5120/8158] loss=0.001980736991390586\n",
            "[0m 15s] Epoch 58 [7680/8158] loss=0.0019722665776498614\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7908998223058298\n",
            "[0m 15s] Epoch 59 [2560/8158] loss=0.0019033347489312291\n",
            "[0m 15s] Epoch 59 [5120/8158] loss=0.0019449114392045886\n",
            "[0m 15s] Epoch 59 [7680/8158] loss=0.0019686246174387633\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.791021106631031\n",
            "[0m 15s] Epoch 60 [2560/8158] loss=0.001976200845092535\n",
            "[0m 15s] Epoch 60 [5120/8158] loss=0.001970536104636267\n",
            "[0m 16s] Epoch 60 [7680/8158] loss=0.0019825596711598335\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7910334245703092\n",
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.0025725580751895906\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.002516192477196455\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.0024839965626597404\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.7526491465563147\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.002325217262841761\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.002258209662977606\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.0022324549577509363\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1927/2658 72.50%\n",
            "Dev set: AUC  0.7627773352444037\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.0020875997724942863\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.002064641221659258\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.0020533678005449476\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1972/2658 74.19%\n",
            "Dev set: AUC  0.7879479387034074\n",
            "[0m 0s] Epoch 4 [2560/8158] loss=0.002037609729450196\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.002022342063719407\n",
            "[0m 1s] Epoch 4 [7680/8158] loss=0.0020083716333222884\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1973/2658 74.23%\n",
            "Dev set: AUC  0.7871624331909717\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.0020760394749231637\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.0020132441190071405\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.00201918197174867\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1969/2658 74.08%\n",
            "Dev set: AUC  0.7891648879351786\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.0019952990929596125\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.0020098467997740955\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.002003572741523385\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dev set: AUC  0.7897157209121339\n",
            "[0m 1s] Epoch 7 [2560/8158] loss=0.001974371331743896\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.0019728921353816984\n",
            "[0m 1s] Epoch 7 [7680/8158] loss=0.001985562763487299\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.789622230911458\n",
            "[0m 1s] Epoch 8 [2560/8158] loss=0.00202831068309024\n",
            "[0m 2s] Epoch 8 [5120/8158] loss=0.0020327437960077077\n",
            "[0m 2s] Epoch 8 [7680/8158] loss=0.002003576773374031\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1955/2658 73.55%\n",
            "Dev set: AUC  0.7896253893574269\n",
            "[0m 2s] Epoch 9 [2560/8158] loss=0.0020239146309904755\n",
            "[0m 2s] Epoch 9 [5120/8158] loss=0.0020028501108754425\n",
            "[0m 2s] Epoch 9 [7680/8158] loss=0.001999436439170192\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dev set: AUC  0.7897163526013276\n",
            "[0m 2s] Epoch 10 [2560/8158] loss=0.002041622018441558\n",
            "[0m 2s] Epoch 10 [5120/8158] loss=0.0019941246311645953\n",
            "[0m 2s] Epoch 10 [7680/8158] loss=0.001983704111383607\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7897233011824589\n",
            "[0m 2s] Epoch 11 [2560/8158] loss=0.001999536214862019\n",
            "[0m 2s] Epoch 11 [5120/8158] loss=0.001994051050860435\n",
            "[0m 2s] Epoch 11 [7680/8158] loss=0.001986972487065941\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dev set: AUC  0.7899216515892984\n",
            "[0m 2s] Epoch 12 [2560/8158] loss=0.0020026951329782604\n",
            "[0m 3s] Epoch 12 [5120/8158] loss=0.002002684719627723\n",
            "[0m 3s] Epoch 12 [7680/8158] loss=0.0019881717666673164\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7902494982808578\n",
            "[0m 3s] Epoch 13 [2560/8158] loss=0.0019802539027296007\n",
            "[0m 3s] Epoch 13 [5120/8158] loss=0.0020015871326904745\n",
            "[0m 3s] Epoch 13 [7680/8158] loss=0.001980741007719189\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7904118424036531\n",
            "[0m 3s] Epoch 14 [2560/8158] loss=0.001989474217407405\n",
            "[0m 3s] Epoch 14 [5120/8158] loss=0.001990811584983021\n",
            "[0m 3s] Epoch 14 [7680/8158] loss=0.001988030942933013\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1970/2658 74.12%\n",
            "Dev set: AUC  0.7906967342300373\n",
            "[0m 3s] Epoch 15 [2560/8158] loss=0.0019733135122805834\n",
            "[0m 3s] Epoch 15 [5120/8158] loss=0.001968995772767812\n",
            "[0m 3s] Epoch 15 [7680/8158] loss=0.001980992911073069\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dev set: AUC  0.790917509603255\n",
            "[0m 3s] Epoch 16 [2560/8158] loss=0.001960465300362557\n",
            "[0m 4s] Epoch 16 [5120/8158] loss=0.0019848556141369043\n",
            "[0m 4s] Epoch 16 [7680/8158] loss=0.0019857134475993613\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1977/2658 74.38%\n",
            "Dev set: AUC  0.7910814329490348\n",
            "[0m 4s] Epoch 17 [2560/8158] loss=0.0019390163011848926\n",
            "[0m 4s] Epoch 17 [5120/8158] loss=0.0019633222662378104\n",
            "[0m 4s] Epoch 17 [7680/8158] loss=0.001970097100517402\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dev set: AUC  0.7912292482203738\n",
            "[0m 4s] Epoch 18 [2560/8158] loss=0.0019535124185495077\n",
            "[0m 4s] Epoch 18 [5120/8158] loss=0.001956880372017622\n",
            "[0m 4s] Epoch 18 [7680/8158] loss=0.001978122330425928\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dev set: AUC  0.7914414957894756\n",
            "[0m 4s] Epoch 19 [2560/8158] loss=0.0020049146260134876\n",
            "[0m 4s] Epoch 19 [5120/8158] loss=0.0019725207705050705\n",
            "[0m 5s] Epoch 19 [7680/8158] loss=0.001964065940895428\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7918672543060674\n",
            "[0m 5s] Epoch 20 [2560/8158] loss=0.001935749384574592\n",
            "[0m 5s] Epoch 20 [5120/8158] loss=0.0019540904671885074\n",
            "[0m 5s] Epoch 20 [7680/8158] loss=0.0019718556975324947\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7922652184981337\n",
            "[0m 5s] Epoch 21 [2560/8158] loss=0.0020143924630247056\n",
            "[0m 5s] Epoch 21 [5120/8158] loss=0.001969938422553241\n",
            "[0m 5s] Epoch 21 [7680/8158] loss=0.0019596782551767927\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.7925071554593424\n",
            "[0m 5s] Epoch 22 [2560/8158] loss=0.0019475750857964158\n",
            "[0m 5s] Epoch 22 [5120/8158] loss=0.0019484054646454751\n",
            "[0m 5s] Epoch 22 [7680/8158] loss=0.001967054978013039\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dev set: AUC  0.7927661480287823\n",
            "[0m 6s] Epoch 23 [2560/8158] loss=0.002008655807003379\n",
            "[0m 6s] Epoch 23 [5120/8158] loss=0.0019824956892989577\n",
            "[0m 6s] Epoch 23 [7680/8158] loss=0.001959021990963568\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dev set: AUC  0.7928981710702773\n",
            "[0m 6s] Epoch 24 [2560/8158] loss=0.001986112166196108\n",
            "[0m 6s] Epoch 24 [5120/8158] loss=0.001986084843520075\n",
            "[0m 6s] Epoch 24 [7680/8158] loss=0.0019658504597221812\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7933580408033317\n",
            "[0m 6s] Epoch 25 [2560/8158] loss=0.0019596336875110864\n",
            "[0m 6s] Epoch 25 [5120/8158] loss=0.0019810623256489635\n",
            "[0m 6s] Epoch 25 [7680/8158] loss=0.0019625534458706777\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dev set: AUC  0.7932658141810436\n",
            "[0m 6s] Epoch 26 [2560/8158] loss=0.0019808921962976454\n",
            "[0m 6s] Epoch 26 [5120/8158] loss=0.001989464182406664\n",
            "[0m 7s] Epoch 26 [7680/8158] loss=0.001965941465459764\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7937143135086103\n",
            "[0m 7s] Epoch 27 [2560/8158] loss=0.0019943574094213544\n",
            "[0m 7s] Epoch 27 [5120/8158] loss=0.0019460267096292228\n",
            "[0m 7s] Epoch 27 [7680/8158] loss=0.001955277406765769\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7938090668876736\n",
            "[0m 7s] Epoch 28 [2560/8158] loss=0.0019354974734596907\n",
            "[0m 7s] Epoch 28 [5120/8158] loss=0.0019522026123013347\n",
            "[0m 7s] Epoch 28 [7680/8158] loss=0.0019627452129498124\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7936663051298847\n",
            "[0m 7s] Epoch 29 [2560/8158] loss=0.0019467420410364867\n",
            "[0m 7s] Epoch 29 [5120/8158] loss=0.0019472024054266513\n",
            "[0m 7s] Epoch 29 [7680/8158] loss=0.001959927290833245\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dev set: AUC  0.793652407967622\n",
            "[0m 7s] Epoch 30 [2560/8158] loss=0.0019102566759102047\n",
            "[0m 7s] Epoch 30 [5120/8158] loss=0.0019374095718376338\n",
            "[0m 8s] Epoch 30 [7680/8158] loss=0.0019531912791232267\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dev set: AUC  0.7940029954701568\n",
            "[0m 8s] Epoch 31 [2560/8158] loss=0.0019022430176846683\n",
            "[0m 8s] Epoch 31 [5120/8158] loss=0.0019339641614351422\n",
            "[0m 8s] Epoch 31 [7680/8158] loss=0.001956354721914977\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7942070310797399\n",
            "[0m 8s] Epoch 32 [2560/8158] loss=0.0019917648402042686\n",
            "[0m 8s] Epoch 32 [5120/8158] loss=0.001965539524098858\n",
            "[0m 8s] Epoch 32 [7680/8158] loss=0.0019610507025693853\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7942986260128346\n",
            "[0m 8s] Epoch 33 [2560/8158] loss=0.0019522366113960744\n",
            "[0m 8s] Epoch 33 [5120/8158] loss=0.001975641894387081\n",
            "[0m 8s] Epoch 33 [7680/8158] loss=0.001958805179068198\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7942626197287905\n",
            "[0m 9s] Epoch 34 [2560/8158] loss=0.00199171380372718\n",
            "[0m 9s] Epoch 34 [5120/8158] loss=0.0019569973286706955\n",
            "[0m 9s] Epoch 34 [7680/8158] loss=0.00195657474687323\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7945171904738743\n",
            "[0m 9s] Epoch 35 [2560/8158] loss=0.002010419359430671\n",
            "[0m 9s] Epoch 35 [5120/8158] loss=0.0019958848133683205\n",
            "[0m 9s] Epoch 35 [7680/8158] loss=0.0019585434541416666\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7940825883085701\n",
            "[0m 9s] Epoch 36 [2560/8158] loss=0.0019553503254428504\n",
            "[0m 9s] Epoch 36 [5120/8158] loss=0.001945134368725121\n",
            "[0m 9s] Epoch 36 [7680/8158] loss=0.0019599211945508918\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7944243321623922\n",
            "[0m 10s] Epoch 37 [2560/8158] loss=0.0019116150215268135\n",
            "[0m 10s] Epoch 37 [5120/8158] loss=0.0019300333573482932\n",
            "[0m 10s] Epoch 37 [7680/8158] loss=0.001948219487288346\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7934761666825643\n",
            "[0m 10s] Epoch 38 [2560/8158] loss=0.0020184396067634224\n",
            "[0m 10s] Epoch 38 [5120/8158] loss=0.0019716170732863247\n",
            "[0m 10s] Epoch 38 [7680/8158] loss=0.001961624117878576\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7946997486508698\n",
            "[0m 10s] Epoch 39 [2560/8158] loss=0.001998244726564735\n",
            "[0m 10s] Epoch 39 [5120/8158] loss=0.001974156894721091\n",
            "[0m 10s] Epoch 39 [7680/8158] loss=0.0019642826441364984\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dev set: AUC  0.7949998010179039\n",
            "[0m 10s] Epoch 40 [2560/8158] loss=0.0019637173507362606\n",
            "[0m 11s] Epoch 40 [5120/8158] loss=0.001967042847536504\n",
            "[0m 11s] Epoch 40 [7680/8158] loss=0.001951751085774352\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7951930979111933\n",
            "[0m 11s] Epoch 41 [2560/8158] loss=0.0019386855070479213\n",
            "[0m 11s] Epoch 41 [5120/8158] loss=0.0019637635385151954\n",
            "[0m 11s] Epoch 41 [7680/8158] loss=0.001955451851245016\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7952486865602438\n",
            "[0m 11s] Epoch 42 [2560/8158] loss=0.001999104814603925\n",
            "[0m 11s] Epoch 42 [5120/8158] loss=0.0019855015794746577\n",
            "[0m 11s] Epoch 42 [7680/8158] loss=0.001961912754147003\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7951817275057058\n",
            "[0m 11s] Epoch 43 [2560/8158] loss=0.001983746944461018\n",
            "[0m 11s] Epoch 43 [5120/8158] loss=0.001980743010062724\n",
            "[0m 11s] Epoch 43 [7680/8158] loss=0.0019767076980012157\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7952474231818564\n",
            "[0m 11s] Epoch 44 [2560/8158] loss=0.001956244476605207\n",
            "[0m 12s] Epoch 44 [5120/8158] loss=0.001964063569903374\n",
            "[0m 12s] Epoch 44 [7680/8158] loss=0.0019511065833891432\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7951987831139372\n",
            "[0m 12s] Epoch 45 [2560/8158] loss=0.0019806649652309716\n",
            "[0m 12s] Epoch 45 [5120/8158] loss=0.0019825088034849613\n",
            "[0m 12s] Epoch 45 [7680/8158] loss=0.0019573118692884843\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7954179792641705\n",
            "[0m 12s] Epoch 46 [2560/8158] loss=0.0019231789745390415\n",
            "[0m 12s] Epoch 46 [5120/8158] loss=0.0019367079250514508\n",
            "[0m 12s] Epoch 46 [7680/8158] loss=0.0019497870389992992\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7955563191976032\n",
            "[0m 12s] Epoch 47 [2560/8158] loss=0.001956672640517354\n",
            "[0m 12s] Epoch 47 [5120/8158] loss=0.0019407493935432285\n",
            "[0m 12s] Epoch 47 [7680/8158] loss=0.0019478589490366479\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7956769718336105\n",
            "[0m 12s] Epoch 48 [2560/8158] loss=0.001992800913285464\n",
            "[0m 13s] Epoch 48 [5120/8158] loss=0.0019971089612226935\n",
            "[0m 13s] Epoch 48 [7680/8158] loss=0.0019630821227716905\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.795572111427447\n",
            "[0m 13s] Epoch 49 [2560/8158] loss=0.0019291077041998506\n",
            "[0m 13s] Epoch 49 [5120/8158] loss=0.0019260462431702763\n",
            "[0m 13s] Epoch 49 [7680/8158] loss=0.0019446286217619975\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.79561127615746\n",
            "[0m 13s] Epoch 50 [2560/8158] loss=0.00199147907551378\n",
            "[0m 13s] Epoch 50 [5120/8158] loss=0.001925036299508065\n",
            "[0m 13s] Epoch 50 [7680/8158] loss=0.0019413960243885716\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7956055909547161\n",
            "[0m 13s] Epoch 51 [2560/8158] loss=0.0018923387280665338\n",
            "[0m 13s] Epoch 51 [5120/8158] loss=0.0019198150315787644\n",
            "[0m 13s] Epoch 51 [7680/8158] loss=0.0019468899777469535\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7955860085897097\n",
            "[0m 14s] Epoch 52 [2560/8158] loss=0.0019269854412414134\n",
            "[0m 14s] Epoch 52 [5120/8158] loss=0.001937116397311911\n",
            "[0m 14s] Epoch 52 [7680/8158] loss=0.0019477340470378597\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7956965541986171\n",
            "[0m 14s] Epoch 53 [2560/8158] loss=0.001962714234832674\n",
            "[0m 14s] Epoch 53 [5120/8158] loss=0.001980839652242139\n",
            "[0m 14s] Epoch 53 [7680/8158] loss=0.0019514964466604094\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7958822708215813\n",
            "[0m 14s] Epoch 54 [2560/8158] loss=0.0019330422393977642\n",
            "[0m 14s] Epoch 54 [5120/8158] loss=0.0019468445156235249\n",
            "[0m 14s] Epoch 54 [7680/8158] loss=0.0019526855399211247\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7958424744023748\n",
            "[0m 14s] Epoch 55 [2560/8158] loss=0.0019105299725197256\n",
            "[0m 14s] Epoch 55 [5120/8158] loss=0.0019276564416941255\n",
            "[0m 14s] Epoch 55 [7680/8158] loss=0.0019481046940200031\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7956573894686041\n",
            "[0m 15s] Epoch 56 [2560/8158] loss=0.001913502044044435\n",
            "[0m 15s] Epoch 56 [5120/8158] loss=0.0019503048213664442\n",
            "[0m 15s] Epoch 56 [7680/8158] loss=0.0019492436743651826\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7955380002109842\n",
            "[0m 15s] Epoch 57 [2560/8158] loss=0.001981501979753375\n",
            "[0m 15s] Epoch 57 [5120/8158] loss=0.0019598665821831673\n",
            "[0m 15s] Epoch 57 [7680/8158] loss=0.0019509239704348148\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7959871312277448\n",
            "[0m 15s] Epoch 58 [2560/8158] loss=0.001966437452938408\n",
            "[0m 15s] Epoch 58 [5120/8158] loss=0.0019332927826326341\n",
            "[0m 15s] Epoch 58 [7680/8158] loss=0.0019499984259406726\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7951773056813495\n",
            "[0m 15s] Epoch 59 [2560/8158] loss=0.00195291651180014\n",
            "[0m 15s] Epoch 59 [5120/8158] loss=0.0019180282892193646\n",
            "[0m 15s] Epoch 59 [7680/8158] loss=0.001949580576426039\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7960515635255079\n",
            "[0m 16s] Epoch 60 [2560/8158] loss=0.0019112476846203208\n",
            "[0m 16s] Epoch 60 [5120/8158] loss=0.0019224917108658702\n",
            "[0m 16s] Epoch 60 [7680/8158] loss=0.0019379303480188052\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7960168206198514\n",
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.002526638936251402\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.002498229849152267\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.002443528501316905\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1782/2658 67.04%\n",
            "Dev set: AUC  0.7603958669839431\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.0021560805616900325\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.0021323675056919454\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.0021026991967422267\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1945/2658 73.18%\n",
            "Dev set: AUC  0.7855775250038375\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.0020144083071500065\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.0020308643695898356\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.0020217502256855368\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1943/2658 73.10%\n",
            "Dev set: AUC  0.7888174588786127\n",
            "[0m 0s] Epoch 4 [2560/8158] loss=0.001980606536380947\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.0019952187431044877\n",
            "[0m 1s] Epoch 4 [7680/8158] loss=0.0020041306541922193\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dev set: AUC  0.7890619225965962\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.0019631700590252875\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.0019678362412378194\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.001990568672772497\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dev set: AUC  0.7892065794219665\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.002037864690646529\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.0020010360167361796\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.0019951334261956314\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dev set: AUC  0.7892501659763356\n",
            "[0m 1s] Epoch 7 [2560/8158] loss=0.0019086714251898228\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.0019775635912083088\n",
            "[0m 1s] Epoch 7 [7680/8158] loss=0.001987991846787433\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.7895919098301578\n",
            "[0m 1s] Epoch 8 [2560/8158] loss=0.0019766953075304626\n",
            "[0m 1s] Epoch 8 [5120/8158] loss=0.001959059527143836\n",
            "[0m 2s] Epoch 8 [7680/8158] loss=0.001976612532356133\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1981/2658 74.53%\n",
            "Dev set: AUC  0.7898970157107419\n",
            "[0m 2s] Epoch 9 [2560/8158] loss=0.001955331757199019\n",
            "[0m 2s] Epoch 9 [5120/8158] loss=0.0019701992336194963\n",
            "[0m 2s] Epoch 9 [7680/8158] loss=0.0019788422000904878\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1973/2658 74.23%\n",
            "Dev set: AUC  0.7906487258513117\n",
            "[0m 2s] Epoch 10 [2560/8158] loss=0.0020080783986486496\n",
            "[0m 2s] Epoch 10 [5120/8158] loss=0.0019606434274464847\n",
            "[0m 2s] Epoch 10 [7680/8158] loss=0.0019707230113757152\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dev set: AUC  0.791251357342155\n",
            "[0m 2s] Epoch 11 [2560/8158] loss=0.0020170232630334793\n",
            "[0m 2s] Epoch 11 [5120/8158] loss=0.0019634617725387215\n",
            "[0m 2s] Epoch 11 [7680/8158] loss=0.0019704458303749562\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1972/2658 74.19%\n",
            "Dev set: AUC  0.7917895565352353\n",
            "[0m 2s] Epoch 12 [2560/8158] loss=0.0019447007565759123\n",
            "[0m 2s] Epoch 12 [5120/8158] loss=0.0019592887721955774\n",
            "[0m 3s] Epoch 12 [7680/8158] loss=0.0019650991579207282\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.791826826197667\n",
            "[0m 3s] Epoch 13 [2560/8158] loss=0.0019530193647369742\n",
            "[0m 3s] Epoch 13 [5120/8158] loss=0.001982519851299003\n",
            "[0m 3s] Epoch 13 [7680/8158] loss=0.0019759811693802476\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7930226138414473\n",
            "[0m 3s] Epoch 14 [2560/8158] loss=0.0019785529002547264\n",
            "[0m 3s] Epoch 14 [5120/8158] loss=0.001961139403283596\n",
            "[0m 3s] Epoch 14 [7680/8158] loss=0.0019701452809385957\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7931521101261674\n",
            "[0m 3s] Epoch 15 [2560/8158] loss=0.0020475457538850607\n",
            "[0m 3s] Epoch 15 [5120/8158] loss=0.0019942560233175755\n",
            "[0m 3s] Epoch 15 [7680/8158] loss=0.0019722160999663175\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dev set: AUC  0.7938400196581676\n",
            "[0m 3s] Epoch 16 [2560/8158] loss=0.001945107919164002\n",
            "[0m 4s] Epoch 16 [5120/8158] loss=0.001968559325905517\n",
            "[0m 4s] Epoch 16 [7680/8158] loss=0.0019527850556187332\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dev set: AUC  0.7939840447943441\n",
            "[0m 4s] Epoch 17 [2560/8158] loss=0.0019805683405138552\n",
            "[0m 4s] Epoch 17 [5120/8158] loss=0.0019379755598492921\n",
            "[0m 4s] Epoch 17 [7680/8158] loss=0.001956178663143267\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7942133479716775\n",
            "[0m 4s] Epoch 18 [2560/8158] loss=0.0019671214278787373\n",
            "[0m 4s] Epoch 18 [5120/8158] loss=0.0019663356244564055\n",
            "[0m 4s] Epoch 18 [7680/8158] loss=0.0019559059447298447\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7928514260699394\n",
            "[0m 4s] Epoch 19 [2560/8158] loss=0.0019630212453193964\n",
            "[0m 4s] Epoch 19 [5120/8158] loss=0.0019810276688076557\n",
            "[0m 4s] Epoch 19 [7680/8158] loss=0.00196634274131308\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7944161202028733\n",
            "[0m 5s] Epoch 20 [2560/8158] loss=0.0019628564710728823\n",
            "[0m 5s] Epoch 20 [5120/8158] loss=0.001957766804844141\n",
            "[0m 5s] Epoch 20 [7680/8158] loss=0.0019623393503328166\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.794766707705408\n",
            "[0m 5s] Epoch 21 [2560/8158] loss=0.0019711616449058058\n",
            "[0m 5s] Epoch 21 [5120/8158] loss=0.001955846237251535\n",
            "[0m 5s] Epoch 21 [7680/8158] loss=0.0019675737828947605\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dev set: AUC  0.7946340529747191\n",
            "[0m 5s] Epoch 22 [2560/8158] loss=0.001971446943935007\n",
            "[0m 5s] Epoch 22 [5120/8158] loss=0.0019589120172895493\n",
            "[0m 5s] Epoch 22 [7680/8158] loss=0.0019573572324588897\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7947989238542895\n",
            "[0m 5s] Epoch 23 [2560/8158] loss=0.001957483496516943\n",
            "[0m 5s] Epoch 23 [5120/8158] loss=0.0019411655608564614\n",
            "[0m 6s] Epoch 23 [7680/8158] loss=0.0019587401766330004\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.795268900614444\n",
            "[0m 6s] Epoch 24 [2560/8158] loss=0.001944241824094206\n",
            "[0m 6s] Epoch 24 [5120/8158] loss=0.0019730348663870244\n",
            "[0m 6s] Epoch 24 [7680/8158] loss=0.001946628726242731\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7953320695338197\n",
            "[0m 6s] Epoch 25 [2560/8158] loss=0.0019790762104094028\n",
            "[0m 6s] Epoch 25 [5120/8158] loss=0.00195927110617049\n",
            "[0m 6s] Epoch 25 [7680/8158] loss=0.00195993334442998\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.794481815879024\n",
            "[0m 6s] Epoch 26 [2560/8158] loss=0.001938021066598594\n",
            "[0m 6s] Epoch 26 [5120/8158] loss=0.0019384571118280292\n",
            "[0m 6s] Epoch 26 [7680/8158] loss=0.0019458524921598533\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7958292089293059\n",
            "[0m 7s] Epoch 27 [2560/8158] loss=0.0019472057232633234\n",
            "[0m 7s] Epoch 27 [5120/8158] loss=0.0019469138176646083\n",
            "[0m 7s] Epoch 27 [7680/8158] loss=0.00195375084100912\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2009/2658 75.58%\n",
            "Dev set: AUC  0.7958633201457685\n",
            "[0m 7s] Epoch 28 [2560/8158] loss=0.0019383155391551553\n",
            "[0m 7s] Epoch 28 [5120/8158] loss=0.001926654449198395\n",
            "[0m 7s] Epoch 28 [7680/8158] loss=0.001950505697944512\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7954653559537023\n",
            "[0m 7s] Epoch 29 [2560/8158] loss=0.001973712525796145\n",
            "[0m 7s] Epoch 29 [5120/8158] loss=0.0019637852266896514\n",
            "[0m 7s] Epoch 29 [7680/8158] loss=0.0019535768932352465\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dev set: AUC  0.7963061342705916\n",
            "[0m 7s] Epoch 30 [2560/8158] loss=0.001996352570131421\n",
            "[0m 7s] Epoch 30 [5120/8158] loss=0.0019580939202569424\n",
            "[0m 7s] Epoch 30 [7680/8158] loss=0.0019426732091233135\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7958064681183307\n",
            "[0m 8s] Epoch 31 [2560/8158] loss=0.001966485835146159\n",
            "[0m 8s] Epoch 31 [5120/8158] loss=0.0019469526247121393\n",
            "[0m 8s] Epoch 31 [7680/8158] loss=0.00195744571974501\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7959334376462754\n",
            "[0m 8s] Epoch 32 [2560/8158] loss=0.0019451902247965337\n",
            "[0m 8s] Epoch 32 [5120/8158] loss=0.001941898016957566\n",
            "[0m 8s] Epoch 32 [7680/8158] loss=0.0019534631283022463\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7963667764331923\n",
            "[0m 8s] Epoch 33 [2560/8158] loss=0.001911989215295762\n",
            "[0m 8s] Epoch 33 [5120/8158] loss=0.001958610425936058\n",
            "[0m 8s] Epoch 33 [7680/8158] loss=0.0019532435146781307\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.796160214066834\n",
            "[0m 8s] Epoch 34 [2560/8158] loss=0.0019152241060510279\n",
            "[0m 8s] Epoch 34 [5120/8158] loss=0.0019145176687743516\n",
            "[0m 9s] Epoch 34 [7680/8158] loss=0.0019523171319936713\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dev set: AUC  0.795831103996887\n",
            "[0m 9s] Epoch 35 [2560/8158] loss=0.0019500356982462108\n",
            "[0m 9s] Epoch 35 [5120/8158] loss=0.0019508182245772331\n",
            "[0m 9s] Epoch 35 [7680/8158] loss=0.0019430166808888315\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7957995195371992\n",
            "[0m 9s] Epoch 36 [2560/8158] loss=0.0019615715951658784\n",
            "[0m 9s] Epoch 36 [5120/8158] loss=0.0019739887793548405\n",
            "[0m 9s] Epoch 36 [7680/8158] loss=0.001944317532858501\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7960136621738825\n",
            "[0m 9s] Epoch 37 [2560/8158] loss=0.0019554764730855823\n",
            "[0m 9s] Epoch 37 [5120/8158] loss=0.0019545249233487993\n",
            "[0m 9s] Epoch 37 [7680/8158] loss=0.0019426340974556904\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7962075907563657\n",
            "[0m 9s] Epoch 38 [2560/8158] loss=0.0019461186602711678\n",
            "[0m 10s] Epoch 38 [5120/8158] loss=0.0019481103692669422\n",
            "[0m 10s] Epoch 38 [7680/8158] loss=0.0019470464166564247\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2011/2658 75.66%\n",
            "Dev set: AUC  0.7963806735954548\n",
            "[0m 10s] Epoch 39 [2560/8158] loss=0.0019796412670984864\n",
            "[0m 10s] Epoch 39 [5120/8158] loss=0.0019508855999447404\n",
            "[0m 10s] Epoch 39 [7680/8158] loss=0.0019521088106557727\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7964211017038553\n",
            "[0m 10s] Epoch 40 [2560/8158] loss=0.0019565732683986424\n",
            "[0m 10s] Epoch 40 [5120/8158] loss=0.0019260325178038328\n",
            "[0m 10s] Epoch 40 [7680/8158] loss=0.0019389405574960013\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.7967401047467021\n",
            "[0m 10s] Epoch 41 [2560/8158] loss=0.0019281128887087106\n",
            "[0m 10s] Epoch 41 [5120/8158] loss=0.0019021401705686003\n",
            "[0m 10s] Epoch 41 [7680/8158] loss=0.0019297336034166316\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7963446673114108\n",
            "[0m 10s] Epoch 42 [2560/8158] loss=0.0019178274902515112\n",
            "[0m 11s] Epoch 42 [5120/8158] loss=0.0019195541215594857\n",
            "[0m 11s] Epoch 42 [7680/8158] loss=0.001926620308465014\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7955506339948593\n",
            "[0m 11s] Epoch 43 [2560/8158] loss=0.0019399920245632529\n",
            "[0m 11s] Epoch 43 [5120/8158] loss=0.001968289603246376\n",
            "[0m 11s] Epoch 43 [7680/8158] loss=0.0019359940624174973\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7953393339595479\n",
            "[0m 11s] Epoch 44 [2560/8158] loss=0.0019418284180574119\n",
            "[0m 11s] Epoch 44 [5120/8158] loss=0.0019405264407396316\n",
            "[0m 11s] Epoch 44 [7680/8158] loss=0.0019375923050877948\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7956201198061724\n",
            "[0m 11s] Epoch 45 [2560/8158] loss=0.0019190736114978791\n",
            "[0m 11s] Epoch 45 [5120/8158] loss=0.0019522238930221648\n",
            "[0m 11s] Epoch 45 [7680/8158] loss=0.0019384700067651768\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7942900982087191\n",
            "[0m 12s] Epoch 46 [2560/8158] loss=0.001933172217104584\n",
            "[0m 12s] Epoch 46 [5120/8158] loss=0.0019337761623319238\n",
            "[0m 12s] Epoch 46 [7680/8158] loss=0.0019445331456760565\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7961229444044023\n",
            "[0m 12s] Epoch 47 [2560/8158] loss=0.0019574463949538767\n",
            "[0m 12s] Epoch 47 [5120/8158] loss=0.0019311586394906044\n",
            "[0m 12s] Epoch 47 [7680/8158] loss=0.0019443912819648783\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7947458619620141\n",
            "[0m 12s] Epoch 48 [2560/8158] loss=0.0019268582924269139\n",
            "[0m 12s] Epoch 48 [5120/8158] loss=0.0019622350169811397\n",
            "[0m 12s] Epoch 48 [7680/8158] loss=0.0019546579802408814\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dev set: AUC  0.7936650417514972\n",
            "[0m 12s] Epoch 49 [2560/8158] loss=0.0018964089220389723\n",
            "[0m 12s] Epoch 49 [5120/8158] loss=0.0019174155604559928\n",
            "[0m 12s] Epoch 49 [7680/8158] loss=0.0019336576379525164\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7962240146754034\n",
            "[0m 13s] Epoch 50 [2560/8158] loss=0.001931585103739053\n",
            "[0m 13s] Epoch 50 [5120/8158] loss=0.001948188547976315\n",
            "[0m 13s] Epoch 50 [7680/8158] loss=0.0019305429343755046\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7965758655563255\n",
            "[0m 13s] Epoch 51 [2560/8158] loss=0.0018317984533496201\n",
            "[0m 13s] Epoch 51 [5120/8158] loss=0.0019211067527066916\n",
            "[0m 13s] Epoch 51 [7680/8158] loss=0.0019385904034910104\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7955304199406592\n",
            "[0m 13s] Epoch 52 [2560/8158] loss=0.0019781202427111566\n",
            "[0m 13s] Epoch 52 [5120/8158] loss=0.001947897463105619\n",
            "[0m 13s] Epoch 52 [7680/8158] loss=0.0019463880259233217\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7945228756766181\n",
            "[0m 13s] Epoch 53 [2560/8158] loss=0.001958412630483508\n",
            "[0m 13s] Epoch 53 [5120/8158] loss=0.0019226597622036933\n",
            "[0m 14s] Epoch 53 [7680/8158] loss=0.0019330639818993707\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7962322266349222\n",
            "[0m 14s] Epoch 54 [2560/8158] loss=0.0019130415632389486\n",
            "[0m 14s] Epoch 54 [5120/8158] loss=0.0019385736959520727\n",
            "[0m 14s] Epoch 54 [7680/8158] loss=0.0019425104449813564\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7926082257303433\n",
            "[0m 14s] Epoch 55 [2560/8158] loss=0.001954960054717958\n",
            "[0m 14s] Epoch 55 [5120/8158] loss=0.0019170709769241513\n",
            "[0m 14s] Epoch 55 [7680/8158] loss=0.0019288650287004809\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7945241390550057\n",
            "[0m 14s] Epoch 56 [2560/8158] loss=0.0019096348783932625\n",
            "[0m 14s] Epoch 56 [5120/8158] loss=0.0019250787037890405\n",
            "[0m 14s] Epoch 56 [7680/8158] loss=0.001938317168969661\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7954824115619337\n",
            "[0m 15s] Epoch 57 [2560/8158] loss=0.001961619162466377\n",
            "[0m 15s] Epoch 57 [5120/8158] loss=0.001954386901343241\n",
            "[0m 15s] Epoch 57 [7680/8158] loss=0.0019279439506741862\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7931129453961544\n",
            "[0m 15s] Epoch 58 [2560/8158] loss=0.0019197132671251893\n",
            "[0m 15s] Epoch 58 [5120/8158] loss=0.0019433392328210175\n",
            "[0m 15s] Epoch 58 [7680/8158] loss=0.0019287187412070732\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.7930662003958164\n",
            "[0m 15s] Epoch 59 [2560/8158] loss=0.0019851457327604294\n",
            "[0m 15s] Epoch 59 [5120/8158] loss=0.0019432042143307626\n",
            "[0m 15s] Epoch 59 [7680/8158] loss=0.0019399419388112923\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7945677256093747\n",
            "[0m 15s] Epoch 60 [2560/8158] loss=0.001957353623583913\n",
            "[0m 15s] Epoch 60 [5120/8158] loss=0.0019415445916820318\n",
            "[0m 15s] Epoch 60 [7680/8158] loss=0.0019268569614117345\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.795779305482999\n",
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.002514921221882105\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.0023309620679356157\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.0022288853845869503\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1971/2658 74.15%\n",
            "Dev set: AUC  0.7898963840215482\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.0019928129040636122\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.0019917449448257686\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.0020056611431452137\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1968/2658 74.04%\n",
            "Dev set: AUC  0.791979379137959\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.0019779273425228894\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.0019604356435593217\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.0019792055517124634\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7926480221495499\n",
            "[0m 0s] Epoch 4 [2560/8158] loss=0.002001010731328279\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.001992973987944424\n",
            "[0m 0s] Epoch 4 [7680/8158] loss=0.0019838955990659694\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7938671822934992\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.001971725549083203\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.0019664283725433053\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.001976350861756752\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1973/2658 74.23%\n",
            "Dev set: AUC  0.792427562620929\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.001968612358905375\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.001992297009564936\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.0019834081176668406\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.794711750745551\n",
            "[0m 1s] Epoch 7 [2560/8158] loss=0.0019101800047792495\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.0019558299565687774\n",
            "[0m 1s] Epoch 7 [7680/8158] loss=0.001971071116471042\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.792914594989315\n",
            "[0m 1s] Epoch 8 [2560/8158] loss=0.001967252162285149\n",
            "[0m 1s] Epoch 8 [5120/8158] loss=0.001968816772568971\n",
            "[0m 1s] Epoch 8 [7680/8158] loss=0.0019574489250468712\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1978/2658 74.42%\n",
            "Dev set: AUC  0.7909449880831833\n",
            "[0m 2s] Epoch 9 [2560/8158] loss=0.001959962910041213\n",
            "[0m 2s] Epoch 9 [5120/8158] loss=0.001987246930366382\n",
            "[0m 2s] Epoch 9 [7680/8158] loss=0.0019687251498301823\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.7941166995250329\n",
            "[0m 2s] Epoch 10 [2560/8158] loss=0.0019338965765200555\n",
            "[0m 2s] Epoch 10 [5120/8158] loss=0.0019502411247231065\n",
            "[0m 2s] Epoch 10 [7680/8158] loss=0.0019664371114534638\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dev set: AUC  0.7930415645172599\n",
            "[0m 2s] Epoch 11 [2560/8158] loss=0.001931090303696692\n",
            "[0m 2s] Epoch 11 [5120/8158] loss=0.0019441478536464274\n",
            "[0m 2s] Epoch 11 [7680/8158] loss=0.001968971445846061\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.795007381288229\n",
            "[0m 3s] Epoch 12 [2560/8158] loss=0.0019431051448918879\n",
            "[0m 3s] Epoch 12 [5120/8158] loss=0.0019348237954545766\n",
            "[0m 3s] Epoch 12 [7680/8158] loss=0.001962309842929244\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.794722489461845\n",
            "[0m 3s] Epoch 13 [2560/8158] loss=0.0019494344596751034\n",
            "[0m 3s] Epoch 13 [5120/8158] loss=0.0019719657255336644\n",
            "[0m 3s] Epoch 13 [7680/8158] loss=0.0019656607105086247\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7940996439168015\n",
            "[0m 3s] Epoch 14 [2560/8158] loss=0.0019440242904238403\n",
            "[0m 3s] Epoch 14 [5120/8158] loss=0.0019584017747547476\n",
            "[0m 3s] Epoch 14 [7680/8158] loss=0.001953952204591284\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7951097149376176\n",
            "[0m 3s] Epoch 15 [2560/8158] loss=0.0020271823043003677\n",
            "[0m 3s] Epoch 15 [5120/8158] loss=0.001984353468287736\n",
            "[0m 3s] Epoch 15 [7680/8158] loss=0.0019649663823656737\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7945374045280744\n",
            "[0m 4s] Epoch 16 [2560/8158] loss=0.001928719796705991\n",
            "[0m 4s] Epoch 16 [5120/8158] loss=0.0019355373340658843\n",
            "[0m 4s] Epoch 16 [7680/8158] loss=0.0019582884619012476\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7956643380497352\n",
            "[0m 4s] Epoch 17 [2560/8158] loss=0.00189145797630772\n",
            "[0m 4s] Epoch 17 [5120/8158] loss=0.0019556067942176013\n",
            "[0m 4s] Epoch 17 [7680/8158] loss=0.0019569402871032556\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7946612156100507\n",
            "[0m 4s] Epoch 18 [2560/8158] loss=0.0020579014788381757\n",
            "[0m 4s] Epoch 18 [5120/8158] loss=0.001999180775601417\n",
            "[0m 4s] Epoch 18 [7680/8158] loss=0.00196675747865811\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1952/2658 73.44%\n",
            "Dev set: AUC  0.7957496160908926\n",
            "[0m 4s] Epoch 19 [2560/8158] loss=0.001954421983100474\n",
            "[0m 4s] Epoch 19 [5120/8158] loss=0.0019924052408896386\n",
            "[0m 5s] Epoch 19 [7680/8158] loss=0.0019638721986363333\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7947218577726513\n",
            "[0m 5s] Epoch 20 [2560/8158] loss=0.002022925508208573\n",
            "[0m 5s] Epoch 20 [5120/8158] loss=0.0019651977985631675\n",
            "[0m 5s] Epoch 20 [7680/8158] loss=0.0019612457642021277\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.795818470213012\n",
            "[0m 5s] Epoch 21 [2560/8158] loss=0.001937123155221343\n",
            "[0m 5s] Epoch 21 [5120/8158] loss=0.0019425605714786798\n",
            "[0m 5s] Epoch 21 [7680/8158] loss=0.00194468133073921\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7950427558830794\n",
            "[0m 5s] Epoch 22 [2560/8158] loss=0.0019082990125752985\n",
            "[0m 5s] Epoch 22 [5120/8158] loss=0.001935615506954491\n",
            "[0m 5s] Epoch 22 [7680/8158] loss=0.0019480595792022845\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7953863948044827\n",
            "[0m 5s] Epoch 23 [2560/8158] loss=0.0019187208963558078\n",
            "[0m 5s] Epoch 23 [5120/8158] loss=0.0019362415187060833\n",
            "[0m 6s] Epoch 23 [7680/8158] loss=0.0019469316583126783\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7951956246679683\n",
            "[0m 6s] Epoch 24 [2560/8158] loss=0.0019074477837421\n",
            "[0m 6s] Epoch 24 [5120/8158] loss=0.0019467925361823291\n",
            "[0m 6s] Epoch 24 [7680/8158] loss=0.0019391451651851336\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7920049625503061\n",
            "[0m 6s] Epoch 25 [2560/8158] loss=0.0019607149297371507\n",
            "[0m 6s] Epoch 25 [5120/8158] loss=0.0019525038253050298\n",
            "[0m 6s] Epoch 25 [7680/8158] loss=0.001943574675048391\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dev set: AUC  0.7922829057955589\n",
            "[0m 6s] Epoch 26 [2560/8158] loss=0.001968153950292617\n",
            "[0m 6s] Epoch 26 [5120/8158] loss=0.0019595833669882267\n",
            "[0m 6s] Epoch 26 [7680/8158] loss=0.0019546450736622014\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dev set: AUC  0.7951766739921557\n",
            "[0m 6s] Epoch 27 [2560/8158] loss=0.0019707358675077557\n",
            "[0m 6s] Epoch 27 [5120/8158] loss=0.001981928793247789\n",
            "[0m 7s] Epoch 27 [7680/8158] loss=0.0019493404659442603\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7950313854775917\n",
            "[0m 7s] Epoch 28 [2560/8158] loss=0.001960960030555725\n",
            "[0m 7s] Epoch 28 [5120/8158] loss=0.0019473787513561546\n",
            "[0m 7s] Epoch 28 [7680/8158] loss=0.0019579009191753962\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7938261224959051\n",
            "[0m 7s] Epoch 29 [2560/8158] loss=0.0019181376788765191\n",
            "[0m 7s] Epoch 29 [5120/8158] loss=0.001951286318944767\n",
            "[0m 7s] Epoch 29 [7680/8158] loss=0.0019257127462575833\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7939221392533561\n",
            "[0m 7s] Epoch 30 [2560/8158] loss=0.001981439092196524\n",
            "[0m 7s] Epoch 30 [5120/8158] loss=0.001951067679328844\n",
            "[0m 7s] Epoch 30 [7680/8158] loss=0.0019310661048317949\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7931660072884299\n",
            "[0m 7s] Epoch 31 [2560/8158] loss=0.0019296971848234534\n",
            "[0m 7s] Epoch 31 [5120/8158] loss=0.0019527287047822028\n",
            "[0m 8s] Epoch 31 [7680/8158] loss=0.001951385549424837\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dev set: AUC  0.791896943698174\n",
            "[0m 8s] Epoch 32 [2560/8158] loss=0.0019512563943862916\n",
            "[0m 8s] Epoch 32 [5120/8158] loss=0.001958465756615624\n",
            "[0m 8s] Epoch 32 [7680/8158] loss=0.0019488427943239609\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7946909050021572\n",
            "[0m 8s] Epoch 33 [2560/8158] loss=0.001886482466943562\n",
            "[0m 8s] Epoch 33 [5120/8158] loss=0.0019184526987373828\n",
            "[0m 8s] Epoch 33 [7680/8158] loss=0.0019298305618576706\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7938033816849298\n",
            "[0m 8s] Epoch 34 [2560/8158] loss=0.0019609072245657445\n",
            "[0m 8s] Epoch 34 [5120/8158] loss=0.0019335029879584908\n",
            "[0m 8s] Epoch 34 [7680/8158] loss=0.0019182962675889333\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dev set: AUC  0.7922039446463394\n",
            "[0m 9s] Epoch 35 [2560/8158] loss=0.0019463744945824146\n",
            "[0m 9s] Epoch 35 [5120/8158] loss=0.00190890611265786\n",
            "[0m 9s] Epoch 35 [7680/8158] loss=0.0019283819400394955\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dev set: AUC  0.7916284757908274\n",
            "[0m 9s] Epoch 36 [2560/8158] loss=0.0018774921190924942\n",
            "[0m 9s] Epoch 36 [5120/8158] loss=0.001946621132083237\n",
            "[0m 9s] Epoch 36 [7680/8158] loss=0.0019337594625540079\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1977/2658 74.38%\n",
            "Dev set: AUC  0.7908830825421953\n",
            "[0m 9s] Epoch 37 [2560/8158] loss=0.00191255125682801\n",
            "[0m 9s] Epoch 37 [5120/8158] loss=0.0018935081199742854\n",
            "[0m 9s] Epoch 37 [7680/8158] loss=0.0019102676305919886\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7908022263253944\n",
            "[0m 10s] Epoch 38 [2560/8158] loss=0.0019252034253440797\n",
            "[0m 10s] Epoch 38 [5120/8158] loss=0.001903722295537591\n",
            "[0m 10s] Epoch 38 [7680/8158] loss=0.0019142913399264217\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2009/2658 75.58%\n",
            "Dev set: AUC  0.7921508827540639\n",
            "[0m 10s] Epoch 39 [2560/8158] loss=0.0018599792500026524\n",
            "[0m 10s] Epoch 39 [5120/8158] loss=0.0019028721551876515\n",
            "[0m 10s] Epoch 39 [7680/8158] loss=0.001910292221388469\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7949726383825724\n",
            "[0m 10s] Epoch 40 [2560/8158] loss=0.0019060668186284603\n",
            "[0m 10s] Epoch 40 [5120/8158] loss=0.0019284520123619587\n",
            "[0m 10s] Epoch 40 [7680/8158] loss=0.0019125052456123134\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7874738559634936\n",
            "[0m 10s] Epoch 41 [2560/8158] loss=0.0018875046400353313\n",
            "[0m 10s] Epoch 41 [5120/8158] loss=0.0019125732418615372\n",
            "[0m 10s] Epoch 41 [7680/8158] loss=0.0019141085678711534\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7896841364524461\n",
            "[0m 11s] Epoch 42 [2560/8158] loss=0.0019054918782785534\n",
            "[0m 11s] Epoch 42 [5120/8158] loss=0.001895565411541611\n",
            "[0m 11s] Epoch 42 [7680/8158] loss=0.0019086439977400004\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7929196485028652\n",
            "[0m 11s] Epoch 43 [2560/8158] loss=0.0018517727265134453\n",
            "[0m 11s] Epoch 43 [5120/8158] loss=0.0018851480446755887\n",
            "[0m 11s] Epoch 43 [7680/8158] loss=0.0019046755507588387\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7896790829388961\n",
            "[0m 11s] Epoch 44 [2560/8158] loss=0.0019742511794902384\n",
            "[0m 11s] Epoch 44 [5120/8158] loss=0.0018854169291444122\n",
            "[0m 11s] Epoch 44 [7680/8158] loss=0.0019141845676737526\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1973/2658 74.23%\n",
            "Dev set: AUC  0.7902766609161893\n",
            "[0m 11s] Epoch 45 [2560/8158] loss=0.0019402819802053274\n",
            "[0m 11s] Epoch 45 [5120/8158] loss=0.0019234078412409873\n",
            "[0m 11s] Epoch 45 [7680/8158] loss=0.0019136531821762521\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dev set: AUC  0.7905413386883732\n",
            "[0m 12s] Epoch 46 [2560/8158] loss=0.001885983196552843\n",
            "[0m 12s] Epoch 46 [5120/8158] loss=0.0018939324771054089\n",
            "[0m 12s] Epoch 46 [7680/8158] loss=0.0018962983430052796\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1979/2658 74.45%\n",
            "Dev set: AUC  0.7895590619920824\n",
            "[0m 12s] Epoch 47 [2560/8158] loss=0.0018951072939671575\n",
            "[0m 12s] Epoch 47 [5120/8158] loss=0.0019085769483353943\n",
            "[0m 12s] Epoch 47 [7680/8158] loss=0.0018941519044650098\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dev set: AUC  0.7875780846804631\n",
            "[0m 12s] Epoch 48 [2560/8158] loss=0.0019101278972811996\n",
            "[0m 12s] Epoch 48 [5120/8158] loss=0.0019225248775910585\n",
            "[0m 12s] Epoch 48 [7680/8158] loss=0.0018926110196237763\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dev set: AUC  0.7885540444848163\n",
            "[0m 12s] Epoch 49 [2560/8158] loss=0.0018302558222785593\n",
            "[0m 12s] Epoch 49 [5120/8158] loss=0.0018568539177067577\n",
            "[0m 13s] Epoch 49 [7680/8158] loss=0.0018844828630487124\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7868560639319999\n",
            "[0m 13s] Epoch 50 [2560/8158] loss=0.0018269826425239444\n",
            "[0m 13s] Epoch 50 [5120/8158] loss=0.0018607615726068615\n",
            "[0m 13s] Epoch 50 [7680/8158] loss=0.001888583965289096\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7874163722468617\n",
            "[0m 13s] Epoch 51 [2560/8158] loss=0.0018285366240888834\n",
            "[0m 13s] Epoch 51 [5120/8158] loss=0.0018540590186603366\n",
            "[0m 13s] Epoch 51 [7680/8158] loss=0.0018590415750319759\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dev set: AUC  0.7885091945520598\n",
            "[0m 13s] Epoch 52 [2560/8158] loss=0.0018644819152541458\n",
            "[0m 13s] Epoch 52 [5120/8158] loss=0.0018561690696515143\n",
            "[0m 13s] Epoch 52 [7680/8158] loss=0.0018526014367428918\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7822232553849925\n",
            "[0m 13s] Epoch 53 [2560/8158] loss=0.0018378105130977928\n",
            "[0m 14s] Epoch 53 [5120/8158] loss=0.0018296727736014872\n",
            "[0m 14s] Epoch 53 [7680/8158] loss=0.0018475212273187935\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1985/2658 74.68%\n",
            "Dev set: AUC  0.7865073714970466\n",
            "[0m 14s] Epoch 54 [2560/8158] loss=0.001874244154896587\n",
            "[0m 14s] Epoch 54 [5120/8158] loss=0.0018709363299421965\n",
            "[0m 14s] Epoch 54 [7680/8158] loss=0.0018591552235496542\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7826578575502967\n",
            "[0m 14s] Epoch 55 [2560/8158] loss=0.001757728506345302\n",
            "[0m 14s] Epoch 55 [5120/8158] loss=0.0017670986417215317\n",
            "[0m 14s] Epoch 55 [7680/8158] loss=0.0018463889524961512\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1964/2658 73.89%\n",
            "Dev set: AUC  0.77398223816325\n",
            "[0m 14s] Epoch 56 [2560/8158] loss=0.001808783272281289\n",
            "[0m 14s] Epoch 56 [5120/8158] loss=0.0018479834194295107\n",
            "[0m 14s] Epoch 56 [7680/8158] loss=0.001848725629194329\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.7817930750440445\n",
            "[0m 14s] Epoch 57 [2560/8158] loss=0.0018035075278021394\n",
            "[0m 15s] Epoch 57 [5120/8158] loss=0.0018333024345338345\n",
            "[0m 15s] Epoch 57 [7680/8158] loss=0.001830922889833649\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1958/2658 73.66%\n",
            "Dev set: AUC  0.7727788702491447\n",
            "[0m 15s] Epoch 58 [2560/8158] loss=0.0017574784928001464\n",
            "[0m 15s] Epoch 58 [5120/8158] loss=0.0018154874967876821\n",
            "[0m 15s] Epoch 58 [7680/8158] loss=0.0018376893014647067\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dev set: AUC  0.7852509416906654\n",
            "[0m 15s] Epoch 59 [2560/8158] loss=0.0018584599369205535\n",
            "[0m 15s] Epoch 59 [5120/8158] loss=0.0018284923513419925\n",
            "[0m 15s] Epoch 59 [7680/8158] loss=0.0018215340717385212\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1963/2658 73.85%\n",
            "Dev set: AUC  0.7786668452241454\n",
            "[0m 15s] Epoch 60 [2560/8158] loss=0.0017710796208120882\n",
            "[0m 15s] Epoch 60 [5120/8158] loss=0.001785631966777146\n",
            "[0m 15s] Epoch 60 [7680/8158] loss=0.0018019172285373012\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1983/2658 74.60%\n",
            "Dev set: AUC  0.7800344523286274\n",
            "Training for 60 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.01092109011951834\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.007327511662151664\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.00585407578231146\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.37607205552295336\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.0027542314492166042\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.0026445074006915094\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.0027010377496480944\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.45295684236259337\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.00265001798979938\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.0025752451503649352\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.00262709214972953\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.37765601617629685\n",
            "[0m 0s] Epoch 4 [2560/8158] loss=0.0026108231395483017\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.0026108919642865657\n",
            "[0m 1s] Epoch 4 [7680/8158] loss=0.0026009118960549434\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.451095254308594\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.002676666434854269\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.0027396600577048956\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.002774334140121937\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6170864346640709\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.002510630153119564\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.0025411007343791427\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.0025708043094103536\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 901/2658 33.90%\n",
            "Dev set: AUC  0.6205111376280197\n",
            "[0m 1s] Epoch 7 [2560/8158] loss=0.0031408927170559764\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.0029401698615401984\n",
            "[0m 1s] Epoch 7 [7680/8158] loss=0.0028192055178806186\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6209536359082458\n",
            "[0m 1s] Epoch 8 [2560/8158] loss=0.0025618883781135084\n",
            "[0m 1s] Epoch 8 [5120/8158] loss=0.0026273966301232576\n",
            "[0m 2s] Epoch 8 [7680/8158] loss=0.002666169428266585\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6292262375896762\n",
            "[0m 2s] Epoch 9 [2560/8158] loss=0.0026289721252396705\n",
            "[0m 2s] Epoch 9 [5120/8158] loss=0.0025969070615246893\n",
            "[0m 2s] Epoch 9 [7680/8158] loss=0.0025900796754285693\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6206807461765432\n",
            "[0m 2s] Epoch 10 [2560/8158] loss=0.002632981794886291\n",
            "[0m 2s] Epoch 10 [5120/8158] loss=0.0026219078106805684\n",
            "[0m 2s] Epoch 10 [7680/8158] loss=0.0026804759011914334\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6205171386753604\n",
            "[0m 2s] Epoch 11 [2560/8158] loss=0.00266537053976208\n",
            "[0m 2s] Epoch 11 [5120/8158] loss=0.002610499900765717\n",
            "[0m 2s] Epoch 11 [7680/8158] loss=0.0025826373836025595\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6203011009710958\n",
            "[0m 3s] Epoch 12 [2560/8158] loss=0.0025946506531909106\n",
            "[0m 3s] Epoch 12 [5120/8158] loss=0.0025560725829564036\n",
            "[0m 3s] Epoch 12 [7680/8158] loss=0.0025970148543516796\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6205244031010886\n",
            "[0m 3s] Epoch 13 [2560/8158] loss=0.0024803418200463057\n",
            "[0m 3s] Epoch 13 [5120/8158] loss=0.0025410301284864547\n",
            "[0m 3s] Epoch 13 [7680/8158] loss=0.002578495504955451\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6197771147848751\n",
            "[0m 3s] Epoch 14 [2560/8158] loss=0.0030819335486739876\n",
            "[0m 3s] Epoch 14 [5120/8158] loss=0.0032655906747095288\n",
            "[0m 3s] Epoch 14 [7680/8158] loss=0.0031555313306550186\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6204915552630133\n",
            "[0m 3s] Epoch 15 [2560/8158] loss=0.00268879015929997\n",
            "[0m 4s] Epoch 15 [5120/8158] loss=0.0027874377090483906\n",
            "[0m 4s] Epoch 15 [7680/8158] loss=0.002784750028513372\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6197726929605188\n",
            "[0m 4s] Epoch 16 [2560/8158] loss=0.0025965237990021707\n",
            "[0m 4s] Epoch 16 [5120/8158] loss=0.0026278847362846136\n",
            "[0m 4s] Epoch 16 [7680/8158] loss=0.0026368669544657073\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6193943111334589\n",
            "[0m 4s] Epoch 17 [2560/8158] loss=0.0025595746468752623\n",
            "[0m 4s] Epoch 17 [5120/8158] loss=0.0026443990180268885\n",
            "[0m 4s] Epoch 17 [7680/8158] loss=0.0026382454360524814\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6178356180478657\n",
            "[0m 5s] Epoch 18 [2560/8158] loss=0.002638030890375376\n",
            "[0m 5s] Epoch 18 [5120/8158] loss=0.0025756563525646927\n",
            "[0m 5s] Epoch 18 [7680/8158] loss=0.0025862712335462373\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.621129561348707\n",
            "[0m 5s] Epoch 19 [2560/8158] loss=0.0025730417808517815\n",
            "[0m 5s] Epoch 19 [5120/8158] loss=0.0025503831682726743\n",
            "[0m 5s] Epoch 19 [7680/8158] loss=0.0025458072777837515\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6169007180411066\n",
            "[0m 5s] Epoch 20 [2560/8158] loss=0.002683473448269069\n",
            "[0m 5s] Epoch 20 [5120/8158] loss=0.002705443208105862\n",
            "[0m 5s] Epoch 20 [7680/8158] loss=0.002678944286890328\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6170355836839735\n",
            "[0m 5s] Epoch 21 [2560/8158] loss=0.002867878251709044\n",
            "[0m 5s] Epoch 21 [5120/8158] loss=0.002756172465160489\n",
            "[0m 5s] Epoch 21 [7680/8158] loss=0.002671573986299336\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6188984351163603\n",
            "[0m 6s] Epoch 22 [2560/8158] loss=0.002587360772304237\n",
            "[0m 6s] Epoch 22 [5120/8158] loss=0.002594523341394961\n",
            "[0m 6s] Epoch 22 [7680/8158] loss=0.0025628346716985106\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6201485480308037\n",
            "[0m 6s] Epoch 23 [2560/8158] loss=0.002603668300434947\n",
            "[0m 6s] Epoch 23 [5120/8158] loss=0.0026701084687374534\n",
            "[0m 6s] Epoch 23 [7680/8158] loss=0.0026707176972801485\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6293639458339151\n",
            "[0m 6s] Epoch 24 [2560/8158] loss=0.0026676228968426584\n",
            "[0m 6s] Epoch 24 [5120/8158] loss=0.0026334997382946312\n",
            "[0m 6s] Epoch 24 [7680/8158] loss=0.0025959046790376304\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 901/2658 33.90%\n",
            "Dev set: AUC  0.620608733608455\n",
            "[0m 6s] Epoch 25 [2560/8158] loss=0.002785254130139947\n",
            "[0m 6s] Epoch 25 [5120/8158] loss=0.0028244278044439854\n",
            "[0m 6s] Epoch 25 [7680/8158] loss=0.002755203152385851\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6190219303537396\n",
            "[0m 7s] Epoch 26 [2560/8158] loss=0.002509799157269299\n",
            "[0m 7s] Epoch 26 [5120/8158] loss=0.002521005691960454\n",
            "[0m 7s] Epoch 26 [7680/8158] loss=0.002542293188162148\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6200146299217274\n",
            "[0m 7s] Epoch 27 [2560/8158] loss=0.0026960249757394194\n",
            "[0m 7s] Epoch 27 [5120/8158] loss=0.0026658216840587555\n",
            "[0m 7s] Epoch 27 [7680/8158] loss=0.0026114049289996427\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.619569920729323\n",
            "[0m 7s] Epoch 28 [2560/8158] loss=0.0025658080819994213\n",
            "[0m 7s] Epoch 28 [5120/8158] loss=0.0026511056581512093\n",
            "[0m 7s] Epoch 28 [7680/8158] loss=0.0026599633196989696\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 901/2658 33.90%\n",
            "Dev set: AUC  0.6210247009425434\n",
            "[0m 7s] Epoch 29 [2560/8158] loss=0.0029784894781187178\n",
            "[0m 7s] Epoch 29 [5120/8158] loss=0.002959729661233723\n",
            "[0m 7s] Epoch 29 [7680/8158] loss=0.0029336291442935663\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6204116465800031\n",
            "[0m 8s] Epoch 30 [2560/8158] loss=0.0027497249888256193\n",
            "[0m 8s] Epoch 30 [5120/8158] loss=0.0026802045991644262\n",
            "[0m 8s] Epoch 30 [7680/8158] loss=0.0027192829952885707\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6179117365957132\n",
            "[0m 8s] Epoch 31 [2560/8158] loss=0.0028227553237229587\n",
            "[0m 8s] Epoch 31 [5120/8158] loss=0.002752867655362934\n",
            "[0m 8s] Epoch 31 [7680/8158] loss=0.0027147127548232675\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6197347916088934\n",
            "[0m 8s] Epoch 32 [2560/8158] loss=0.002593429945409298\n",
            "[0m 8s] Epoch 32 [5120/8158] loss=0.0026224001543596388\n",
            "[0m 8s] Epoch 32 [7680/8158] loss=0.002637263860863944\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6171325479752151\n",
            "[0m 8s] Epoch 33 [2560/8158] loss=0.002744490746408701\n",
            "[0m 8s] Epoch 33 [5120/8158] loss=0.002750196016859263\n",
            "[0m 9s] Epoch 33 [7680/8158] loss=0.002753724957195421\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6183295989973829\n",
            "[0m 9s] Epoch 34 [2560/8158] loss=0.0026887300657108424\n",
            "[0m 9s] Epoch 34 [5120/8158] loss=0.002661719499155879\n",
            "[0m 9s] Epoch 34 [7680/8158] loss=0.002660381840541959\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6196226667770017\n",
            "[0m 9s] Epoch 35 [2560/8158] loss=0.0025721196783706547\n",
            "[0m 9s] Epoch 35 [5120/8158] loss=0.002595916565041989\n",
            "[0m 9s] Epoch 35 [7680/8158] loss=0.002577117085456848\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6215597416896549\n",
            "[0m 9s] Epoch 36 [2560/8158] loss=0.0025861028349027037\n",
            "[0m 9s] Epoch 36 [5120/8158] loss=0.0026169301592744885\n",
            "[0m 9s] Epoch 36 [7680/8158] loss=0.0025993137154728175\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6193273520789206\n",
            "[0m 9s] Epoch 37 [2560/8158] loss=0.002579610422253609\n",
            "[0m 10s] Epoch 37 [5120/8158] loss=0.0025839779525995256\n",
            "[0m 10s] Epoch 37 [7680/8158] loss=0.0026015994527066747\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6205935730678049\n",
            "[0m 10s] Epoch 38 [2560/8158] loss=0.0026530766393989325\n",
            "[0m 10s] Epoch 38 [5120/8158] loss=0.002623870060779154\n",
            "[0m 10s] Epoch 38 [7680/8158] loss=0.002630942004422347\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6206030484057112\n",
            "[0m 10s] Epoch 39 [2560/8158] loss=0.0025663137435913085\n",
            "[0m 10s] Epoch 39 [5120/8158] loss=0.0025540155591443183\n",
            "[0m 10s] Epoch 39 [7680/8158] loss=0.0026107453585912785\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 901/2658 33.90%\n",
            "Dev set: AUC  0.6210089087126995\n",
            "[0m 10s] Epoch 40 [2560/8158] loss=0.0027018575463443996\n",
            "[0m 10s] Epoch 40 [5120/8158] loss=0.002591597102582455\n",
            "[0m 11s] Epoch 40 [7680/8158] loss=0.0025565308906758824\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.6202038208352574\n",
            "[0m 11s] Epoch 41 [2560/8158] loss=0.0027009571669623254\n",
            "[0m 11s] Epoch 41 [5120/8158] loss=0.0027071400661952795\n",
            "[0m 11s] Epoch 41 [7680/8158] loss=0.0028329715055103104\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 901/2658 33.90%\n",
            "Dev set: AUC  0.3822499758378883\n",
            "[0m 11s] Epoch 42 [2560/8158] loss=0.0032270540948957207\n",
            "[0m 11s] Epoch 42 [5120/8158] loss=0.0030504244961775838\n",
            "[0m 11s] Epoch 42 [7680/8158] loss=0.002894787723198533\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.4048268634673294\n",
            "[0m 11s] Epoch 43 [2560/8158] loss=0.0024979595094919206\n",
            "[0m 11s] Epoch 43 [5120/8158] loss=0.0025600004941225053\n",
            "[0m 11s] Epoch 43 [7680/8158] loss=0.0025829473820825418\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.3898681475145873\n",
            "[0m 11s] Epoch 44 [2560/8158] loss=0.002631623181514442\n",
            "[0m 12s] Epoch 44 [5120/8158] loss=0.0026077268877997993\n",
            "[0m 12s] Epoch 44 [7680/8158] loss=0.002590980458383759\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 901/2658 33.90%\n",
            "Dev set: AUC  0.47426529809097207\n",
            "[0m 12s] Epoch 45 [2560/8158] loss=0.0028829539427533747\n",
            "[0m 12s] Epoch 45 [5120/8158] loss=0.002717617724556476\n",
            "[0m 12s] Epoch 45 [7680/8158] loss=0.0026876572597151\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.3503970482427354\n",
            "[0m 12s] Epoch 46 [2560/8158] loss=0.002642841334454715\n",
            "[0m 12s] Epoch 46 [5120/8158] loss=0.002586797229014337\n",
            "[0m 12s] Epoch 46 [7680/8158] loss=0.0025916505139321087\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.39431587112782424\n",
            "[0m 12s] Epoch 47 [2560/8158] loss=0.002513662283308804\n",
            "[0m 12s] Epoch 47 [5120/8158] loss=0.0026416397537104785\n",
            "[0m 13s] Epoch 47 [7680/8158] loss=0.0026730829151347278\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.401983630406233\n",
            "[0m 13s] Epoch 48 [2560/8158] loss=0.002808463922701776\n",
            "[0m 13s] Epoch 48 [5120/8158] loss=0.002767181780654937\n",
            "[0m 13s] Epoch 48 [7680/8158] loss=0.0027943897759541867\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.40093471050000096\n",
            "[0m 13s] Epoch 49 [2560/8158] loss=0.0025892040692269804\n",
            "[0m 13s] Epoch 49 [5120/8158] loss=0.0026320942328311504\n",
            "[0m 13s] Epoch 49 [7680/8158] loss=0.0026042291040842733\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.40267311916121784\n",
            "[0m 13s] Epoch 50 [2560/8158] loss=0.0026296126190572975\n",
            "[0m 13s] Epoch 50 [5120/8158] loss=0.00263307933928445\n",
            "[0m 13s] Epoch 50 [7680/8158] loss=0.0027336872881278396\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.3916435099936389\n",
            "[0m 13s] Epoch 51 [2560/8158] loss=0.00279603386297822\n",
            "[0m 13s] Epoch 51 [5120/8158] loss=0.0026727563235908747\n",
            "[0m 14s] Epoch 51 [7680/8158] loss=0.0026697265682742\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.41669883017478204\n",
            "[0m 14s] Epoch 52 [2560/8158] loss=0.0027327540796250106\n",
            "[0m 14s] Epoch 52 [5120/8158] loss=0.0028694158885627986\n",
            "[0m 14s] Epoch 52 [7680/8158] loss=0.002856100800757607\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.39694717246441547\n",
            "[0m 14s] Epoch 53 [2560/8158] loss=0.002627040771767497\n",
            "[0m 14s] Epoch 53 [5120/8158] loss=0.002596698573324829\n",
            "[0m 14s] Epoch 53 [7680/8158] loss=0.0025945217503855625\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.4034665207885756\n",
            "[0m 14s] Epoch 54 [2560/8158] loss=0.002570247952826321\n",
            "[0m 14s] Epoch 54 [5120/8158] loss=0.00255921269999817\n",
            "[0m 14s] Epoch 54 [7680/8158] loss=0.002575061353854835\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.4091466700188307\n",
            "[0m 14s] Epoch 55 [2560/8158] loss=0.0025124714244157075\n",
            "[0m 15s] Epoch 55 [5120/8158] loss=0.002667695446871221\n",
            "[0m 15s] Epoch 55 [7680/8158] loss=0.0026292468265940745\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.4150684403656975\n",
            "[0m 15s] Epoch 56 [2560/8158] loss=0.0029531442560255527\n",
            "[0m 15s] Epoch 56 [5120/8158] loss=0.0029191480600275098\n",
            "[0m 15s] Epoch 56 [7680/8158] loss=0.002853730639132361\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.3932521065255389\n",
            "[0m 15s] Epoch 57 [2560/8158] loss=0.0026126001961529256\n",
            "[0m 15s] Epoch 57 [5120/8158] loss=0.002698500920087099\n",
            "[0m 15s] Epoch 57 [7680/8158] loss=0.0027757410425692797\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.4072512865929654\n",
            "[0m 15s] Epoch 58 [2560/8158] loss=0.0030138575937598945\n",
            "[0m 15s] Epoch 58 [5120/8158] loss=0.00278093998786062\n",
            "[0m 15s] Epoch 58 [7680/8158] loss=0.0027247698977589606\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.4806905247252625\n",
            "[0m 15s] Epoch 59 [2560/8158] loss=0.0026685463497415187\n",
            "[0m 16s] Epoch 59 [5120/8158] loss=0.0026303687249310316\n",
            "[0m 16s] Epoch 59 [7680/8158] loss=0.0026272056391462685\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.4083456881211479\n",
            "[0m 16s] Epoch 60 [2560/8158] loss=0.0028652252862229944\n",
            "[0m 16s] Epoch 60 [5120/8158] loss=0.002874619932845235\n",
            "[0m 16s] Epoch 60 [7680/8158] loss=0.002854323744152983\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1757/2658 66.10%\n",
            "Dev set: AUC  0.453755613348098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubW1f8qJpo7h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "c43c703d-d146-4e62-e3dc-226cb5806a60"
      },
      "source": [
        "for i, LR in enumerate(LRs):\r\n",
        "\r\n",
        "  epoch = np.arange(1, len(acc_list) + 1, 1)\r\n",
        "  plt.plot(epoch, aucs[i], label = str(LR))\r\n",
        "  \r\n",
        "plt.title('LR')\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylabel('AUC')\r\n",
        "plt.legend()\r\n",
        "plt.grid()\r\n",
        "plt.show()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "KKhNx8SkgT0m",
        "outputId": "af0da393-1645-474a-c279-29863ce6a7a2"
      },
      "source": [
        "for i, LR in enumerate(LRs[:-1]):\r\n",
        "\r\n",
        "  epoch = np.arange(1, len(acc_list) + 1, 1)\r\n",
        "  plt.plot(epoch[10:60], aucs[i][10:60], label = str(LR))\r\n",
        "\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylabel('AUC')\r\n",
        "plt.title('LR')\r\n",
        "plt.legend()\r\n",
        "plt.grid()\r\n",
        "plt.show()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd5xVxfm4n3Nub3u33O2FbcDSQZoUlSL23qLfqFFjNEZM1MQYU40p9iRqNLZoNEZNbD8FuyBYAOl16Wzvd+vt7czvj7O7sLCNsizCeWA+5+y9c85955Z5Z973nXklIQQaGhoaGhr7Iw+2ABoaGhoaxyaagtDQ0NDQ6BZNQWhoaGhodIumIDQ0NDQ0ukVTEBoaGhoa3aIpCA0NDQ2NbtEUhIaGhoZGt2gKQkPjCCJJUqkkSafv99gsSZIUSZK8kiR5JEnaLknS9YMlo4ZGf9EUhIbG0aFaCGEH4oA7gOckSRo+yDJpaPSKpiA0NI4iQuUDoAkYO9jyaGj0hn6wBdDQOJGQJEkGzgNcwK5BFkdDo1c0BaGhcXTIkCSpBbCg/u7uFEKsG2SZNDR6RTMxaWgcHaqFEPGoPojHgTmDLI+GRp9oCkJD4ygihAgBdwNjJEm6aLDl0dDoDU1BaGgceQySJJk7CvuZcoUQYeBR4LeDIp2GRj+RtHwQGhpHDkmSSoEh+z38NZArhMjap54VKAeuF0IsOHoSamj0H01BaGhoaGh0i2Zi0tDQ0NDoFk1BaGhoaGh0i6YgNDQ0NDS6RVMQGhoaGhrdctyspHa5XCI3N7fXOj6fD5vNdnQEOsY4UduutfvEQmv3wbNmzRq3ECK5u+eOGwWRm5vL6tWre62zZMkSZs2adXQEOsY4UduutfvEQmv3wSNJUllPz2kmJg0NDQ2NbtEUhIaGhoZGt2gKQkNDQ0OjWzQFoaGhoaHRLZqC0NDQ0NDoFk1BaGhoaGh0i6YgNDQ0NDS65bhZB6GhcaKjBIMEt2whsHETJncD0XHj0CckDLZYGt9iNAWhofEtQQgB0SgiHEYJh1F8PoKbNxNYtx7/+nUEi7dCJAJAPLDzhRcxjxmDfeZMbKfMxDJmDJL++PrJi2iUYHEx/lWr8K9Zi81iRsyYgWQwDLZoxwXH17dF44QjtGsXrQsWYj1pArbp0w+pY4jU1eFdvBjP4s+J1tdjzMnBmJuLMXeIehwyBF1SEpIkDUAL1I4/1tREpLKSSFUVkepqItXVhKuqiFZXE61vQAmFEKEQdJO/RTKZsIwZQ9J112GZMB7LmDGsWLiQ4T4/vi+/xP3007ifego5Lo64s88mef6t6JO73VnhW0Fo5048S5bgX7WKwJq1KD4fAIbMTOxVVZRWVpL56F8wZmUe0v2FEETKyzEO2T/v04mHpiA0vpXEvF7cf3+SpldegWiURkCXkEDc2WcRd975WCaM77FDF0IQ2r4dz6JFeBd/TnDLFgAMQ3Iw5eYR2rULz5IlnaNxANnpxDJ+HNaTJmI9aQLmMWOQzeZ+yyuEIFpXR7i0lHBpGeGKciLlFYQrKoiUl6P4/V3qy04nhowMDEOGYJ08BcliRjIakY1GJKNJPbeYMQ0vwlw0/ADFGM3LI3nWLJLn30qspQXf8uV4lyyl5e23aV2wANcPbiTxuuuQLZZ+t+FwaHrpJcIVlaTe/fPDGt2Hdu1izyWXQiSCsaCAuAvOxzZlCtZJk9AnJ7P80UdJfO11Si6+mPQ//pG4M884eFn/9RL1Dz5IxiOP4Dzv3EOWtTcisQiNwUbSbGn9qh9TYnxY+iF1vjoMsgGDzoBBNmDUGTHIBsoD5cxi1hGXU1MQ3zJiMYVoWMFk6d9HF43E2Lmqjkjg2Moc2PbhhwS3bkNEImoJhzvP9UlJ2GbOxDp50gGdsBCCtvfeo+6RR4i5G4m//HJct95KcMtmWhcsoOWtt2l+9TUMmZnEnXM2tro6ar/8imhjIzG3m2hjI1G3G8XjAUnCMn48yT+9E8ecORjz8zuViohGidTUdHbooR078K9bS8PSL1RBDAYsI0diGT8O2RGHZDDsU/Sg0xGtrSNcUkKotIRwaRliHyUgGQwYsrIw5GRjnTQJY042hqxsDJmZGDIz0NntR+y91sXHE3f22cSdfTauH91C/SOP0vDY4zT/93+k3HE7ceefjyQPXLyKZ/Fi6u5/AIBIbQ2Zf/kLstF4SPeqf+RRZJOJ/I8+xJB54AwhNHEieVdcQdWdP6XqJz/B/39XkXL33cgmU7f3E0LQHGqm0lNJlbcK39IvGPHQu8jAyqf/wALrxxh1Rkw6EwbZgElnwmVxkWnPJMuRRZYjiwRTQr9ml/6In2XVy1hUvoillUvxhD3MzZnLTyf+lOy47B6vK24s5r7l97GlcUu3z6c3Cgpj6XD2zX3KcLAcNylHJ02aJL5tm/V5m4N89cYuskckMHJmRp9fspY6Px89u5m2xgCz/m84w6b0Pvpocwf46NnNNJR7kHQw8cxcJpyRg9E8eOMCIQQNjz1G49PPgF6PZDTu17kaiNbWIsJhJJMJ65Qp2E+ZiW3mKYhQkNo//JHA2rWYx44l7Te/xjJmTJf7x7w+PJ99StvC9/EtWwaKguxwoE9KQudKQp/kQp+UhHnkCOyzZqF3uQ5K/mhzM4F16wmsW4t/zVqCmzcjwuHuK0sShsxMjHl5GPNyMebmYsrLwzhkCPrUVCSd7hDfxb7p67vuX7WKugcfIrh5M+ZRo3DNvxXzyFHoU5J7nnlFo4R27yG4ZQvhinLiL7wQYy87KAshaN6zlbrvXEssI5mWU8eS9Px7NIzPYcktU3DHWmkKNuGP+okzxhFviu9SHEYHMREjHAsTVsJYN+xm3B/eZtuVk9l97lgkSUJCQv2v/qsoryA/Lx9dTFDw2gqyFq7Bl5vC9tvPpSZJpjnUTEuohZZgC82hZtwBN4FoAIBMt+BPL8doTNBTMjqJUz+r4693FVCTKBFRIoRjYYKxIJ6wp0s7LXoLWY4sUq2pJJoTu5QEcwItoRYWlS1iWfUygrEgTpOT07JOI9WayitbXyGqRLl6xNXcNPYm7Ma9AwNv2Mvf1/+d17a9RoIpgbun3M3s7NlElIhaYhECq1cTuOv3BK1mxi/6/JC+U5IkrRFCTOr2OU1BDA7VO5v56NnNBL0RhIDcsS7mXFOExdH9yGr3unoWvbQVWSfhTLZSX9pG0clpnHLlsG47/LLNjXz6whaEgBmXFbJm6TbaysHiMDDlvDxGzMxAp+s6aoxFFRrKPVTvbCEajhGXbCHOZcGZbMEap8qleL0ofn9nEYEAit+PpNdjnTq11y+oEIK6+++n+eV/E3/55aTd+7tu6yuBAP7Vq/F++SW+L78iXFLS+ZwuIYGUn96J85JL+hz1Kj4fXyxbxqx583qtd7iIWAwRje6dDUUiEI2iS0rqceQ60PTnuy4UhbaFC6n/y1+J1tYCIFut7f4XtehcSYR37SKwZQuhbdtVP0g7usREUp59ktoMMyWtJZS0llDaWkqtv5Z6fz2tbQ389kU/rja4+3odDfESc9cp3PyRwtYCI/+9Pg9HnAuL3kJbqI3WUCstoRZaQ61ERbSLrJIQ3P+vGA4//OJHNqIGGSEEArH3iEBRFBSUzusm7FK4daGCMQqvnmliw9Rk4s0JJJgTiDfFk2hOJMuRRbYST/KPH0UOhsl/439IBgM7Z80m6frrSPnZz7rI4o/4qfZWU+lVZx2VnkoqPZXUB+ppDjbTGGgkrISJ8wlu+FQhpUXw5A/SObXgdObmzGVi6kT0svqbbfA38Njax3h397skmhO5bcJtXFx4MYvKF/HgygdpCDRwxfAr+PFJPybOGNdFjtZ336X617/BmJVF9Q3Xc8rllx/09wQ0BdFJTz8aIcQRcUCGAlEaK72k5scd0Pnu+1qbllTx9Rs7iUu2cPbNY6jY2sSyd3ZhthqY870RDBmV1Fk/FlNY/s5uNnxWQUpuHGfdNBqb08iqD0pZ80EpcckWzrxxNMk5DgAURbDq/RJWf1BKUqads28ejTPZypIlSygaMoFlb+2iZlcr8alWTr4oH7PVQPWuFqp3tlC7p5VouP3HJQH7fDV0koIl1Ehi7ToK9rxLd++WecwY0n77mwNG9aB2orX33kvLG2+S+L1rqZn8XdrcQWzxRmxOk1riTdjijRgt+i6fR7iyEt9XXxFraSHhqqvQOZ39/kyOpUHBQBBVotT4aqjwVFDpqew8ltWVYXFYCMQCBKPBzgJgNVjVolePDsVEVokHa00rjto2nPU+EhtCJDZHkQUEjBKVGQZqsqzUZdtoyHGil3Rc/o9ijMEY91+hY0eWOoLPsGeQac/EZXEx95VtZH+xg5rffx/7qaeSZEkiyZyEeH8Rtb/+DdbJk8n+x1PI++UxEELgj/rxhD3oJB1GnZHQ+5/S+MvfkPHwQzjPP7/H92PJkiWcdtppKEJBEQpRESVUU0Pzr+8l+M0qHPPmkXbf77uE/4pIhPIf3ERgzRpyXn4J64QJAFTMn09g3XqGLvn8oPwmQgjcHy2k8b4/oXi8SNEYSfNvJWX+/B6v2eLewoOrHmRd/ToSzYk0BZsoSizityf/ljHJXX9PQgjcT/wd91NPYZ06lazHH+PLdesOZ7tvTUFA951FJBTjpV9+jSvTTvKQOFKGOEjOceBMthyU0qje2cynLxTjbQ5hjTMyYkY6I2dmEJe01wkYDcdY+up2tq2oJXesi9OvH9npS3BXevn0hS00VfsYOzuLaZcUEPJF+fj5zdTsamXMaZnMuGwoOsNexVO1Q33NgCfMtIsLGD41jU9fLKaiuImiaWmcdtVw9EZdl7YLISjd6Gb5O7tprm23iUvgyrKTURhPxtB40gqcULaL2g+WUL9iM942hYA1BV/GSJrkVE6f0EJahhHZakG2WpEtFkIlpdT/5VHVL3DFFSTf/pPOH6GIRKj+xT20vf8+Sbf8EC6+njf+vBq9QSYa2Tva6yBliIOzfzgGe0L/ncA9cTwpiGA0yLambWxyb2Jz9VbkZel8lfYOLaaGzjpG2UimIxMpKJGWlIZZZ8asN2PRWzDrzZ2drz/i7zwGogEiSgSr3orFYOlUHHZhwuEXeOIMBJUQoViIYCxIKBoirIQpDDo5+7GVmJv86B/6FbmnX4hJp86YWt58k5pf/4akW35Iyk9+ckBbWhcsoPruX2CZMIHsZ57u1eeiBIPsPucc9PEJ5L75Rq8zxx4HgYpC04svUv+3x9AnJpLx4APYTj4ZgNr77qP51ddIf+B+4i+6qPMaz+efU3nLj8h84nHi+jkLjbW0UPvHP9G2cCHmkSNJf+B+Gp9+Gs+ixeS/vxBjVlaP1woh+LjsY97c/ianZZ/GVUVXdc40Ot+LcJiaX/6KtoULcV5yCen3/g7JaDzcfBCagoDuvzwBT5iVC0qoL/fQWOklFlU7LJNVjyvbQd5YFyOmp2PswSkciymsWlDCmo/LcCZbOOnMIZRscFO2yY0AhoxKYtQpGSRl2jv9AVPOz2PS2blIclcFFA3HWP7ObjZ+XklCuo2gN0wkFGP2NUUMm9y9vyHojbD431sp2eBGb5BRhODU7wxj6FAdgVWr1VDAzZvwhCMkFRaid7nQJycjJ7qoCiQgR4MkhmuQ6quI1NSopboapa0NdDpsU6fiOOtMHPPmIWxxvHTP12QOS+Dsmw+cJcS8XtxP/J2mV15B53CQ/NM7cZ5/PlU//RneRYtI/umduH7wAz58ZhOV25q59s/TkXUS/tYQvpYQvpYwbY0B1nxUhtGk45wfjSVlSFw3re4/B/PDcVd6aa71UXhSygGfzWBR7a3mhc0vsLFhIzubd3aaXk6rvYwRJacQO7mGrBkWsh3ZZDuySbGmIEvyUVOMUbeb8u/fSHjPHjL+8ihx8+YRLC6m9MqrsE6aSPZzz/Vodmz78EOqfnYX5lGjyHriCQypKd3Wcz/3HA2P/oWcf/0L28lTe5Wnr3YHtmyh+md3ES4tJfGG6zGkplH35z+T+P0bSL3rri51RTTKrjlzMY0oIueZZ3p/IwDv0qXU/Po3RJubcd3yQ1w33YRkMBCprWX3OedimzaN7Cf/3ud9eiLa3Ezl/NsIrFlD8h13kHTTDzoHsZqC6IMj4YOIxRSaqn00lHmoL/dQV9KKu8KLwaxjxPR0xs7Oxpm8d0bQUu/n0xeKqS9tY8T0dGZeMbTTH+BpClL8VTXFX1fjb1WdmEazjtNvGEXe2N4do2VbGln00lbMVj1n3TSGxIzeUwkKIdi8uJytn+1glLIW0/rFRMrKAZDtdixjx9Lc0IA9Gt0bwbMfstOJIS0NQ3o6hox0TCNG4Dj99ANW4q74f7tZ83EZV983rct7sS/B7Tuo/cN9BFavQY6LQ2lrI/U3vybxu9+lscrL639YyaRzcpl6QX631zdWeVn45AaC3gjzbhhF/vhDj9nv7w9HiSm8+vtvaK0PkF7g5LTvDicp48BRrT/iZ3fLbnY072BH8w6qvFWMTR7L7OzZFMYX9jrrDEaDbGncQpotjUx73zH6m92bmb9oPv6on3HJ4xjjGsNo12iG2Ubw4R93Eg5EGT41jdOvH3nI7T4SxFpbqbjpZgKbN5N6zz00/etfiGiUvLffQp+Y2Ou1nkWLqPrZXchWK5mPPIxt2rQuz0ebm9k97wyskyaR/fQ/+pSlP+1WAgHqHniQlv/+FwD7aaeR9dST3Sqy+r/9jcZnn6Nw0WcY0tN7vGfdww/T9M8XMA0dSvoD92MZNarL8x1KLvvZZ7Cfemqf7difaEMDZddcS6S6mowH7ifunHO6PK8piD4YKCd1fVkbGxZXsGtVPUII8sYlM25uFq0NQb787w5kncSs7xZROLH70U8splC2sZGqnc2MPjWThLTeO3slGCRcUoJv5x5so0ZiLsjrU0YRjVJ5++14P1uEHBeHddIkrJMnY508GfOIIiSdrkvblWCQqLuRmLsB2W5Hn5aOzt6/fLa+lhAv/2oZo0/L5JQrhvUskxC0LVhA43PPk3jDDcRfrE7dP3l+M6WbGrn2z9Mx23q26/paQ3zw1Ebqyz1Mv6SQ8adnH5KfaN92+yN+yj3llLeV4w64aQu34Ql7aAu3IbY7SV95ErW5W3FV5SNH9TQV7aRt1B50RplILMLOlp2Ut5Uj2p0zVr2VVFsqJa2qEz3Tnsns7NnMyZnDhJQJBKNB1jesZ03dGtbWrWWTexMRJYJRNnLbhNu4ZuQ16OTuR9eLyxdz9xd3k2RJ4qm5T5Efv1eZrlywh1Xvl+JMsaDTy1z12wNH1UfbtKb4fFTMn49/+QowGMj998tYxo/v17WhXbuo/MnthEtKSP7xbSTddFOnGan2j3+i+bXXyH/vXUwFBX3e62Da7Vm0CM/ixaTec0+PJq5wRQW7552B68e3kfyjH3Vbp3Xh+1T/7GfEX3EFqb/+VbchvCIcZs+FFyGUGPkLFhxUmG+spYWya79HuKKCnOeexTrpwL5cUxB9MNBRTL6WEJuWVLL5yypCPnWanzE0ntOvH4kj8eAWTMVaWojW16ulro5QSQnh3XsI7d5NpLKyc7WsbLOR/ewzWCdO7PV+tb+7l5b//Y/Ue35BwtVXdzsSOpIdxqcvbqFkvZvvPTCj3+sxAJprfbz6+2846Ywcpl1c2Gf9SDjGon8Vs3ttA6NOyeCUK4d1cf4LRRAKRBGK6BL9FVEibG3cytq6tXy97WvCtjDlHlUp7I9Fb8Gpd3LGN7cgDDF2zvkEEZTJ3HQSKZVD8Vtb2DTiU1qSqyiML2RowlCGJQxjWMIwMu2ZyJJMg7+BpZVL+bzic1ZUryCshLEZbASiARShoJf0jHSNZGLKRMYlj+Pd3e/yecXnjEsexx9n/JFcZ24XmV4pfoWHVj3EaNdoHp/zOC7L3hln0Bfh379aRlZRIgnpVtZ+XM5Nj52K3tD1Mx8M34sSClH/8CNYxo3Def55B3etz0fN7+6lbeFCbKeeQsaDD6K0trL7vPOJv/RS0n9/b7/uMxDtLrv+eiLlFRR8+skB/o9wWRklF1+CqaiIIS+/1OtWJt6vvqbixhtJvv0nuH74w369dszro/z7NxAq3kr2M09jmz6923oDpSC0hXL9xBZv4uSLCph4Ti47V9aBBEXT0pH7sFWLaJS2Dz6g5Y03iVRXE62vV8Mg90EyGDDm5WEePQrnhRdiKshHn5pKza9+TfmNPyD7qScPmHp34H7i77T8738k3Xwzid/73hFrb2+Mn5vDjm/qKP6qmgnzcvp93ZoPy9DrZcbNzaGirYJKbyVt4Ta1hNo6zyUkkq3JpFhScJ3tYojDzpal1VTvbEFv1BHyRwj5o4QC0c5Iq8QRBjzj97A2vJy19Ws7Y9sdsoOhtqHMzJzJkLgh5DhyyInLIcWagsPowCAb2LqsmsWfb+OcH40lb2x7qOCFULGtiaX/2Y51zeUUTUtj1nlF6PQHOkiTrclcNuwyLht2Gf6In+XVy/m6+muSLElMTJ3IWNdYrAZrZ/05OXN4v+R97v/mfi5bcBk/nvBjvjviuwA8vPph/rP1P8zNmcv9p9yPRd/VjLdhcQXhYIzJ5+XSUhdAKIKmat9h+2qOBLLJRNqvf3Vo19psZDz8ENZJE6n7058pufRSDBkZyEYjyfNvPcKSHhzxl11G9U9/hn/Fii4dtBIOU3XHnWAwkPnIw33uc2WfOQPHvHm4n34G5wUXYMjI6LW+EgpROX8+wc1byHr8sR6Vw0CiKYiDxGDUMXJm7x8sqFPKlnffpfHZ54hUVGAsKMA6aSL6lBT0ySnqsb0Y0lK7/XIN+ffLlN/wfSpu/iFZTzyO/bTTujzf/PrruJ96Cuell5B8+4GRIgNFco6DjKHxbPy8gnFzspB7COndl5Z6P9tX1aKMcvN/i69gd+vuA+oYZANxxjgUodAcau7y3LDCyYyom0YkGCKsDxCKCxBJChIxBDGEzYzcMRP91kwSs0Zx4bR8JhWMY2LqRDZ/s7lPv9Oq90tJGeIgd0xSl+eyixK58jdTWPVBKWs/KkMoMPe6Eb2auqwGK3OHzGXukLk91pEkifPyz2Nq2lTuW34fD69+mEXli3AYHSytXMq1I6/lzol3HmB+CvoibFxUQf6EZFxZjs4INXel95hQEIeLJEkkXHkl5lGjqbr9dgKr1+D68W2Dvm+U4/TTkZ1OWt58s0sn3fDoowSLi8l68u99dvYdpN7zC7xffkndAw+S9fhjPdYTkQhVt9+Bf8UKMh56EMfcnr9PA4mmII4wSihEy5tv0vj8P4nW1GAePZrUX9yNffbsg97OQO9ykfPSv6i48QdUzL+NzEcfIe4MdW+Ztk8+ofb392GfNYv03/9+wDaS64nxp2fzwT82sXFlCbuS1rGufh0WvYU4YxxxxjgcRgdxpjgMsoFvar6h8SMjmWIkr5meYKRlKJcPv5zhCcOJM6n1nSYnZp25sx2RWAR3wE19oJ4Gf4O68CrUSkyAIkwowoAi7ChCQZZliqwGDOtT0X+tQ3pHwj47C/uZfa+X2LasBk9jkFOvHNbte6g36ph2UQEGo8w375VgT1BnkkeCZGsyj895nAV7FvDAygfwRXz8cuovuaroqm7rb1jUPns4NxcAp8uCwazDXe6BGUdEpGMCy5jR5L39Fm0ff4zzwgsHWxxkkwnnBRfQ8vrrRJub0Sck4Fn8OU0vvUzC1VcfVOdtyMjA9cObafjbY3i//hr7jAM/OKEoVN/zS7yff07qb3+D84ILjmRzDgpNQRwCSjBIaNs2dV+fBjdRt5too5uY241//XpiDW4sEyaQft/vsc2ceVidtz4hgZwXX6DippupuuNOxAMPoE9Nofpnd2EZN47Mv/7lqG/hXO+vZ7lxEUGbhbffWso7Y/5KojkRRSh4wh5iItalfkI4hStq7sE2LsKHV79HgrnvHAUGnYF0ezrp9p4jRw5gBEw5I8CqBSWs/6yc4q+qSZnQs48tFlVY/WEpqXlxDBmd1GM9gIln5+JpDrHmozLsiWZGn3poO4XujyRJXFBwAdMzptMYaGR44vBu6wV9ETYu3jt7AJBkCVeWHXel94jIciyhczpJuOKKwRajk/jLLqX53/+m7b33cJxxBjX33INp5AhSfn5X3xfvR+INN9DyzjvU/u5eHGeegWwyIZnMSCYjstmMf+1a2hYuJPnOO0n8v/8bgNb0H01BHCTh0lIqfngL4dLSLo/r4uPRJ7uwjB1H4jXXYJ065YiN6nVxceT883kqbvkR1T//OZLFgiE7m+yn/zEgu3HGlBiNwUbcATfugJvGQGPn38WNxayvX49AcFrORYzYOpvnx/2HKePGIEkSQgh8EV9ndJA/6qfpEyM75Houu2ImjoPYAfVQiEuyMPe6kYyfl8PSV7dTuayVDZkVjJt74GZoW5fV4G0KMfu7RX1+VpIkcdqVw/C1hPjite3Y4k19hisfDC6Lq4szen/2zh66RrW5shxsW16DUMQhr92o2tFM6UY30y/tPUz3RMY8fDjmsWNpfuMN2j79FBGJkHWImw7KRiPpv7+P6p//nOb/vIoIBg+ok3TTTbhu+sGREP2w0BTEQeBbuZKq234MkkTGww9jzMtDn+xCn5g44AlKZJuN7Geepur2Owjt2kXO88+hi48/7Pu2hdvY3rS9M65/e9N2drXsIhQLHVDXYXCQHZfNreNvZd6QeWSZc3j5l8toXikjjVc7FkmSsBvt2I120knH2xzi3yuWUTQt/aCivQ6XpEw7F9w+nlcfXMpXb+zE2xJi+sUFnZ1oLKKw5sNS0vKdZI/sPVa/A1knc+aNo/l/f1nLJ89t5qI7TyI1b+Bt/x2zh4IJybiyuoZjurLtRJbEaHUHiE+x9nCHnhFC8NUbO3FXeMkd4yJzuJaBrifiL7uU2t/+DoCMhx/qdaPCvrCdPJWhXywF1M9ARCKIYBAlGESSpEH3u3SgKYh+0vLW29Tcey/G9pG7Maf/0TtHCtliIevpf0AsdtBmpbASprixmF0tu9TSrB5rfDWddeJN8QxPGM4Vw69giGMILqurc2SbZE7CrD+wgx85M4P1n5bT5g4Q5zpwNrPuU9W5e9KZRz/5iiDG4WkAACAASURBVN6gI3u6hFynyuhvDTHn2hHo9DLFX1fjbQ4x53u9O533x2DSce6t43jrodUsfHIDl/584iF1zAdDx+xh0rkHrolJzlbNTe4K7yHJUbNLXQwKaoSUpiB6Ju6cc2l4/Akcc+b0uh/UwSJJEpLRCEYjurhjK9hAUxB9IBSFhr/+lcbnnsc2fRqZf/vboH6IkiRBP5SDP+JnVe0qvqr6ihU1KyhrK0NUqPZ4vawn35nP+JTxXJFwBcMShjE8YTgp1pSDNjGMnZ3Fhs8q2LikkhmXFtLmDtJQ7qGhwkNDuYeq7c0Mn5La46rrgUaSJU69chi2eBPfvLuHgCfMvBtGsebDUtILnWQdQodojTNy/m3jeeuhNSx4YgNzrikiY2j8gJhngr4IG3qYPQAkpFuRZQl3hafHxZq9sWFxBWabgeHT0tiwqILWhsCgfVbHOjq7jcLPPj2oRFHfdjQF0QuK30/13Xfj+fQz4r/zHdJ+/atjNtetEIJdLbv4uuprvqr+irV1a4koESx6C5PTJjNCGsHc8XMZGj+U7LhsDPKRaYc9wUzBxBQ2L6li27IaQn51EaEsSyRm2iians7U87vfUuNoIUkSk87OxeY08vkr2/nPvSsI+aLMu2HUIXfq8alWzr11LO8/uZH/95d1JKRZGXVKJsNPTut1hfjBEA5E+eAfG4mGYkw+r/sV9XqDjoR06yE5qtvcAUrWN3DSmUMYMyuLTYsr2fR5JTOvGHq4oh+3nEjKATQFQaylhYr58xHh9qxmHSUSUXMfBAKk/vIeEq655phz4IVjYVbVrmJJxRK+qPyCal81AEMThnL1iKuZkTmDCSkTMOrad3vMnTUgckw6Jxd/W5j4FAvJOepuuEkZ9i47zx4LjJiegcVh5ONnN5M5POGwzSlp+U6+d/90dq2pZ/MXVXz1xk6W/7/dDJ2UwuhTs0jJdRzydybojbDgifW4K7zM+/4okjJ73u3Ule2gcmvTQb/GxiWVSJLE6NOysMWbKJiYQvGyaqacn9fj5pQaJxbat0CnQ9LpkZ1WNaOZ0YhkNHRmOoubN29QVjB2R0SJUO2tZl39OpZWLGVZ9TL8UT9mnZlpGdO4aexNzMic0e88t0eKxHQbF90x4ai+5qGSO8bF1X+YhsF0ZLK56Y06iqalUzQtnYZyD1u+rGL7yjq2La8lY2g8J19UQHpB//NXgLoP1XuPrae1PsDZPxxDbh/RUq4sO9tX1OJvC3cmduqLcDDK1q+qKZiYgj1B3aJ73Jxsdq6qY+uymm6jvjROPE54BaFzOBjy0r8GW4xOfBEfdf466v31auKXtjJKW0spbSul0lPZud1ziiWFc/PPZVb2LKakTenWgazRPbb4gcnylpzjYNZ3i5h+SSFbl9ew5qMy3n54Dbljkph6YUG3PoT98TQFee+x9Xibg5w7fyzZRX1HWLk6HNWVHnJG9r6eo4Nty2sJB2OMm7NXEaTmxZGW72TjkkrGzM7qcxsZjeOfE15BHE18ER813hpq/bXU+Gqo8dZQ56+jzldHnb+OhkADvoivyzVG2UhOXA6F8YWcPuR0hsQNYXjCcIoS+47d1xgcjBY94+ZkM3JGBhs/r2DdJ+X8908rGToplakX5OFM7j7aqLXBz7t/XU/IH+GCH48nvbB/Ycwdisdd4e2XghCKYOPiCtLy4w4I0x07J4tPnt9C2SY3eeOOjVDLE5ZoGPxu8DWAz62WaBBEDISibuopFFBipNS5gVlHXARNQRxhIrEIFZ4KStpKuuTpLW0rpS3c1qWuTtKRbE0m1ZrK0IShzMicQYo1hRRrCqnWVDLsGaTb0pGlY8uWr9E/DCYdE8/KZdQpmaz7pJyNiyvYvaaetAInRoseo0WHyazHYNFjNOvY+HklSlRw4R0TDmpvJbPNgCPR3G9HddnmRlobAky98MDggYIJydgTTGxYXKEpiP4gBMTCascd9kPYCyFP+9GrHsNe9blIACIdR1/7MaBeGwlCNKAeI34INEGwtd9iZDmGAb894s3TFMRhEIqF2NG0gy2NWyhuLGZL4xb2tOzpknA9xZpCnjOPs/PO7uzw023ppNnScFlcB6QU1Dj+MNsMTLu4gLFzslj7cRnuCi+epiCRoLojbTgQQygCW7yJi+4c36tDuidc2XbcFQcmguqODYsrsCeYKJhwoAKQdTJjZmWx/J3duCu9/TKLdRAORmmu8eNMsRyxSK4BIRoGby146sBXr3bEwTa1Yw/tc97ReUeD+3TkAYiG2h9vP3IQKRMkHRhtYLCCwawe9SbQW8DsBHua+rg1CWzJYHO1H5PB6gKDBSRZLbKu/Vxiw9crOGUA3qoB7Z0kSToLeAzQAc8LIR7Y7/m/ArPb/7QCKUKI+PbnHgTObX/uD0KI/w6krN0hhKA11KqahNpNQ7U+1TxU0lrCruZdncogwZTASNdITs06lXxnPvnOfHKdudgM/UvEo3H8Y3Oauk2yJIQgGlGQdVKXfBcHgyvLTslGN5FQrFcHfGOVl8ptzUy7uKDHXXhHzsxg1cISNi6uYM61I7qt428LU1/WhrvSi7vCi7vSQ2tDAARYHAZmX110+DMQIUCJqiP0WEQtSkTtmP2N4Knd29F71TK2oQYqktSOeN9ONBpU63tq1Gt7Qm8Bk0MtHZ243gz2FPWoN+99rLOY2h+3gCkOTHYw2tuPDlUhGK3q/XQDozhj+oFZuzJgCkKSJB3wJDAPqARWSZL0nhCiuKOOEOKOferfBkxoPz8XOAkYD5iAJZIkfSiE6GqjOcKEYiG2uLewtn4t6+vXs75hPa2hrtM8vawn1ZrKkLghXDf6OkYljWJU0ijSbGmaT0DjkJAkCYPx8KKqXNkOENBY7SUtr+eoqQ2LK9Ab5F63rFcXzqWzbVkN0y4u6EzGJBRBxbYmNi2ponSTu3PgHOcy4Uo3MXyMEaczytqvAnzwj02MGCsxc7bAqFfaO/oQhH3txbv3POSBYAsEWtpH8y0QaIVQG/0bnUvqCNuRii4WUWcAQlFt9YqinuuNEJ8D2VPAkQ721PZjsjpyNzlVpaA/+L2VjmcGcgYxBdglhNgDIEnS68CFQHEP9a8Cftd+PhL4QggRBaKSJG0EzgL+d6SFbA428+LmF1lbv5bixmIiiprMJzculznZcyiMLyTdnk6aNY10ezqJ5kTNJ6AxeHTavEN7R9SxCC6HmiDJvWUnaWYjds8eqEkAJJAkQCLgF+xY0UDRWD3m2q/2mkw6bOP72M3HKhJbomew5YlHGJuymq31w9lcN4GWkAuLzsPEhGXkmNaSJLZhog0aUAtQIOlZabuStRsvpmpLPac7HyPduK2bxkjqSNtoA0s8mOPVTjtlhHpujlNH57IBdEZ19K1rP7cmqZ28PVVVDjq1K1t3mBnlhBAEIjF8oRgGnYTZoMOokw8poksIQTimEFMEJr0O3UHcI6YIIjGFcEwhElWIxNS/IzGFqCKIxgRRZe95aWus75seAgOWclSSpMuAs4QQN7b/fQ0wVQgxv5u6Q4AVQJYQIiZJ0hmoymIequlpJfCkEOLR/a67CbgJIDU1deLrr7/eq0xerxf7frlng0qQX1X+ikxjJvmmfPJN+eSZ8nDoHIfW8GOU7tp+InDE2i0UdLHgfiWALhZEElFkJYokYvuUKLISaS9RZCWMJNS/pfbt0KXO354ABLISa7+nWvTRwD6v0X6fffxbXcQT8M/6f1No/ppZzqe7rbPaexnfeL/LVa7bSNRXdn8fZKI6M1GdhY8a7qAhlI8QMlFhIslSSWHSWjKde5D0MjGdhaje2n60EJQstCoWopIRg14m6HFQuiWLUMBARkELGcPbCMhm3GET9RET9SEDjUFoDQtiCkQUQUxAVIGoIlAExJskkiwyLotEolnCZZFJMEuEYoLWkKAlJGgJqsfWkMATjCDp9MQExBTUoxD01M0pAgJRCEQF/qggGFWv2R+9DAYZjDoJnQSyROdRLRKKEIRjEIpBOCYIK+r9O9BJ6j0MOjDIEnpZlTEqVIUQ7Tw/KK8GALkOwb0zDu17Pnv27GM+5eiVwJtCqL8cIcQnkiRNBpahjkuWAweoSCHEs8CzoOak7mvk0FPe1rmxuRgGyDZ4rDAYOYq7RQjVXtywHQLNeyNAoqG954qijno7nHEdRcT2RoaE2tTzkEc1U4gYKB3hf3uL39OK1aRvN3HsHXGjxAAB+3bSQvTwWPv54TRbklFkI1HJQAw9AoEiJASgIKEIiKEnJFsI62xE9A5ixnQUg5WY3oY/pscfk/FGZdqiMp6IDk9EIibpUCQ9QtZjN0ZZH5vOEksC/lAYo8GAIgSKIpCiejJ8E/CbmviF/nsEhZGgMBAQRvzCQECYaI4ZCWAC1JFulk7mImFktz7G7vgYclIuKXFFpDhMJNmNtAUi1LQGqW0OUtMapDUQOaDdBmOMeUIHuxNYXe7kVWsI0TmQjqCTJRJtRkx6GaNOxqCTMRplTDoJSZIobw2ysi5ITOn7/Y+3GjAIGYfNjF4nodfLGHQSRp2Mbh/zr9jns5QkiSyTHodZj8Ns6DzaTToiMUEoqhCMxAhGY4QiCqFojGhMEFMEMSGIKoJYTD0adBIWgw6zUYfF0F6M6swh3H6fUFS9Ryiizgb0OlVGg05GL8sY9BIGWcaoV98Lg07qPNfL7fV0EnpZQi93nMvsKt4wIL/vgVQQVcC+yzGz2h/rjiuBLolnhRB/Av4EIEnSq8COAZAR4PhUDrEI1G2GytVQu4mhtfUQXdLugItrLw61I903SqMj5C7sbbcHt+61Cwdb1c5YibV3yNF2G29M7cDtKRCXCXEZ7SVTjcJorYKGbeDeoR4PInzvQCRVbqN9rzPRaANZ31WZtKfr9IpmrOlZ7WYKvWqekA0gy+w1v4BAwh+O4Q2r0/pwrGOKD+GoIBIT+CULPsz4hAmvYsYjTPiEkQgGFHREJR0x9MQkPVF01PoEpa1RGoMSMfb6GGQJbCY9NqMeq0mH3aTH2t6R+EIxfKEovlAUry+KLxwjpghsRh3xViPxVgMJDiNOqwGnxYBEx+hTIAsvcdUh6jJOx93oJiMtFZ0soZcgZaMXSQoTmlpIpmMEEpI68pUlJNSO0qiXMen3dljG9vzbBk+YRE+QurYQDZ4gO+s8uL0hnBYDaU4zWQlWJucmkh5vJi3O3LUdYbUtzXu8ZGzxcld2GlmnpJPhNJMRbyHFYULfh2M+pgjq2oJUtwSoaglQ0xrEatSR4jCTEmcixWEi2WHCpNcdOwOho0y06sjsDLA/A6kgVgFDJUnKQ1UMVwIHpEeSJKkISECdJXQ8pgPihRCNkiSNBcYCnwygrEcfIaBqDez8FBypkHsqJBV0dlgHEGyDkqWw6zNoLgNLgmqHtSaCJVE9lySoXqcqhZr17SF4gCWRlEgEaj9VO/X+IBva7cLOvSV+iBqNIevVKBFZvzdKRChqlEhbNZQtB09119eyuiC5CEZfqh5dw1SFojO1R4G0F51JvWfHIqB9iySBwdbeufdNIBxjxZIvOHnaNBRFNTUoijr6q24JsKPOw456LztqPeyo89AW7P29MellzAYdZoOsjhQNOox6ucfghOREI2fnW8hKsJKVsPcYbzX0O6BBCFXevjpRgG0ralj0r638ed4INmxbxaxZ6vYnu9fW89GizUy7pICTzjj626538NmLxexYWcvkswpIz+1/LhOdLJERbyEj3kK3dhCNAWPAFIQQIipJ0nzgY9Qw1xeEEFskSboPWC2EeK+96pXA66KrM8QAfNn+I2oDrm53WH+7URSoXAnF70Lxe9C2nx3YkQF5p0DuKeox5FEVws7PoGKF2uEaHeAaCi3larhesJUu5g+dCTLGw6TvQ9YktTiz+XrpUmaddppqygl52k00HrXT1Vu6hu4ZLO1K4DCishRFXQXqrVMdj7Yjl32tJ4KRGKtLm/l6t5tlu9xsqmpVbcCLF/V4jdNiYFiqnfPGZTAsxU52ohW7Sa+O8E16bCYdNqMei0E3KFtPSJKEXte/1+1IRdqwz3qIoC/CF6/vwJVtZ/wg76906pXDqNndwqcvFvOdX0/BpG0IeMwzoJ+QEOID4IP9Hvvtfn/f2811QdRIpm8/bTVQ8Q2UfglbF6px2zoTFM6Fub+BYWepS+hLv4CSL2DXIti435KP1DEwbT4MnQfZU7vGUisxNTww0KR2/q5hPYfqSVL74hyzGt43kMjtJif7weUoCEcVWvzhdnOHOkLfN/ojGInR5AvT6A3T6AvR6A1T1RJgxZ5GVpc1E44q6GWJ8dnx3Dq7kOaacoYPH4ZOUu22siyhk8FlNzE81UGyw3TchCcnpFuR9ZKaAKh9C6flb+8i4I1w3vxxPa57OFoYLXrm3TCKtx9ZyxevbWfeDaN6rOtrCbFjZR2jTsk47J1lfS0hIqEY8akDm9jpeERT4UcSJaba/StWQvkK9dharj6nt8DQ02HkRTD0DDWErwNLPLgKYdINqmmlYRuUfqWO5AvmQlx6z68p68CWpJZvMb5QlP98U8ZzX5bQ4Oma7tSgkzDpdWq+63D34Xwj0uO49uQhzCh0MTkvEbtJ/WovWVLDrJMHz6xyNNHpZJIy7LgrvcQlQuX2Zoq/rmHCGTkk5xwbUXlp+U4mn5vLygUl5IxKYvjUA3ce3rm6jqWvbSfki1K1o5lzfjT2kGdvJRvdfPZiMXqjzHUPzDhuBgNHC01B9IQQUL8VEJBYoI66uyPYBrsXw46PYecnqlkFVLNK9lQ4+RbImarOAvqzCEeS1DjwlO5XsB5vtAYivLSslBe+LqHFH2FGYRK3zSkkGlNjyDsiR0JRBSEgyW4kyWYk0WYkyW4iyWbE5TB1KoQTHVeWndJNbuwjBUte2UZcsqXHZEODxcSzhlBR3MTS17aTXuDsTFXbYQ7buaqOlNw4cmYlsvr9Upa9vYuZlx1cEiMlpvDNghLWflSG0azD3xqmtT6gzSIOEu1XtS9KTDUHbV0I2xZCS1n7ExIkDAHXcNX+nzxctd/v+AjKlqm+AXO8agIqnAdDpoEz+/Bs+N9y/OEo/15exivflGHW68hJtJLdURIspDstfLi5hpeXl+ENRZlblMKtcwo5KUfLiXw4uLLtbF1WQ81qaG0IcMHt4w97lfaRRtbJnH79SP77x5V8+sIWLv7pSVRtb2HRy1sJtIWZcn4eE88agqyTCfmibPisgsR0GyNn9Lz6e1/8bWE++ecWqrY3M3JGOqNOzeSN+1dTW9KqKYiDRFMQ0TDsWQLbFsD2D9WtdXVGyJ8Fp9yphlE27FBDNN071LqxdhNI8gjVNzDsLMia3Lma80QmEI7xn2/KeHrpbtzeMNMLkrCb9FQ0q36CfU1EkgTnjEnn1lmFjMw4tpK1f1vpyA3RUgpF09P7lU9iMIhzWTjtu8P59J/FvPXwWupL20hIs3LOLRO77GQ78/JCWup8LH11O/EpFjKG9j6A8LsF//vzKoK+CHOuLWLE9AyEIjBa9NTubqXo5F7MtRoHoPVovgZ49XI1OmjoPBhxnjoLMPfQYSkxNYJI1kO8lnWrg2Akxmsry3lqyW4aPCFmFrq4Y95QJg7Z20EJIWj2Ryhv8lPVHKAo3UFB8om3unsgcbXvBKszwYxLCwdZmt4ZNjmN8s1NbP+mlnFzsjn5onz0+812ZJ3MGTeO5q2H1vDhM5u5/BeTOk1S+xLwhtnyRRUliwRxSRKX/nwiye3KUpIl0vLiqN0zoFu5HZdoCsKZCdd/CJkT1Tj8vpB1kHhs2XSPFkIIKpoCVLcGqG0N7j22BNlY2UK9J8TUvET+ftUEpuYf6DSXJHXVbKLNyPjs/sfBa/Qfo0XP5PPyqPeVHttbbrcz59oippyf122n34HZZuDcH43lzQdX8/5TG7n0rokYLXqEEFTtaKH4yyp2r29AiQocWXDFnZMxWbu2PTXfyar3SwgHolq+7YNAe6cAhhwbOaePZYKRGPNfXcdnW+u6PO60GEh3mhmfHc9103OZVpCkRYoMMlPOy2PJkrK+Kx4DyDq5V+XQQXyqlbNuGs17j2/gk39uIWNoPMVfVdPaEMBk1TP6lExGzsxg087VBygHgPR8JwioK2kje+SxaXY7FtEUhEafeIIRbnxpNStLm7jj9GFMyk0gzaluq2DTooc0jhJZRYmceuUwlr66nbLNjaQXOpl8Xh4FE5L3mqZ2dn9tal4cSFBb0qopiINA+3Vr9EqTL8x1L66kuLqNv31nPBeOzxxskTROYEafmok9wYQz2UJCWv+TcRktepIybNTu7nsfsFhUYePnlYyaefiL9L7tnNit1+iV2tYgV//zGyqa/Dx77UTmFKUOtkgaGuSOObRtW9LynexcXY9QBFIvC+/2rGtg2Vu7EIrgpDNPjEWWPaFlvtHollK3j8ueXkZta5CXbpiiKQeNbz1p+U7CgShNtb5e6+1eWw/AthW1DFS+nG8LmoLQ6IIQgnXlzVz+zHJ8oSiv/mAqJ3cTkaSh8W0jLV9NxdqbmSkSilG2uRFLnJHmGp+6r9UJjKYgNBBCsLmqlUc+3s7cvyzl4qeWIUvwv5unMTZLC0fVOD5wplgw2w3UlvS8HqJscyPRiMKsq4Yj6yW2rag5ihIee2g+iBOYbbVtvLO2ig8311Le5EcnS0zNS+T6GXmcOyadRJuWwF3j+EGSJNLynb3OIHatqcfiMJA7zkXeWBc7V9Ux/dJCdIO8E+5goSmIE5TVpU1c9dwKAGYUurh1dgHzRqZpSkHjuCYtP47SjW6C3ghme9f1EpFwjLLNboafnI4sSww/OZ3daxuo2NJE7tiBz2cCEPRGePW+bzjz+6PIHD74+5JpCuIEpKolwA9fWUNmvIU3b5mOy96PFeQaGscBnX6IktYDoqHKNzcSDSsUnqTmSskZlYjZbmDbitqjpiCaanwE2sKUbnIfEwrixJw3ncAEwjFuenk1oYjC89+bpCkHjROKlNw4JFmids+BZqbda+sx2w1kDFX9bjqdzLDJqeqMwxc5KvJ5m9U0wXWlx8a+UZqCOIEQQnDXmxsormnj8asmUJhybCSR0dA4WhiMOlxZ9gMURDQco2RTI/kTkrtk3ht+chqxqMKuNfVHRT5vs7pTdEOZh1hMOSqv2RuagjiBeGrJbhZurOHus4qYXXRwqUA1NI4X0gqc1JW0oezTAZcXNxENxSic0PV3kZzjICHdxvYVtUdFNm+TOoOIRhSaqnpfr3E00BTECcK6+igPf7ydi8ZncPOp+YMtjobGoJGe7yQaVmjcpwPetaYes81AxvCuYd2SJFF0chq1e1ppqfcPuGye5hAmq+oaPhbMTJqCOAHYUefhmQ0hxmY5eeDSsdpuqxonNKn5aq6XDjNTNBKjdJObvPGubsNZh01JAwm2fzPwswhvc5C0fCdmu4G6kr73jRpoNAVxnOMNRbnxpdWY9BLPXjMJs+HYSj+poXG0cSSasTmNnQqioriJSDBG4Undm13tCSayixLYvqIWoQzs1hve5hD2BBOpuXHU9bKg72ihKYjjnPc3VlPe5OfmsSbSnObBFkdDY9DpXDDXriB2r23AZNWTWdRzWOnwk9PxNAap6cdusIdKNBwj6I1gTzCTmhdHc52fUCA6YK/XHzQFcZzz1toqCpJtjEjUPmoNjQ7SCpy0uYO0NQYo2dBA3vjkXldL549PRm/SsX0At97oiGCyJ6ozCATUD7IfQus1jmMqmvysLGnikpOyNL+DhsY+dCyYW/1BKeFezEsdGEw6Cicks2tNPdFwbEBk6lgDYU8wk5Kr+kkG28ykKYjjmHfWVQFw0QQtyY+Gxr4kZzuQ9RJbl9VgsurJ6sW81MHwk9MIB2OUbHAPiEyepvYZRIIJs81AfKp10COZNAVxnCKE4O21lUzLTyIzvu+cvxoaJxI6g0xKjmrGyRvrQqfvuyvMHJaAyaqnemfLgMi0dwah7m6gOqpbBzUnhaYgjlPWlrdQ2ujnkpO02YOGRnektYe7FvRhXupAkiUS0mw01QzMAjZvcwiLw4C+PdIwNS+OgCeCp33x3GCgbdZ3nPL22krMBpmzx6QPtigaGsckRdPSCfoiZI9M7Pc1CelWSjcOjInJ2xzEnrA30jA1b68fIi5pcKwA2gziOCQUjbFgQzVnjUrDbtLGABoa3ZGUaWfu90b2y7zUQWK6jYAnQsAb7lf93evq+fzfW/tVt2MNxL7y6fTyoPohNAVxHLJ4az1twSiXnJQ12KJoaBxXJKTbAGiu6d+2G9tX1FL8dQ2RUN+RT96mIPbEvTMInV4mOcdO/SBGMmkK4jjkrbVVpDhMzCg8OnvYa2icKCS2K4j++iEaq9Sc1i11vSuUcCBKOBjrMoMA+P/t3Xt41dWV8PHvIldyIRcuIRAkwSA3gVBTLjPWAa0VaQ3SsTY6taPiMIPQqbx21I4jpdTO2NHRzlSnFce+b9unmlKrgIqAF9LaUcFSglwCgnghIRcSCEkg96z3j3MSTsLJSQLnl3Nyzvo8Dw85+/zOL2trHlb23r+9dlpmEpWfBa6yqyWIEFNd30ThoUqWzBpLxBDb+2CMPyWkxBAZE8Gp8t4TRHNDK7VVrgXm3q6vcz/BlJjStdpBWtYw2gJY2dUSRIh5ec9xWtvVppeMcYCIkDo6jlN9GEF0jB4ATpX7HkF07qLuPoLoXKgOTOE+SxAh5sXdpUwbM4xJo+0wIGOckJIez8k+rEF0JIjo2IjeE4T7UVbPNQiAxOGxDE2MCtiOakcThIgsFJFDInJERB7w8v4TIlLk/vOhiNR4vPfvIrJfRIpF5L/EakX06nBFHR+UnLbRgzEOShkdx5mapl4L6VWVniEmLpIxE5OpqfA94qg/1YQIxCdFd2kXEdeGuQA9yeRYghCRCOAp4HpgKnCLiEz1vEZVV6lqjqrmAD8BXnR/9i+AvwRmAJcDI8IDaQAAIABJREFUnwf+yqlYQ8WLu0uJGCLkzRwT6FCMCVkdC9W9rStUl9QxfGwCKaPjqalooN1HqfD6U43EJcV0Oe60Q1rWME6Vn6XprPdzsRvqm2k548xuaydHELOBI6p6VFWbgQJgsY/rbwGed3+tQCwQDcQAUUCFg7EOem3tyobdpVw1cQQjE2N6/4Ax5oKce9S15wSh7Up16RmGZySQPDqOttZ26qp73hHdfQ+Ep7RMV2HByk/qznvvzOkmNjy+m0//oD4T0IVychfVWOCYx+sSYI63C0VkPJAFvAWgqu+KyHagDBDgSVU9b7eJiCwDlgGkpaVRWFjoM6D6+vperxmsDlS3UXa6kRsz1WsfQ7nvvli/w8tA9FvbFRkCe3YepKL5Q6/XNNcpLU1KdX0p9e4K4X94/T0Sx3ifKa8sbSc2Ga+xtzW7/uF/b/sePqo89/mWs8on25XWBhiZ28gf/vD7i+uYF8GyzTYfeEFV2wBEJBuYAnRMpr8uIl9Q1bc9P6Sq64B1ALm5uTp//nyf36SwsJDerhmsXvntHhJiyvn2TQu8nhoXyn33xfodXgaq3xX/u5OEqBjmz5/p9f2ju09wmL38xTVXkDRiKM++8TYZIy9l1vxLzrtWVTn0u9+TNWksV86f6P37vfMecQzt/H61VQ1s/PFutKWFG++ZyaGSIkf67eQUUykwzuN1hrvNm3zOTS8BLAHeU9V6Va0HXgPmORJlCGhsaWPLvnKuv3y0HSlqzABITff9qGtVaT0IpI6JJzYhiqGJUdT0sGbRdKaV1pb28/ZAeOpYqFZVairP8tJ//Jmms60s/vYs0rOTL7o/PXEyQbwPTBSRLBGJxpUENnW/SEQmAynAux7NnwF/JSKRIhKFa4G6bwVNwtAbxRXUN7XauQ/GDJCU9Hhqqxt7LKFRXVJP8qg4oqJdv7Alp8Vxqofd1HXdynx701HZ9bP9J3npP/5Ma0s7i1fN6twn4RTHEoSqtgIrga24/nFfr6r7RWStiOR5XJoPFGjXoucvAB8Be4E9wB5VfdmpWAe7DbuPkzYshrkThgc6FGPCQmp6PGjPJTSqSusZPjah83VKenyPeyHObZLzMYLIci1Uv/rUHlC48f/MYuQ45/c6OboGoaqbgc3d2lZ3e73Gy+fagL93MrZQcepMM4WHKrnjLzOttIYxAyRl9LmaTCMv6foPdXNjK7UnGpgyb/S569PiaKx3VYEdmtB1r8O5TXI9jyBSx8YTFRNBTFwki++ZRXJanL+64lOwLFKbC/Tq3jJa29Wml4wZQEmjhjJkiHhdhzh53NXWZQQxumPvxFmGZndLEKeaGBIhxCV2bfcUETGEr/7TFcQNiyZuWM/X+ZuV2hjkNhaVMnFUAlPTnZ2LNMacExE5hKRRQ71Wda0qcZXYGJ7hmSBcv/HXeJlmqj/VSHxyDNLLDMCIjIQBTQ5gCWJQO3byLO9/coobZ43FKpEYM7B6WleoLq0nOjaCRI+6SgmpsUREDfG6+9rXJrlAswQxiG3acxyAxTlWWsOYgZaaHs/pEw20tXQ9q6G6pJ7hGQldfmkbMkRIHuX9SabuR40GE0sQg5Sq8tLuUmZnppKRMjALVsaYc1LS49B2176EDqpKVWk9IzzWHzyv7z7i0Hal/lQTiT4WqAPJEsQgtf94LUcq61k8y0YPxgTCuaJ95/7Rr6tupKWxrcv6Q4eUtDjqqhpobTm3d+JsXTPtbWojCONfG3aXEhUhfHl6eqBDMSYsJY+KA+l6/GjnArW3EcToeFThdGVDZ1tPBwUFC0sQg1Bbu7Jpz3HmTxpFctzAPtVgjHGJjI5g2IihXR51rfYosdFdsvtJJs8RR33nLmobQRg/efejairrmlhiex+MCajU9PguI4jqknqSRgwlOvb8LWYdm9s8n2SqP+keQdgahPGXDUWlJMZEcvXkUYEOxZiwljI6jprKs7S3uZ5kqiqtZ4SX9QeAqGjXo6/dRxARUUOIjY8akHj7yxLEINNZuXW6VW41JtBS0+Npb1Vqq1yF+06faPC6QN0hJT2uS/2mjj0QwbqPqccEISLXichNXtpvEpFrnQ3L9KSzcmuOTS8ZE2gdp8udLDtD9fF6UO8L1J3Xp8VzqvwM6j79LZj3QIDvEcRqwNsRRYXAWkeiMb169YMyRiXGMMcqtxoTcCmjz60rVLufYOppiglcC9Wtze3U17jWHupPNZEYpE8wge8EEaOqJ7o3qmoVcP4SvXFcU2sbf/jwBNdOTbPKrcYEgejYSBJSYlwjiJJ6orqV2OjOM6G0t7VzpqaJBB/XB5qvaq7DRCTSfa5DJ/cBPkOdDct4897Rk5xpbuOLU9ICHYoxxi01PZ5TZWeJjB7C8DEJPovueVZ17dgXEax7IMD3COJF4BkR6RwtiEgC8DP3e2aAvVlcwdCoCOZdatNLxgSLlPR4TpWdobr0jM/pJYChiVHExEVSU362TwcFBZqvBPEvQAXwqYjsEpE/Ax8DJ9zvmQGkqrxZXMmVE0fY00vGBJGU0XG0trTT3NDq8wkmABFxHz96xmOTXPCOIHqcYnJPLT0gIt8Hst3NR1S1oafPGOcUl9VRWtPAt6+ZGOhQjDEeOmoyge8nmDqkpMfz2f5qj01ywTuC6DFBiMhXuzUpkCwiRapa52xYprs3iysQgQW2Oc6YoJLSJUH0/vxOSlocB98p42SZa1E7ZmjwHuzpK7IbvLSlAjNEZKmqvuVQTMaLNw5WMjMjmZGJwTscNSYcxcZHETcsmsjoIV5LbHTX8STTseJTQb3+AL6nmO7w1i4i44H1wByngjJdVdY2sudYDd/50mWBDsUY48WlnxtFVEzfClN0PMl0pqapTyOOQOr32EZVP3U/6moGyFsHKwH44lR7vNWYYHRVft9/eUscEcuQCAnqcyA69LsWk4hMBpociMX04I3iSsYmD2VSWmKgQzHGXKSIiCEkjXRtJQvmJ5jA9yL1y7gWpj2lAunAN5wMypzT2NLGH4+c4Ou544K2oJcxpn9S0uM5VX426EcQvqaYHuv2WoGTuJLEN4B3nQrKnPO/R6pobGnnGts9bUzISHGfDRGs50B08LVI3VmoT0RmAbcCX8O1We53zodmwDW9lBATyZwJqYEOxRjjJyPHJyLiPrY0iPmaYroMuMX9pwr4DSCqumCAYgt77e3KWwcruOqyEcRE2u5pY0LFhJyR/M3aeT4L+wUDX1NMB4G3ga+o6hEAEVk1IFEZAPYdP01FbRPXTLbpJWNCiYh0LlQHM19PMX0VKAO2i8gzInINYKukA+iN4kqG2O5pY0yA9JggVHWDquYDk4HtwD3AKBH5qYh8aaACDGdvFldwxfgUUuOjAx2KMSYM9boPQlXPqOpzqnoDkAHsBu53PLIwd7ymgf3Ha+3pJWNMwPRro5yqnlLVdap6jVMBGZc3O3ZPT7HpJWNMYPR7J7UZGNv2l5M5PI5LR/ZePtgYY5xgCSIIvfNRFW8fruJrtnvaGBNAliCCTGtbO2tfPkBGylCWXpkV6HCMMWHM0QQhIgtF5JCIHBGRB7y8/4SIFLn/fCgiNe72BR7tRSLSKCI3OhlrsCh4/xgHy+t4cNEUO1rUGBNQjh1lJCIRwFPAtUAJ8L6IbFLVAx3XqOoqj+u/Bcxyt28HctztqcARYJtTsQaL02db+I9th5g7IZWFl48OdDjGmDDn5AhiNq4zrI+qajNQACz2cf0twPNe2m8CXlPVsw7EGFSeeONDTje08L0bptnagzEm4ES1e0VvP91Y5CZgoare5X59GzBHVVd6uXY88B6Qoapt3d57C3hcVV/x8rllwDKAtLS0KwoKCnzGVF9fT0JCcD4VVFrXzkPvNDA/I5JvTvN/hcdg7ruTrN/hxfrdfwsWLNilqrne3guW07LzgRe8JId0YDqw1duHVHUdsA4gNzdX58+f7/ObFBYW0ts1gaCqfPPnO0mIaeGx2xc4snM6WPvuNOt3eLF++5eTU0ylwDiP1xnuNm/y8T69dDPwkqq2+Dm2oPL6gQrePlzFqmsvs7Iaxpig4WSCeB+YKCJZIhKNKwls6n6R+wjTFLwfQNTTukTIaGpt4+FXi5k4KoFvzB0f6HCMMaaTYwlCVVuBlbimh4qB9aq6X0TWikiex6X5QIF2WwwRkUxcI5DfE8Ke/ePHfHbyLKtvmEpUhG1LMcYED0fXIFR1M7C5W9vqbq/X9PDZT4CxTsUWDCpqG3nqrSN8cUoaX5g4MtDhGGNMF/YrawD96LWDtLQp//LlKYEOxRhjzhMsTzGFnV2fnuTF3aWsWHApmSPiAx2OMSGlpaWFkpISGhsbAx3KgEhKSqK4uNjnNbGxsWRkZBAVFdXn+1qCCIC2duV7m/Yzelgsd8/PDnQ4xoSckpISEhMTyczMDItNp3V1dSQmJvb4vqpSXV1NSUkJWVl9r/FmU0wBsP5Px9hXWst3F00mPsZytDH+1tjYyPDhw8MiOfSFiDB8+PB+j6gsQQyw02dbeHTrIWZnppI3c0ygwzEmZFly6OpC/ntYghhgT7zxITVnm/le3lT7ATbGBDVLEAPoUHkdv3rvU26dcwnTxiQFOhxjjIMyMzOZPn06OTk55OZ6LXXUo127djF9+nSys7P5x3/8Rzq2ia1Zs4axY8eSk5NDTk4Omzdv7uVOF8cmwAeIqrJm034SYyO599pJgQ7HGDMAtm/fzogRI/r9ueXLl/PMM88wZ84cFi1axJYtW7j++usBWLVqFd/5znf8HapXliAGyGv7ynn3aDU/uPFyUqzekjED5vsv7+fA8Vq/3nPqmGF874Zp/f7cRx99xIoVKzhx4gRxcXE888wzTJ48ucs1ZWVl1NbWMnfuXAC++c1vsmHDhs4EMZBsimkANDS38cNXi5mSPoxbZ18S6HCMMQNARPjSl77EFVdcwbp16wBYtmwZP/nJT9i1axePPfYYd99993mfKy0tJSMjo/N1RkYGpaXn6pw++eSTzJgxgzvvvJNTp0452gcbQQyAl3aXUlrTwKNfm0HEEFuYNmYgXchv+v7wxz/+kbFjx1JZWcm1117L5MmTeeedd/ja177WeU1TU1O/7rl8+XIeeughRISHHnqIe++9l5///Of+Dr2TJYgB8NzOT5k8OpF5E4YHOhRjzAAZO9ZVSm7UqFEsWbKEwsJCkpOTKSoq6nJdW1sbV1xxBQB5eXksX76ckpKSzvdLSko675WWltbZ/nd/93d85StfcbQPNsXksA9KathXWsutcy6xx1qNCRNnzpyhrq6u8+tt27Yxe/ZssrKy+O1vfwu4HlzZs2cPERERFBUVUVRUxNq1a0lPT2fYsGG89957qCq//OUvWbzYdVpzWVlZ5/d46aWXuPzyyx3th40gHPb8zs8YGhXBjbNCujCtMcZDRUUFS5YsAaC1tZVbb72VhQsXMmnSJJYvX87DDz9MS0sL+fn5zJw587zP//d//ze33347DQ0NXH/99Z0L1Pfddx9FRUWICJmZmTz99NOO9sMShIPqGlvYWHScG2amMyy27wWyjDGD24QJE9izZ8957VlZWWzZsqXXz+fm5rJv377z2n/1q1/5Jb6+sikmB20sOs7Z5jZusSeXjDGDkCUIh6gqz+34jCnpw8gZlxzocIwxpt8sQThkT8lpDpTZ4rQxZvCyBOGQ53Z8Slx0BDfmWMVWY8zgZAnCAbWNLby8p4y8mWNItMVpY8wgZQnCARt3l9LQYovTxpjBzRKEn6kqv97xGdPGDGNGhpX0NiZcbdmyhUmTJpGdnc0jjzxy3vtNTU18/etfJzs7mzlz5vDJJ590vvdv//ZvZGdnM2nSJLZu3drrPZ9++mmys7MREaqqqvzWB0sQfrb7WA0Hy+tscdqYMNbW1saKFSt47bXXOHDgAM8//zwHDhzocs2zzz5LSkoKR44cYdWqVdx///0AHDhwgIKCAvbv38+WLVu4++67aWtr83nPuXPn8sYbbzB+/Hi/9sM2yvnZczs+Iz46gsU5tnPamKDw2gNQvte/9xw9Ha4/f1TQYefOnWRnZzNhwgQA8vPz2bhxI1OnTu28ZuPGjaxZswaAm266iZUrV6KqbNy4kfz8fGJiYsjKyiI7O5udO3cC9HjPmTNnkpiY6N8+YiMIvzrd0MIrHxwnL2csCTGWe40JV6WlpYwbN67zdfeS3d2viYyMJCkpierq6h4/25d7+pv9K+ZHm4pKaWxptzMfjAkmPn7TN77ZCMKPXv6gjImjEphui9PGhLWxY8dy7NixzteeJbu9XdPa2srp06cZPnx4j5/tyz39zRKEn1TWNvL+JydZND090KEYYwLs85//PIcPH+bjjz+mubmZgoIC8vLyulyTl5fHL37xCwBeeOEFrr76akSEvLw8CgoKaGpq4uOPP+bw4cPMnj27T/f0N0sQfrJlfzmq8OUZliCMCXeRkZE8+eSTXHfddUyZMoWbb76ZadOmsXr1ajZt2gTA0qVLqa6uJjs7m8cff7zzsdVp06Zx8803M3XqVBYuXMhTTz1FREREj/cE+OlPf0pGRgYlJSXMmDGDu+66yz/98MtdDK9+UEb2qAQuS/P/kwTGmMFn0aJFLFq0qEvb2rVrO7+OjY3tPDyouwcffJAHH3ywT/cE11Gk991330VGfD4bQfhBZV0jO216yRgTYixB+MHWfe7pJUsQxpgQYgnCD17dW8alI+O5LC0h0KEYY4zfWIK4SCfqmtj58Um+PD3dSmsYY0KKowlCRBaKyCEROSIiD3h5/wkRKXL/+VBEajzeu0REtolIsYgcEJFMJ2O9UFv2l9OusMieXjLGhBjHnmISkQjgKeBaoAR4X0Q2qWpnxSpVXeVx/beAWR63+CXwQ1V9XUQSgHanYr0Ymz8oY8LIeCbZ00vGmBDj5AhiNnBEVY+qajNQACz2cf0twPMAIjIViFTV1wFUtV5VzzoY6wU5UdfEjo+rbXrJGHOegSz3/Q//8A9kZWWRk5NDTk4ORUVFfumDkwliLHDM43WJu+08IjIeyALecjddBtSIyIsisltEHnWPSIJK5/SSPb1kjPEw0OW+AR599FGKioooKioiJyfHL/0Ilo1y+cALqtrmfh0JfAHXlNNnwG+A24FnPT8kIsuAZQBpaWkUFhb6/Cb19fW9XtMfz+1sYHScUH5wFxWHgnsE4e++DxbW7/DS0e+kpCTq6uoA+PGeH3P49GG/fp+JSRO5Z+Y9Pb6/Y8cOMjMzGTlyJE1NTSxZsoT169dz7733dl7zu9/9ju9+97vU1dVx3XXXsWLFCmpra1m/fj1LliyhubmZESNGkJmZyfbt2wF6vKeq0tDQ0NnnnjQ2Nvbr58LJBFEKjPN4neFu8yYfWOHxugQoUtWjACKyAZhLtwShquuAdQC5ubk6f/58nwEVFhbS2zV9VVXfxKGtb3D3/GwWLJjkl3s6yZ99H0ys3+Glo9/FxcWd5yNER0cTEeHfCYjo6Gif5y/U1NSQlZXVec2ll17Kjh07unymoqKCyZMnd7YlJyfT3NxMVVUVc+fO7WzPzMykpsb1/E5P9xQRHn74YR599FGuueYaHnnkEWJiYs6LKzY2llmzZp3X3hMnE8T7wEQRycKVGPKBW7tfJCKTgRTg3W6fTRaRkap6Arga+JODsfbbln2u6SWrvWRMcLt/9v2BDsFxa9asITs7m+bmZpYtW8aPfvQjVq9efdH3dWwNQlVbgZXAVqAYWK+q+0VkrYh4liDMBwpUVT0+2wZ8B3hTRPYCAjzjVKwXYvPeMiaMiGfyaHt6yRjT1UCX+x49ejQiQkxMDHfccUfnCXQXy9F9EKq6WVUvU9VLVfWH7rbVqrrJ45o1qnreHglVfV1VZ6jqdFW93f0kVFCoqm/ivaPVLLKnl4wxXgx0ue/y8nIAVJUNGzZw+eWX+6UfwbJIPahstaeXjDE+eJbmbmtr48477+ws952bm0teXh5Lly7ltttuIzs7m9TUVAoKCoCu5b4jIyM7y30DXu8JcNddd3Hy5ElUlZycHH72s5/5px9+uUuY2by3jKwR8UxJt+klY4x3A1nu+5VXXvG5aH6hrBZTP1XXN/HuR9Usmj7appeMMSHNEkQ/bd1fYdNLxpiwYAminzbvLSNzeBxT04cFOhRjjHGUJYh+OHmmmXft6SVjTJiwBNEPW/eX09auNr1kjAkLliD6YfPeMsYPj2PaGJteMsaEPksQfXTyTDPvfGTTS8aYvnGi3Pedd97JqFGj/LYRrjeWIPpom3t66cs2vWSM6YUT5b4Bbr/9drZs2TJg/bCNcn306t4yLkm16SVjBpvyf/1XmooP+vWeMVMmM/qf/7nH93fu3El2djYTJkwAID8/n40bNzJ16tTOazZu3MiaNWsAuOmmm1i5ciWqysaNG8nPzycmJoasrCyys7PZuXMn8+bN46qrruoy0nCajSD64JR7eunLM2x6yRjTu9LSUsaNO3faQUZGBqWlpT1eExkZSVJSEtXV1X367ECxEUQfbDtg00vGDFa+ftM3vtkIog9e3Vtu00vGmD5zotx3IFiC6MWpM828c6TKnl4yxvSZE+W+A8ESRC+2HSin1aaXjDH94Fnue8qUKdx8882d5b43bXIdh7N06VKqq6vJzs7m8ccf73wU1rPc98KFC7uU+77llluYN28ehw4dIiMjg2effbbHGPzSD0fvHgJe3VvOuNShXD7WppeMMX3nRLnv559/3r9B9sJGED7UnLXpJWNM+LIE4cO2/RU2vWSMCVuWIHx4ZW8ZGSlDmT42KdChGGPMgLME0YM/f3aKP3x4gr/+XIZNLxljwpIlCC/a25Xvv3yAUYkxLLtqQqDDMcaYgLAE4cVLu0vZc6yG+xZOJj7GHvQyxoQnSxDdnGlq5UdbDjIzI4mvzgrM7kVjzOB3oeW+q6urWbBgAQkJCaxcuXKAo+7KEkQ3Py38iMq6JlbfMJUhQ2ztwRjTfxdT7js2NpYf/OAHPPbYY4EIvQubP/Fw7ORZ1r19lLyZY7hifGqgwzHG+MHb6z+k6li9X+85YlwCX7j5sh7fv5hy3/Hx8Vx55ZUcOXLErzFfCBtBeHjktYMMEXjg+smBDsUYM4hdTLnvYGIjCLcdR6t5dW8Z93xxImOShwY6HGOMn/j6Td/4ZiMIoK1dWfvKAcYkxfL3V10a6HCMMYPcxZT7DiaWIIAXdh1j//Fa7r9+MkOjIwIdjjFmkLuYct/BJOynmOoaW3h06yGuGJ9C3swxgQ7HGBMCPMt9t7W1ceedd3aW+87NzSUvL4+lS5dy2223kZ2dTWpqKgUFBZ2fz8zMpLa2lubmZjZs2MC2bdu6LHAPWD8G/DsGmYaWNj53SQorFmQHXfY2xgxeF1Puu2NPRKCFfYIYlRjLum/mBjoMY4wJOrYGYYwxxitLEMaYkKSqgQ4hqFzIfw9HE4SILBSRQyJyREQe8PL+EyJS5P7zoYjUeLzX5vHeJifjNMaEltjYWKqrqy1JuKkq1dXVxMbG9utzjq1BiEgE8BRwLVACvC8im1S1syCJqq7yuP5bwCyPWzSoao5T8RljQldGRgYlJSWcOHEi0KEMiMbGxl7/8Y+NjSUjI6Nf93VykXo2cERVjwKISAGwGDjQw/W3AN9zMB5jTJiIiooiKysr0GEMmMLCQmbNmtX7hf3k5BTTWOCYx+sSd9t5RGQ8kAW85dEcKyJ/EpH3RORG58I0xhjjTbA85poPvKCqbR5t41W1VEQmAG+JyF5V/cjzQyKyDFgGkJaWRmFhoc9vUl9f3+s1oSpc+279Di/Wb/9yMkGUAuM8Xme427zJB1Z4NqhqqfvvoyJSiGt94qNu16wD1gHk5ubq/PnzfQZUWFhIb9eEqnDtu/U7vFi//UucWuUXkUjgQ+AaXInhfeBWVd3f7brJwBYgS93BiEgKcFZVm0RkBPAusNhzgdvL9zsBfNpLWCOAqgvs0mAXrn23focX63f/jVfVkd7ecGwEoaqtIrIS2ApEAD9X1f0ishb4k6p2PLqaDxRo10w1BXhaRNpxrZM84is5uL+f1w56EpE/qWpYbpsO175bv8OL9du/HF2DUNXNwOZubau7vV7j5XPvANOdjM0YY4xvtpPaGGOMV+GWINYFOoAACte+W7/Di/XbjxxbpDbGGDO4hdsIwhhjTB9ZgjDGGONVyCYIEfm5iFSKyD6PtlQReV1EDrv/TglkjE4QkXEisl1EDojIfhH5trs9pPsuIrEislNE9rj7/X13e5aI7HBXFP6NiEQHOlYniEiEiOwWkVfcr0O+3yLyiYjsdVd8/pO7LaR/zgFEJFlEXhCRgyJSLCLznOp3yCYI4P8BC7u1PQC8qaoTgTfdr0NNK3Cvqk4F5gIrRGQqod/3JuBqVZ0J5AALRWQu8CPgCVXNBk4BSwMYo5O+DRR7vA6Xfi9Q1RyPPQCh/nMO8J/AFlWdDMzE9f/dmX6rasj+ATKBfR6vDwHp7q/TgUOBjnEA/htsxFVyPWz6DsQBfwbm4NpdGulunwdsDXR8DvQ3w/2PwtXAK4CESb8/AUZ0awvpn3MgCfgY9wNGTvc7lEcQ3qSpapn763IgLZDBOE1EMnHVsNpBGPTdPc1SBFQCr+Oq3VWjqq3uS3qsKDzI/Ri4D2h3vx5OePRbgW0isstduBNC/+c8CzgB/F/3lOL/iEg8DvU73BJEJ3Wl2pB9xldEEoDfAfeoaq3ne6Had1VtU9chUxm4ziOZHOCQHCciXwEqVXVXoGMJgCtV9XPA9bimUq/yfDNEf84jgc8BP1XVWcAZuk0n+bPf4ZYgKkQkHcD9d2WA43GEiEThSg6/VtUX3c1h0XcAVa0BtuOaWkl2F44E3xWFB6u/BPJE5BOgANc0038S+v1Gz1V8rgRewvVLQaj/nJcAJaq6w/36BVwJw5F+h1uC2AT8rfvrv8U1Px9SRESAZ4FiVX3c462Q7rsNiySBAAACgklEQVSIjBSRZPfXQ3GtuxTjShQ3uS8LuX6r6ndVNUNVM3EVvnxLVf+GEO+3iMSLSGLH18CXgH2E+M+5qpYDx0RkkrvpGlyndDrS75DdSS0izwPzcZXBrcB1nOkGYD1wCa7S4Der6slAxegEEbkSeBvYy7k56X/GtQ4Rsn0XkRnAL3BVDh4CrFfVte4DpwqAVGA38A1VbQpcpM4RkfnAd1T1K6Heb3f/XnK/jASeU9UfishwQvjnHEBEcoD/AaKBo8AduH/m8XO/QzZBGGOMuTjhNsVkjDGmjyxBGGOM8coShDHGGK8sQRhjjPHKEoQxxhivLEEY0w8i0uauHtrxx2/F4EQk07P6sDGBFtn7JcYYDw3uch7GhDwbQRjjB+6zCf7dfT7BThHJdrdnishbIvKBiLwpIpe429NE5CX3+RV7ROQv3LeKEJFn3GdabHPvCjcmICxBGNM/Q7tNMX3d473TqjodeBJXhVWAnwC/UNUZwK+B/3K3/xfwe3WdX/E5YL+7fSLwlKpOA2qAv3a4P8b0yHZSG9MPIlKvqgle2j/BdWDRUXexxHJVHS4iVbjq9Le428tUdYSInAAyPMtfuMuzv66uQ18QkfuBKFV92PmeGXM+G0EY4z/aw9f94VkvqQ1bJzQBZAnCGP/5usff77q/fgdXlVWAv8FVSBFcJ8Ath86DjpIGKkhj+sp+OzGmf4a6T63rsEVVOx51TRGRD3CNAm5xt30L1+lf/4TrJLA73O3fBtaJyFJcI4XlQBnGBBFbgzDGD9xrELmqWhXoWIzxF5tiMsYY45WNIIwxxnhlIwhjjDFeWYIwxhjjlSUIY4wxXlmCMMYY45UlCGOMMV79f+3B5o4/cMh4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_TRP0hHAgnMB",
        "outputId": "8f7e0138-cdc9-4cb9-f821-88e7cbdbc770"
      },
      "source": [
        "#调参 learning rate\r\n",
        "\r\n",
        "HIDDEN_SIZE = 128\r\n",
        "N_LAYER = 3\r\n",
        "BATCH_SIZE = 256\r\n",
        "\r\n",
        "N_EPOCHS1  = 10\r\n",
        "LR1 = 0.001\r\n",
        "N_EPOCHS2  = 500\r\n",
        "LR2 = 0.0001\r\n",
        "\r\n",
        "\r\n",
        "trainset = DayFeatureDataset(is_train_set = True)\r\n",
        "trainloader = DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True)\r\n",
        "devset = DayFeatureDataset(is_train_set = False)\r\n",
        "devloader = DataLoader(devset, batch_size = BATCH_SIZE, shuffle = False)\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "  classifier = RNNClassifier(11, HIDDEN_SIZE, N_CLASS, N_LAYER, bidirectional = False)\r\n",
        "  if USE_GPU:\r\n",
        "    device = torch.device(\"cuda:0\")\r\n",
        "    classifier.to(device)\r\n",
        "\r\n",
        "  criterion = torch.nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "  optimizer = torch.optim.Adam(classifier.parameters(), lr=LR1)\r\n",
        "\r\n",
        "  start = time.time()\r\n",
        "  print(\"Training for %d epochs...\" % N_EPOCHS1 )\r\n",
        "  acc_list = []\r\n",
        "  auc_list = []\r\n",
        "  for epoch in range(1, N_EPOCHS1  + 1):\r\n",
        "    # Train cycle\r\n",
        "    trainModel()\r\n",
        "    acc, auc = devModel_auc()\r\n",
        "    acc_list.append(acc)\r\n",
        "    auc_list.append(auc)\r\n",
        "\r\n",
        "  optimizer = torch.optim.Adam(classifier.parameters(), lr=LR2)\r\n",
        "\r\n",
        "  start = time.time()\r\n",
        "  print(\"Training for another %d epochs...\" % N_EPOCHS2 )\r\n",
        "  for epoch in range(N_EPOCHS1, N_EPOCHS2  + N_EPOCHS1):\r\n",
        "    # Train cycle\r\n",
        "    trainModel()\r\n",
        "    acc, auc = devModel_auc()\r\n",
        "    acc_list.append(acc)\r\n",
        "    auc_list.append(auc)\r\n",
        "  accs.append(acc_list)\r\n",
        "  aucs.append(auc_list)\r\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 10 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.002543340320698917\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.002475746313575655\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.0023982268447677296\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1923/2658 72.35%\n",
            "Dev set: AUC  0.7615272223299603\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.0020847768289968373\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.00205995308351703\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.0020572996698319914\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1963/2658 73.85%\n",
            "Dev set: AUC  0.7884536059030093\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.0020389616140164433\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.002017208148026839\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.002016541330764691\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1987/2658 74.76%\n",
            "Dev set: AUC  0.7893025961794176\n",
            "[0m 0s] Epoch 4 [2560/8158] loss=0.002022141800262034\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.002041573345195502\n",
            "[0m 0s] Epoch 4 [7680/8158] loss=0.002013451466336846\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1962/2658 73.81%\n",
            "Dev set: AUC  0.789272590942714\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.001984097412787378\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.0019679922726936637\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.002001348576353242\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.7889147390144512\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.00197098848875612\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.001998804183676839\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.0019891257359025377\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dev set: AUC  0.7894061932071934\n",
            "[0m 1s] Epoch 7 [2560/8158] loss=0.001995752938091755\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.001990409241989255\n",
            "[0m 1s] Epoch 7 [7680/8158] loss=0.001981155399698764\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1975/2658 74.30%\n",
            "Dev set: AUC  0.7897135099999558\n",
            "[0m 1s] Epoch 8 [2560/8158] loss=0.0020162184839136897\n",
            "[0m 1s] Epoch 8 [5120/8158] loss=0.001995446375804022\n",
            "[0m 1s] Epoch 8 [7680/8158] loss=0.0019817057337301472\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dev set: AUC  0.7902494982808579\n",
            "[0m 2s] Epoch 9 [2560/8158] loss=0.0020264391554519535\n",
            "[0m 2s] Epoch 9 [5120/8158] loss=0.0019984777783975003\n",
            "[0m 2s] Epoch 9 [7680/8158] loss=0.0019864557194523512\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7909411979480208\n",
            "[0m 2s] Epoch 10 [2560/8158] loss=0.001940118765924126\n",
            "[0m 2s] Epoch 10 [5120/8158] loss=0.0019316131889354437\n",
            "[0m 2s] Epoch 10 [7680/8158] loss=0.0019724941036353507\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1980/2658 74.49%\n",
            "Dev set: AUC  0.7908306523391135\n",
            "Training for another 500 epochs...\n",
            "[0m 0s] Epoch 10 [2560/8158] loss=0.0019793259678408505\n",
            "[0m 0s] Epoch 10 [5120/8158] loss=0.001977572147734463\n",
            "[0m 0s] Epoch 10 [7680/8158] loss=0.0019677573853793245\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dev set: AUC  0.7913360036941185\n",
            "[0m 0s] Epoch 11 [2560/8158] loss=0.001908868202008307\n",
            "[0m 0s] Epoch 11 [5120/8158] loss=0.001959368819370866\n",
            "[0m 0s] Epoch 11 [7680/8158] loss=0.001963185794496288\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7914193866676942\n",
            "[0m 0s] Epoch 12 [2560/8158] loss=0.0018883018754422664\n",
            "[0m 0s] Epoch 12 [5120/8158] loss=0.0019646770611871033\n",
            "[0m 0s] Epoch 12 [7680/8158] loss=0.0019623565409953394\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7915368808577329\n",
            "[0m 0s] Epoch 13 [2560/8158] loss=0.0019327693968079983\n",
            "[0m 0s] Epoch 13 [5120/8158] loss=0.001943394122645259\n",
            "[0m 1s] Epoch 13 [7680/8158] loss=0.0019548891616674763\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7915924695067834\n",
            "[0m 1s] Epoch 14 [2560/8158] loss=0.0019260750152170659\n",
            "[0m 1s] Epoch 14 [5120/8158] loss=0.0019432925852015614\n",
            "[0m 1s] Epoch 14 [7680/8158] loss=0.0019710044531772533\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7917042784940784\n",
            "[0m 1s] Epoch 15 [2560/8158] loss=0.0019246410112828017\n",
            "[0m 1s] Epoch 15 [5120/8158] loss=0.001955204916885123\n",
            "[0m 1s] Epoch 15 [7680/8158] loss=0.0019744412003395457\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7917990318731416\n",
            "[0m 1s] Epoch 16 [2560/8158] loss=0.001950761384796351\n",
            "[0m 1s] Epoch 16 [5120/8158] loss=0.0019778216432314367\n",
            "[0m 1s] Epoch 16 [7680/8158] loss=0.001970016867077599\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7918596740357422\n",
            "[0m 1s] Epoch 17 [2560/8158] loss=0.0019965383457019926\n",
            "[0m 2s] Epoch 17 [5120/8158] loss=0.001978413638425991\n",
            "[0m 2s] Epoch 17 [7680/8158] loss=0.001959713757969439\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7919594809283557\n",
            "[0m 2s] Epoch 18 [2560/8158] loss=0.0019242169102653861\n",
            "[0m 2s] Epoch 18 [5120/8158] loss=0.0019596131751313806\n",
            "[0m 2s] Epoch 18 [7680/8158] loss=0.0019603741820901632\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7920018041043373\n",
            "[0m 2s] Epoch 19 [2560/8158] loss=0.0019399916171096265\n",
            "[0m 2s] Epoch 19 [5120/8158] loss=0.0019648695655632765\n",
            "[0m 2s] Epoch 19 [7680/8158] loss=0.0019753692128385105\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7921887841056892\n",
            "[0m 2s] Epoch 20 [2560/8158] loss=0.001966219104360789\n",
            "[0m 2s] Epoch 20 [5120/8158] loss=0.001962341892067343\n",
            "[0m 2s] Epoch 20 [7680/8158] loss=0.001968046130302052\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7922191051869895\n",
            "[0m 2s] Epoch 21 [2560/8158] loss=0.0020099325920455158\n",
            "[0m 3s] Epoch 21 [5120/8158] loss=0.001974589010933414\n",
            "[0m 3s] Epoch 21 [7680/8158] loss=0.001976727011303107\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7923460747149343\n",
            "[0m 3s] Epoch 22 [2560/8158] loss=0.0019561716355383394\n",
            "[0m 3s] Epoch 22 [5120/8158] loss=0.0019396531046368183\n",
            "[0m 3s] Epoch 22 [7680/8158] loss=0.0019546898431144653\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.7924288259993165\n",
            "[0m 3s] Epoch 23 [2560/8158] loss=0.001938863773830235\n",
            "[0m 3s] Epoch 23 [5120/8158] loss=0.0019473637628834694\n",
            "[0m 3s] Epoch 23 [7680/8158] loss=0.0019657942660463354\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.792512840662086\n",
            "[0m 3s] Epoch 24 [2560/8158] loss=0.0019105149782262742\n",
            "[0m 3s] Epoch 24 [5120/8158] loss=0.0019363675150088967\n",
            "[0m 4s] Epoch 24 [7680/8158] loss=0.001961203357980897\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7926492855279376\n",
            "[0m 4s] Epoch 25 [2560/8158] loss=0.001930313347838819\n",
            "[0m 4s] Epoch 25 [5120/8158] loss=0.001985642610816285\n",
            "[0m 4s] Epoch 25 [7680/8158] loss=0.0019681635545566677\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7927554093124884\n",
            "[0m 4s] Epoch 26 [2560/8158] loss=0.0019841417204588653\n",
            "[0m 4s] Epoch 26 [5120/8158] loss=0.001958020927850157\n",
            "[0m 4s] Epoch 26 [7680/8158] loss=0.0019695092109031976\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7928564795834894\n",
            "[0m 4s] Epoch 27 [2560/8158] loss=0.0019985053688287733\n",
            "[0m 4s] Epoch 27 [5120/8158] loss=0.0019612725358456374\n",
            "[0m 4s] Epoch 27 [7680/8158] loss=0.001957125736710926\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7928482676239705\n",
            "[0m 4s] Epoch 28 [2560/8158] loss=0.0019624331616796553\n",
            "[0m 5s] Epoch 28 [5120/8158] loss=0.0019425023871008306\n",
            "[0m 5s] Epoch 28 [7680/8158] loss=0.0019611943202714126\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7929644984356216\n",
            "[0m 5s] Epoch 29 [2560/8158] loss=0.0019373292336240411\n",
            "[0m 5s] Epoch 29 [5120/8158] loss=0.0019340782717335969\n",
            "[0m 5s] Epoch 29 [7680/8158] loss=0.0019654722845492264\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dev set: AUC  0.7930737806661414\n",
            "[0m 5s] Epoch 30 [2560/8158] loss=0.001987575425300747\n",
            "[0m 5s] Epoch 30 [5120/8158] loss=0.0019601514330133797\n",
            "[0m 5s] Epoch 30 [7680/8158] loss=0.00195748625556007\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7932140156671554\n",
            "[0m 5s] Epoch 31 [2560/8158] loss=0.0019148411112837494\n",
            "[0m 5s] Epoch 31 [5120/8158] loss=0.0019416522700339556\n",
            "[0m 5s] Epoch 31 [7680/8158] loss=0.0019564473923916617\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7932942401947622\n",
            "[0m 5s] Epoch 32 [2560/8158] loss=0.001965858624316752\n",
            "[0m 6s] Epoch 32 [5120/8158] loss=0.0019719469652045517\n",
            "[0m 6s] Epoch 32 [7680/8158] loss=0.00196099440411975\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7932948718839563\n",
            "[0m 6s] Epoch 33 [2560/8158] loss=0.0019423144753091038\n",
            "[0m 6s] Epoch 33 [5120/8158] loss=0.0019446393533144146\n",
            "[0m 6s] Epoch 33 [7680/8158] loss=0.0019548262857521574\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.793269604316206\n",
            "[0m 6s] Epoch 34 [2560/8158] loss=0.0019304399727843701\n",
            "[0m 6s] Epoch 34 [5120/8158] loss=0.001961106539238244\n",
            "[0m 6s] Epoch 34 [7680/8158] loss=0.0019662396788286667\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7933877301954383\n",
            "[0m 6s] Epoch 35 [2560/8158] loss=0.0019776840228587387\n",
            "[0m 6s] Epoch 35 [5120/8158] loss=0.0019712681823875755\n",
            "[0m 6s] Epoch 35 [7680/8158] loss=0.0019608949311077594\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7934344751957763\n",
            "[0m 6s] Epoch 36 [2560/8158] loss=0.00201088854810223\n",
            "[0m 7s] Epoch 36 [5120/8158] loss=0.0019663728366140277\n",
            "[0m 7s] Epoch 36 [7680/8158] loss=0.001967089157551527\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7934641645878828\n",
            "[0m 7s] Epoch 37 [2560/8158] loss=0.001997355406638235\n",
            "[0m 7s] Epoch 37 [5120/8158] loss=0.001998930214904249\n",
            "[0m 7s] Epoch 37 [7680/8158] loss=0.001968803466297686\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7935905024266342\n",
            "[0m 7s] Epoch 38 [2560/8158] loss=0.0018957831431180238\n",
            "[0m 7s] Epoch 38 [5120/8158] loss=0.0019368850160390138\n",
            "[0m 7s] Epoch 38 [7680/8158] loss=0.0019546047667972745\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7936126115484154\n",
            "[0m 7s] Epoch 39 [2560/8158] loss=0.001946844079066068\n",
            "[0m 7s] Epoch 39 [5120/8158] loss=0.001959966530557722\n",
            "[0m 7s] Epoch 39 [7680/8158] loss=0.0019569580249177913\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7936754646231943\n",
            "[0m 8s] Epoch 40 [2560/8158] loss=0.001958414970431477\n",
            "[0m 8s] Epoch 40 [5120/8158] loss=0.001950284605845809\n",
            "[0m 8s] Epoch 40 [7680/8158] loss=0.0019563796425548694\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.793665673440691\n",
            "[0m 8s] Epoch 41 [2560/8158] loss=0.0019580012653023005\n",
            "[0m 8s] Epoch 41 [5120/8158] loss=0.0019663287384901196\n",
            "[0m 8s] Epoch 41 [7680/8158] loss=0.001954102384236952\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7936940994544102\n",
            "[0m 8s] Epoch 42 [2560/8158] loss=0.001964554947335273\n",
            "[0m 8s] Epoch 42 [5120/8158] loss=0.0019632998621091245\n",
            "[0m 8s] Epoch 42 [7680/8158] loss=0.0019530386353532473\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.793709891684254\n",
            "[0m 8s] Epoch 43 [2560/8158] loss=0.0019368894398212432\n",
            "[0m 9s] Epoch 43 [5120/8158] loss=0.001907057932112366\n",
            "[0m 9s] Epoch 43 [7680/8158] loss=0.001946133243230482\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7937427395223292\n",
            "[0m 9s] Epoch 44 [2560/8158] loss=0.0020164704765193164\n",
            "[0m 9s] Epoch 44 [5120/8158] loss=0.0019632390467450023\n",
            "[0m 9s] Epoch 44 [7680/8158] loss=0.001957577579499533\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7937894845226672\n",
            "[0m 9s] Epoch 45 [2560/8158] loss=0.0019374686758965253\n",
            "[0m 9s] Epoch 45 [5120/8158] loss=0.001948986144270748\n",
            "[0m 9s] Epoch 45 [7680/8158] loss=0.0019519295621042451\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1986/2658 74.72%\n",
            "Dev set: AUC  0.7938968716856059\n",
            "[0m 9s] Epoch 46 [2560/8158] loss=0.0019086145446635782\n",
            "[0m 9s] Epoch 46 [5120/8158] loss=0.001959279243601486\n",
            "[0m 9s] Epoch 46 [7680/8158] loss=0.001959683552073936\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7938880280368932\n",
            "[0m 9s] Epoch 47 [2560/8158] loss=0.0019249796983785926\n",
            "[0m 10s] Epoch 47 [5120/8158] loss=0.0019745135272387416\n",
            "[0m 10s] Epoch 47 [7680/8158] loss=0.0019598162073331577\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7939379314831999\n",
            "[0m 10s] Epoch 48 [2560/8158] loss=0.0019196210545487703\n",
            "[0m 10s] Epoch 48 [5120/8158] loss=0.001964107697131112\n",
            "[0m 10s] Epoch 48 [7680/8158] loss=0.001950196933466941\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7939556187806251\n",
            "[0m 10s] Epoch 49 [2560/8158] loss=0.001936717692296952\n",
            "[0m 10s] Epoch 49 [5120/8158] loss=0.0019473494263365864\n",
            "[0m 10s] Epoch 49 [7680/8158] loss=0.0019539674782815077\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7940554256732385\n",
            "[0m 10s] Epoch 50 [2560/8158] loss=0.0019944937899708747\n",
            "[0m 10s] Epoch 50 [5120/8158] loss=0.001960456505184993\n",
            "[0m 10s] Epoch 50 [7680/8158] loss=0.001958327581329892\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7939830972605534\n",
            "[0m 11s] Epoch 51 [2560/8158] loss=0.0019647546461783348\n",
            "[0m 11s] Epoch 51 [5120/8158] loss=0.0019576674269046633\n",
            "[0m 11s] Epoch 51 [7680/8158] loss=0.0019621478316063684\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7940408968217822\n",
            "[0m 11s] Epoch 52 [2560/8158] loss=0.001942260644864291\n",
            "[0m 11s] Epoch 52 [5120/8158] loss=0.0019288249895907938\n",
            "[0m 11s] Epoch 52 [7680/8158] loss=0.0019399706895152728\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7940497404704947\n",
            "[0m 11s] Epoch 53 [2560/8158] loss=0.0019487465848214923\n",
            "[0m 11s] Epoch 53 [5120/8158] loss=0.001960880123078823\n",
            "[0m 11s] Epoch 53 [7680/8158] loss=0.0019410223544885715\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7940914319572826\n",
            "[0m 11s] Epoch 54 [2560/8158] loss=0.001990544272121042\n",
            "[0m 11s] Epoch 54 [5120/8158] loss=0.0019749200670048594\n",
            "[0m 12s] Epoch 54 [7680/8158] loss=0.0019684294898373385\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7941185945926141\n",
            "[0m 12s] Epoch 55 [2560/8158] loss=0.0019519247696734964\n",
            "[0m 12s] Epoch 55 [5120/8158] loss=0.0019474583968985826\n",
            "[0m 12s] Epoch 55 [7680/8158] loss=0.0019521868865316112\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7941097509439016\n",
            "[0m 12s] Epoch 56 [2560/8158] loss=0.0019260191591456532\n",
            "[0m 12s] Epoch 56 [5120/8158] loss=0.001958696753717959\n",
            "[0m 12s] Epoch 56 [7680/8158] loss=0.0019462310515033702\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.794160286079402\n",
            "[0m 12s] Epoch 57 [2560/8158] loss=0.0019316689460538328\n",
            "[0m 12s] Epoch 57 [5120/8158] loss=0.0019477214431390167\n",
            "[0m 12s] Epoch 57 [7680/8158] loss=0.0019509721353339652\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7941703931065022\n",
            "[0m 12s] Epoch 58 [2560/8158] loss=0.0019550463068298994\n",
            "[0m 13s] Epoch 58 [5120/8158] loss=0.001928974752081558\n",
            "[0m 13s] Epoch 58 [7680/8158] loss=0.001952002333321919\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7942146113500651\n",
            "[0m 13s] Epoch 59 [2560/8158] loss=0.0019481245893985034\n",
            "[0m 13s] Epoch 59 [5120/8158] loss=0.0019490106496959925\n",
            "[0m 13s] Epoch 59 [7680/8158] loss=0.001960353316583981\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7941558642550457\n",
            "[0m 13s] Epoch 60 [2560/8158] loss=0.002013285318389535\n",
            "[0m 13s] Epoch 60 [5120/8158] loss=0.0019722450408153234\n",
            "[0m 13s] Epoch 60 [7680/8158] loss=0.001966007212953021\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.794297362634447\n",
            "[0m 13s] Epoch 61 [2560/8158] loss=0.0019677440286614\n",
            "[0m 13s] Epoch 61 [5120/8158] loss=0.001931167347356677\n",
            "[0m 13s] Epoch 61 [7680/8158] loss=0.0019513493364987273\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.794261356350403\n",
            "[0m 14s] Epoch 62 [2560/8158] loss=0.0019075457355938851\n",
            "[0m 14s] Epoch 62 [5120/8158] loss=0.0019401117227971555\n",
            "[0m 14s] Epoch 62 [7680/8158] loss=0.001955680518100659\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7942670415531468\n",
            "[0m 14s] Epoch 63 [2560/8158] loss=0.0019442906719632446\n",
            "[0m 14s] Epoch 63 [5120/8158] loss=0.0019398602133151143\n",
            "[0m 14s] Epoch 63 [7680/8158] loss=0.0019447712266507248\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7943523195943039\n",
            "[0m 14s] Epoch 64 [2560/8158] loss=0.0019753943546675146\n",
            "[0m 14s] Epoch 64 [5120/8158] loss=0.001959795644506812\n",
            "[0m 14s] Epoch 64 [7680/8158] loss=0.0019540428610829016\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7942992577020285\n",
            "[0m 14s] Epoch 65 [2560/8158] loss=0.002005060843657702\n",
            "[0m 14s] Epoch 65 [5120/8158] loss=0.0019791227707173674\n",
            "[0m 14s] Epoch 65 [7680/8158] loss=0.0019593342828253904\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.794419278648842\n",
            "[0m 15s] Epoch 66 [2560/8158] loss=0.00196515730349347\n",
            "[0m 15s] Epoch 66 [5120/8158] loss=0.0019618509511929007\n",
            "[0m 15s] Epoch 66 [7680/8158] loss=0.0019497347413562239\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7943826406756043\n",
            "[0m 15s] Epoch 67 [2560/8158] loss=0.0019279600586742164\n",
            "[0m 15s] Epoch 67 [5120/8158] loss=0.001953696471173316\n",
            "[0m 15s] Epoch 67 [7680/8158] loss=0.00195178782256941\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7945165587846805\n",
            "[0m 15s] Epoch 68 [2560/8158] loss=0.001981930248439312\n",
            "[0m 15s] Epoch 68 [5120/8158] loss=0.0019673122151289135\n",
            "[0m 15s] Epoch 68 [7680/8158] loss=0.0019612431603794295\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2010/2658 75.62%\n",
            "Dev set: AUC  0.7944805525006363\n",
            "[0m 15s] Epoch 69 [2560/8158] loss=0.0019108065636828542\n",
            "[0m 15s] Epoch 69 [5120/8158] loss=0.0019193179497960955\n",
            "[0m 15s] Epoch 69 [7680/8158] loss=0.001952357504827281\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2009/2658 75.58%\n",
            "Dev set: AUC  0.7945481432443684\n",
            "[0m 16s] Epoch 70 [2560/8158] loss=0.001976553350687027\n",
            "[0m 16s] Epoch 70 [5120/8158] loss=0.0019697519484907387\n",
            "[0m 16s] Epoch 70 [7680/8158] loss=0.0019662353210151196\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7945892030419625\n",
            "[0m 16s] Epoch 71 [2560/8158] loss=0.001968893432058394\n",
            "[0m 16s] Epoch 71 [5120/8158] loss=0.0019656411895994097\n",
            "[0m 16s] Epoch 71 [7680/8158] loss=0.0019522739108651876\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.7946548987181131\n",
            "[0m 16s] Epoch 72 [2560/8158] loss=0.002025119587779045\n",
            "[0m 16s] Epoch 72 [5120/8158] loss=0.001989247719757259\n",
            "[0m 16s] Epoch 72 [7680/8158] loss=0.0019564040820114316\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7946441600018195\n",
            "[0m 16s] Epoch 73 [2560/8158] loss=0.0018753914046101272\n",
            "[0m 16s] Epoch 73 [5120/8158] loss=0.0019116529263556003\n",
            "[0m 16s] Epoch 73 [7680/8158] loss=0.0019541072271143396\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7948052407462273\n",
            "[0m 17s] Epoch 74 [2560/8158] loss=0.0019354421645402908\n",
            "[0m 17s] Epoch 74 [5120/8158] loss=0.0019444896723143756\n",
            "[0m 17s] Epoch 74 [7680/8158] loss=0.0019502178921053806\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7946921683805448\n",
            "[0m 17s] Epoch 75 [2560/8158] loss=0.001965516642667353\n",
            "[0m 17s] Epoch 75 [5120/8158] loss=0.0019373234536033124\n",
            "[0m 17s] Epoch 75 [7680/8158] loss=0.00194404215629523\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7947357549349139\n",
            "[0m 17s] Epoch 76 [2560/8158] loss=0.0019861275563016535\n",
            "[0m 17s] Epoch 76 [5120/8158] loss=0.001953453116584569\n",
            "[0m 17s] Epoch 76 [7680/8158] loss=0.0019478464693141481\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7947673393946016\n",
            "[0m 17s] Epoch 77 [2560/8158] loss=0.0019219311303459107\n",
            "[0m 17s] Epoch 77 [5120/8158] loss=0.0019312024989631027\n",
            "[0m 18s] Epoch 77 [7680/8158] loss=0.001957698806654662\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.794896203990128\n",
            "[0m 18s] Epoch 78 [2560/8158] loss=0.0019450351479463278\n",
            "[0m 18s] Epoch 78 [5120/8158] loss=0.0019302158325444907\n",
            "[0m 18s] Epoch 78 [7680/8158] loss=0.0019413515420941016\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7948835702062529\n",
            "[0m 18s] Epoch 79 [2560/8158] loss=0.0019274081336334349\n",
            "[0m 18s] Epoch 79 [5120/8158] loss=0.0019272641628049314\n",
            "[0m 18s] Epoch 79 [7680/8158] loss=0.0019425603949154416\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7949378954769158\n",
            "[0m 18s] Epoch 80 [2560/8158] loss=0.0019191845087334514\n",
            "[0m 18s] Epoch 80 [5120/8158] loss=0.0019491350976750255\n",
            "[0m 18s] Epoch 80 [7680/8158] loss=0.0019474408822134138\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7950610748696983\n",
            "[0m 18s] Epoch 81 [2560/8158] loss=0.002032929239794612\n",
            "[0m 18s] Epoch 81 [5120/8158] loss=0.0019621477695181966\n",
            "[0m 19s] Epoch 81 [7680/8158] loss=0.0019556427219261725\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7949783235853163\n",
            "[0m 19s] Epoch 82 [2560/8158] loss=0.001919185766018927\n",
            "[0m 19s] Epoch 82 [5120/8158] loss=0.0019443980359937995\n",
            "[0m 19s] Epoch 82 [7680/8158] loss=0.001940516964532435\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7950598114913108\n",
            "[0m 19s] Epoch 83 [2560/8158] loss=0.0019052867428399622\n",
            "[0m 19s] Epoch 83 [5120/8158] loss=0.0019379558099899441\n",
            "[0m 19s] Epoch 83 [7680/8158] loss=0.001953703471614669\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.795064233315667\n",
            "[0m 19s] Epoch 84 [2560/8158] loss=0.0018983261776156723\n",
            "[0m 19s] Epoch 84 [5120/8158] loss=0.001918816100805998\n",
            "[0m 19s] Epoch 84 [7680/8158] loss=0.001946662327585121\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7951627768298931\n",
            "[0m 19s] Epoch 85 [2560/8158] loss=0.001953617832623422\n",
            "[0m 20s] Epoch 85 [5120/8158] loss=0.0019634755211882294\n",
            "[0m 20s] Epoch 85 [7680/8158] loss=0.001959820829021434\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7951419310864991\n",
            "[0m 20s] Epoch 86 [2560/8158] loss=0.0019421016564592718\n",
            "[0m 20s] Epoch 86 [5120/8158] loss=0.0019590296142268926\n",
            "[0m 20s] Epoch 86 [7680/8158] loss=0.0019565903154822686\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7952474231818564\n",
            "[0m 20s] Epoch 87 [2560/8158] loss=0.0019592050812207162\n",
            "[0m 20s] Epoch 87 [5120/8158] loss=0.0019664616731461136\n",
            "[0m 20s] Epoch 87 [7680/8158] loss=0.0019555775836731\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7952303675736251\n",
            "[0m 20s] Epoch 88 [2560/8158] loss=0.001954989123623818\n",
            "[0m 20s] Epoch 88 [5120/8158] loss=0.0019353976065758615\n",
            "[0m 20s] Epoch 88 [7680/8158] loss=0.0019468457515661914\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7952929048038069\n",
            "[0m 21s] Epoch 89 [2560/8158] loss=0.001981221023015678\n",
            "[0m 21s] Epoch 89 [5120/8158] loss=0.001979375717928633\n",
            "[0m 21s] Epoch 89 [7680/8158] loss=0.001953758687401811\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7952809027091254\n",
            "[0m 21s] Epoch 90 [2560/8158] loss=0.0019321757601574064\n",
            "[0m 21s] Epoch 90 [5120/8158] loss=0.0019271799479611217\n",
            "[0m 21s] Epoch 90 [7680/8158] loss=0.0019553591613657773\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7953447033176948\n",
            "[0m 21s] Epoch 91 [2560/8158] loss=0.0019635106436908245\n",
            "[0m 21s] Epoch 91 [5120/8158] loss=0.001965557225048542\n",
            "[0m 21s] Epoch 91 [7680/8158] loss=0.0019443049173181255\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7953061702768757\n",
            "[0m 21s] Epoch 92 [2560/8158] loss=0.0019117298536002636\n",
            "[0m 21s] Epoch 92 [5120/8158] loss=0.0019191185128875077\n",
            "[0m 21s] Epoch 92 [7680/8158] loss=0.001939792368405809\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7953238575743009\n",
            "[0m 22s] Epoch 93 [2560/8158] loss=0.0019183409516699613\n",
            "[0m 22s] Epoch 93 [5120/8158] loss=0.0019554649596102537\n",
            "[0m 22s] Epoch 93 [7680/8158] loss=0.001952628418803215\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.795289746357838\n",
            "[0m 22s] Epoch 94 [2560/8158] loss=0.001965966506395489\n",
            "[0m 22s] Epoch 94 [5120/8158] loss=0.001958756649401039\n",
            "[0m 22s] Epoch 94 [7680/8158] loss=0.0019501360637756684\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7953156456147821\n",
            "[0m 22s] Epoch 95 [2560/8158] loss=0.001993963681161404\n",
            "[0m 22s] Epoch 95 [5120/8158] loss=0.0019760983064770698\n",
            "[0m 22s] Epoch 95 [7680/8158] loss=0.0019481224589981139\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7953756560881888\n",
            "[0m 22s] Epoch 96 [2560/8158] loss=0.0020216281292960046\n",
            "[0m 23s] Epoch 96 [5120/8158] loss=0.0019582170876674353\n",
            "[0m 23s] Epoch 96 [7680/8158] loss=0.0019577724083016316\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.795375024398995\n",
            "[0m 23s] Epoch 97 [2560/8158] loss=0.0019503866555169226\n",
            "[0m 23s] Epoch 97 [5120/8158] loss=0.0019274542632047088\n",
            "[0m 23s] Epoch 97 [7680/8158] loss=0.0019405958785985906\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7953873423382732\n",
            "[0m 23s] Epoch 98 [2560/8158] loss=0.0019675834686495365\n",
            "[0m 23s] Epoch 98 [5120/8158] loss=0.001915315439691767\n",
            "[0m 23s] Epoch 98 [7680/8158] loss=0.001945789639527599\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7954261912236895\n",
            "[0m 23s] Epoch 99 [2560/8158] loss=0.00195955210365355\n",
            "[0m 24s] Epoch 99 [5120/8158] loss=0.001956994895590469\n",
            "[0m 24s] Epoch 99 [7680/8158] loss=0.001949835669559737\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7954615658185397\n",
            "[0m 24s] Epoch 100 [2560/8158] loss=0.0019071385497227312\n",
            "[0m 24s] Epoch 100 [5120/8158] loss=0.0019419114105403423\n",
            "[0m 24s] Epoch 100 [7680/8158] loss=0.001948292541783303\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7954874650754837\n",
            "[0m 24s] Epoch 101 [2560/8158] loss=0.0020072077051736413\n",
            "[0m 24s] Epoch 101 [5120/8158] loss=0.0019711750850547106\n",
            "[0m 24s] Epoch 101 [7680/8158] loss=0.0019500672390374044\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7954609341293459\n",
            "[0m 24s] Epoch 102 [2560/8158] loss=0.0019617709098383785\n",
            "[0m 24s] Epoch 102 [5120/8158] loss=0.001938158750999719\n",
            "[0m 24s] Epoch 102 [7680/8158] loss=0.001950448239222169\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7954931502782276\n",
            "[0m 25s] Epoch 103 [2560/8158] loss=0.0019556020619347692\n",
            "[0m 25s] Epoch 103 [5120/8158] loss=0.0019165493955370038\n",
            "[0m 25s] Epoch 103 [7680/8158] loss=0.0019393193768337369\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.795532946697434\n",
            "[0m 25s] Epoch 104 [2560/8158] loss=0.0018930208403617143\n",
            "[0m 25s] Epoch 104 [5120/8158] loss=0.001949458965100348\n",
            "[0m 25s] Epoch 104 [7680/8158] loss=0.0019404694826031725\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7953175406823633\n",
            "[0m 25s] Epoch 105 [2560/8158] loss=0.001953484013210982\n",
            "[0m 25s] Epoch 105 [5120/8158] loss=0.001944414450554177\n",
            "[0m 25s] Epoch 105 [7680/8158] loss=0.0019488610443659127\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7955398952785655\n",
            "[0m 25s] Epoch 106 [2560/8158] loss=0.001969052175991237\n",
            "[0m 25s] Epoch 106 [5120/8158] loss=0.00197624767315574\n",
            "[0m 25s] Epoch 106 [7680/8158] loss=0.0019579032940479617\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7955809550761596\n",
            "[0m 26s] Epoch 107 [2560/8158] loss=0.0019348473870195448\n",
            "[0m 26s] Epoch 107 [5120/8158] loss=0.001959619572153315\n",
            "[0m 26s] Epoch 107 [7680/8158] loss=0.0019554073999946318\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7955657945355095\n",
            "[0m 26s] Epoch 108 [2560/8158] loss=0.0018865427351556717\n",
            "[0m 26s] Epoch 108 [5120/8158] loss=0.0019180752686224877\n",
            "[0m 26s] Epoch 108 [7680/8158] loss=0.0019466958978834252\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7955310516298529\n",
            "[0m 26s] Epoch 109 [2560/8158] loss=0.00190106937661767\n",
            "[0m 26s] Epoch 109 [5120/8158] loss=0.0019351105845998972\n",
            "[0m 26s] Epoch 109 [7680/8158] loss=0.0019359686450722317\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7955594776435719\n",
            "[0m 26s] Epoch 110 [2560/8158] loss=0.0019053806900046766\n",
            "[0m 26s] Epoch 110 [5120/8158] loss=0.0019569700525607914\n",
            "[0m 27s] Epoch 110 [7680/8158] loss=0.0019436972603822749\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7956277000764976\n",
            "[0m 27s] Epoch 111 [2560/8158] loss=0.001902760425582528\n",
            "[0m 27s] Epoch 111 [5120/8158] loss=0.00192105108872056\n",
            "[0m 27s] Epoch 111 [7680/8158] loss=0.0019401680173662802\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7955342100758216\n",
            "[0m 27s] Epoch 112 [2560/8158] loss=0.0019760112976655365\n",
            "[0m 27s] Epoch 112 [5120/8158] loss=0.0019813210470601915\n",
            "[0m 27s] Epoch 112 [7680/8158] loss=0.0019513491657562553\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7954975721025838\n",
            "[0m 27s] Epoch 113 [2560/8158] loss=0.001959164580330253\n",
            "[0m 27s] Epoch 113 [5120/8158] loss=0.0019604117318522186\n",
            "[0m 27s] Epoch 113 [7680/8158] loss=0.0019320195346760253\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7955999057519724\n",
            "[0m 27s] Epoch 114 [2560/8158] loss=0.0019968749838881195\n",
            "[0m 27s] Epoch 114 [5120/8158] loss=0.0019510336744133383\n",
            "[0m 28s] Epoch 114 [7680/8158] loss=0.001954566116910428\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7957053978473296\n",
            "[0m 28s] Epoch 115 [2560/8158] loss=0.0019000512547791004\n",
            "[0m 28s] Epoch 115 [5120/8158] loss=0.001944122539134696\n",
            "[0m 28s] Epoch 115 [7680/8158] loss=0.0019462472448746363\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7957338238610486\n",
            "[0m 28s] Epoch 116 [2560/8158] loss=0.001989132503513247\n",
            "[0m 28s] Epoch 116 [5120/8158] loss=0.001962997461669147\n",
            "[0m 28s] Epoch 116 [7680/8158] loss=0.0019555508391931654\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7956397021711789\n",
            "[0m 28s] Epoch 117 [2560/8158] loss=0.0018796979798935353\n",
            "[0m 28s] Epoch 117 [5120/8158] loss=0.001934417913435027\n",
            "[0m 28s] Epoch 117 [7680/8158] loss=0.0019441941908250253\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7957085562932983\n",
            "[0m 28s] Epoch 118 [2560/8158] loss=0.0019486392615363003\n",
            "[0m 29s] Epoch 118 [5120/8158] loss=0.0019565101771149783\n",
            "[0m 29s] Epoch 118 [7680/8158] loss=0.0019503928061264256\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7957028710905546\n",
            "[0m 29s] Epoch 119 [2560/8158] loss=0.0018935001222416758\n",
            "[0m 29s] Epoch 119 [5120/8158] loss=0.001912756118690595\n",
            "[0m 29s] Epoch 119 [7680/8158] loss=0.001942550064995885\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7956390704819851\n",
            "[0m 29s] Epoch 120 [2560/8158] loss=0.0019135337555781008\n",
            "[0m 29s] Epoch 120 [5120/8158] loss=0.0019091661204583942\n",
            "[0m 29s] Epoch 120 [7680/8158] loss=0.0019464126128392915\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7956877105499044\n",
            "[0m 29s] Epoch 121 [2560/8158] loss=0.0019123752601444722\n",
            "[0m 29s] Epoch 121 [5120/8158] loss=0.0019628080248367042\n",
            "[0m 29s] Epoch 121 [7680/8158] loss=0.001948581108202537\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7956984492661983\n",
            "[0m 29s] Epoch 122 [2560/8158] loss=0.0019033990567550064\n",
            "[0m 30s] Epoch 122 [5120/8158] loss=0.0019166728132404387\n",
            "[0m 30s] Epoch 122 [7680/8158] loss=0.00194282391651844\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7958007829155868\n",
            "[0m 30s] Epoch 123 [2560/8158] loss=0.0019361717044375838\n",
            "[0m 30s] Epoch 123 [5120/8158] loss=0.001931646274169907\n",
            "[0m 30s] Epoch 123 [7680/8158] loss=0.0019484065589495004\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7957470893341174\n",
            "[0m 30s] Epoch 124 [2560/8158] loss=0.0019391474779695272\n",
            "[0m 30s] Epoch 124 [5120/8158] loss=0.0019471896288450808\n",
            "[0m 30s] Epoch 124 [7680/8158] loss=0.0019460709881968796\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7956295951440789\n",
            "[0m 30s] Epoch 125 [2560/8158] loss=0.0018856662209145725\n",
            "[0m 30s] Epoch 125 [5120/8158] loss=0.0018852043664082884\n",
            "[0m 30s] Epoch 125 [7680/8158] loss=0.0019294461196598907\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.795634016968435\n",
            "[0m 31s] Epoch 126 [2560/8158] loss=0.0019082713522948324\n",
            "[0m 31s] Epoch 126 [5120/8158] loss=0.001942348579177633\n",
            "[0m 31s] Epoch 126 [7680/8158] loss=0.001945854735095054\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7957799371721929\n",
            "[0m 31s] Epoch 127 [2560/8158] loss=0.0018811842543072998\n",
            "[0m 31s] Epoch 127 [5120/8158] loss=0.0019344464526511729\n",
            "[0m 31s] Epoch 127 [7680/8158] loss=0.0019499860005453228\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.795688342239098\n",
            "[0m 31s] Epoch 128 [2560/8158] loss=0.0019266214221715927\n",
            "[0m 31s] Epoch 128 [5120/8158] loss=0.0019126817060168833\n",
            "[0m 31s] Epoch 128 [7680/8158] loss=0.0019364625176725289\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7957540379152488\n",
            "[0m 31s] Epoch 129 [2560/8158] loss=0.001950835168827325\n",
            "[0m 31s] Epoch 129 [5120/8158] loss=0.0019320748397149146\n",
            "[0m 31s] Epoch 129 [7680/8158] loss=0.0019329914435123404\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.795766671699124\n",
            "[0m 32s] Epoch 130 [2560/8158] loss=0.001947375212330371\n",
            "[0m 32s] Epoch 130 [5120/8158] loss=0.0019527715921867639\n",
            "[0m 32s] Epoch 130 [7680/8158] loss=0.0019380258435072997\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7957028710905545\n",
            "[0m 32s] Epoch 131 [2560/8158] loss=0.0019089074805378914\n",
            "[0m 32s] Epoch 131 [5120/8158] loss=0.0019103475438896566\n",
            "[0m 32s] Epoch 131 [7680/8158] loss=0.0019471048726700246\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.795592957170841\n",
            "[0m 32s] Epoch 132 [2560/8158] loss=0.001958927104715258\n",
            "[0m 32s] Epoch 132 [5120/8158] loss=0.0019347150053363293\n",
            "[0m 32s] Epoch 132 [7680/8158] loss=0.0019304751379725833\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7957502477800863\n",
            "[0m 32s] Epoch 133 [2560/8158] loss=0.0019206308294087648\n",
            "[0m 32s] Epoch 133 [5120/8158] loss=0.0018868138024117798\n",
            "[0m 33s] Epoch 133 [7680/8158] loss=0.0019341946695931255\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7957129781176545\n",
            "[0m 33s] Epoch 134 [2560/8158] loss=0.0019134992617182434\n",
            "[0m 33s] Epoch 134 [5120/8158] loss=0.0019103990809526295\n",
            "[0m 33s] Epoch 134 [7680/8158] loss=0.0019482628519957264\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dev set: AUC  0.795696554198617\n",
            "[0m 33s] Epoch 135 [2560/8158] loss=0.0019260131637565791\n",
            "[0m 33s] Epoch 135 [5120/8158] loss=0.0019174762186594308\n",
            "[0m 33s] Epoch 135 [7680/8158] loss=0.0019457305199466646\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.795789412510099\n",
            "[0m 33s] Epoch 136 [2560/8158] loss=0.0019267089199274778\n",
            "[0m 33s] Epoch 136 [5120/8158] loss=0.0019455119676422329\n",
            "[0m 33s] Epoch 136 [7680/8158] loss=0.0019575870712287722\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7957868857533241\n",
            "[0m 34s] Epoch 137 [2560/8158] loss=0.0019002670887857676\n",
            "[0m 34s] Epoch 137 [5120/8158] loss=0.001943236612714827\n",
            "[0m 34s] Epoch 137 [7680/8158] loss=0.0019488069073607525\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.795831419841484\n",
            "[0m 34s] Epoch 138 [2560/8158] loss=0.0019627419766038655\n",
            "[0m 34s] Epoch 138 [5120/8158] loss=0.0019803248753305523\n",
            "[0m 34s] Epoch 138 [7680/8158] loss=0.0019502681020336847\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.795947966497732\n",
            "[0m 34s] Epoch 139 [2560/8158] loss=0.001944866869598627\n",
            "[0m 34s] Epoch 139 [5120/8158] loss=0.001959339895984158\n",
            "[0m 34s] Epoch 139 [7680/8158] loss=0.0019480334711261093\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7957837273073554\n",
            "[0m 34s] Epoch 140 [2560/8158] loss=0.0019210580736398696\n",
            "[0m 34s] Epoch 140 [5120/8158] loss=0.001927664381219074\n",
            "[0m 35s] Epoch 140 [7680/8158] loss=0.0019414767196091513\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7957963610912304\n",
            "[0m 35s] Epoch 141 [2560/8158] loss=0.0019419818185269833\n",
            "[0m 35s] Epoch 141 [5120/8158] loss=0.001968382246559486\n",
            "[0m 35s] Epoch 141 [7680/8158] loss=0.001948185245661686\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7958727954836751\n",
            "[0m 35s] Epoch 142 [2560/8158] loss=0.0019551446312107146\n",
            "[0m 35s] Epoch 142 [5120/8158] loss=0.0019420538563281297\n",
            "[0m 35s] Epoch 142 [7680/8158] loss=0.001938695840847989\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7958690053485123\n",
            "[0m 35s] Epoch 143 [2560/8158] loss=0.0019627095316536725\n",
            "[0m 35s] Epoch 143 [5120/8158] loss=0.0019385666004382075\n",
            "[0m 35s] Epoch 143 [7680/8158] loss=0.00193854501315703\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7957534062260552\n",
            "[0m 35s] Epoch 144 [2560/8158] loss=0.001960222597699612\n",
            "[0m 36s] Epoch 144 [5120/8158] loss=0.001928357657743618\n",
            "[0m 36s] Epoch 144 [7680/8158] loss=0.0019400238408707083\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7959302792003068\n",
            "[0m 36s] Epoch 145 [2560/8158] loss=0.001954675139859319\n",
            "[0m 36s] Epoch 145 [5120/8158] loss=0.0019588644441682844\n",
            "[0m 36s] Epoch 145 [7680/8158] loss=0.001944987615570426\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7957464576449238\n",
            "[0m 36s] Epoch 146 [2560/8158] loss=0.0019597261911258102\n",
            "[0m 36s] Epoch 146 [5120/8158] loss=0.0019347485445905476\n",
            "[0m 36s] Epoch 146 [7680/8158] loss=0.001934489949295918\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7958058364291366\n",
            "[0m 36s] Epoch 147 [2560/8158] loss=0.0018864061916247011\n",
            "[0m 36s] Epoch 147 [5120/8158] loss=0.0019298495724797248\n",
            "[0m 36s] Epoch 147 [7680/8158] loss=0.0019470291134590903\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7958348941320497\n",
            "[0m 37s] Epoch 148 [2560/8158] loss=0.0019704061560332775\n",
            "[0m 37s] Epoch 148 [5120/8158] loss=0.0019406325067393483\n",
            "[0m 37s] Epoch 148 [7680/8158] loss=0.0019353569912103315\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7958317356860808\n",
            "[0m 37s] Epoch 149 [2560/8158] loss=0.0019325175904668867\n",
            "[0m 37s] Epoch 149 [5120/8158] loss=0.0019494152395054698\n",
            "[0m 37s] Epoch 149 [7680/8158] loss=0.0019432921389428278\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7957710935234802\n",
            "[0m 37s] Epoch 150 [2560/8158] loss=0.0019973919959738852\n",
            "[0m 37s] Epoch 150 [5120/8158] loss=0.001959230884676799\n",
            "[0m 37s] Epoch 150 [7680/8158] loss=0.0019506321211035054\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.795738245685405\n",
            "[0m 37s] Epoch 151 [2560/8158] loss=0.0019611050258390604\n",
            "[0m 38s] Epoch 151 [5120/8158] loss=0.001956237986451015\n",
            "[0m 38s] Epoch 151 [7680/8158] loss=0.0019432701519690454\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7958045730507493\n",
            "[0m 38s] Epoch 152 [2560/8158] loss=0.001933483558241278\n",
            "[0m 38s] Epoch 152 [5120/8158] loss=0.0019183080061338843\n",
            "[0m 38s] Epoch 152 [7680/8158] loss=0.0019331784336827696\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7959700756195134\n",
            "[0m 38s] Epoch 153 [2560/8158] loss=0.001888006611261517\n",
            "[0m 38s] Epoch 153 [5120/8158] loss=0.0019187994010280817\n",
            "[0m 38s] Epoch 153 [7680/8158] loss=0.0019314786225246886\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.795955546768057\n",
            "[0m 38s] Epoch 154 [2560/8158] loss=0.00199706491548568\n",
            "[0m 38s] Epoch 154 [5120/8158] loss=0.0019657862780150027\n",
            "[0m 38s] Epoch 154 [7680/8158] loss=0.0019474541186355054\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.795887956024325\n",
            "[0m 38s] Epoch 155 [2560/8158] loss=0.0019185973447747528\n",
            "[0m 39s] Epoch 155 [5120/8158] loss=0.0019454898603726179\n",
            "[0m 39s] Epoch 155 [7680/8158] loss=0.0019449140216844776\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7958228920373682\n",
            "[0m 39s] Epoch 156 [2560/8158] loss=0.0019875413621775807\n",
            "[0m 39s] Epoch 156 [5120/8158] loss=0.001944220025325194\n",
            "[0m 39s] Epoch 156 [7680/8158] loss=0.00194384950834016\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7959612319708007\n",
            "[0m 39s] Epoch 157 [2560/8158] loss=0.0019049154361709952\n",
            "[0m 39s] Epoch 157 [5120/8158] loss=0.001916918042115867\n",
            "[0m 39s] Epoch 157 [7680/8158] loss=0.0019451269879937172\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7958121533210745\n",
            "[0m 39s] Epoch 158 [2560/8158] loss=0.001971624232828617\n",
            "[0m 39s] Epoch 158 [5120/8158] loss=0.001950088026933372\n",
            "[0m 39s] Epoch 158 [7680/8158] loss=0.0019402827058608333\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7959726023762884\n",
            "[0m 40s] Epoch 159 [2560/8158] loss=0.00190678023500368\n",
            "[0m 40s] Epoch 159 [5120/8158] loss=0.0019456863170489668\n",
            "[0m 40s] Epoch 159 [7680/8158] loss=0.0019426508030543725\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7959997650116198\n",
            "[0m 40s] Epoch 160 [2560/8158] loss=0.0019244616501964628\n",
            "[0m 40s] Epoch 160 [5120/8158] loss=0.0019562693545594812\n",
            "[0m 40s] Epoch 160 [7680/8158] loss=0.0019545802458499867\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7959871312277448\n",
            "[0m 40s] Epoch 161 [2560/8158] loss=0.0019555573584511877\n",
            "[0m 40s] Epoch 161 [5120/8158] loss=0.0019493513507768511\n",
            "[0m 40s] Epoch 161 [7680/8158] loss=0.0019449753531565269\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7960275593361451\n",
            "[0m 40s] Epoch 162 [2560/8158] loss=0.0019497289671562613\n",
            "[0m 40s] Epoch 162 [5120/8158] loss=0.0019424858328420669\n",
            "[0m 41s] Epoch 162 [7680/8158] loss=0.0019412623058694103\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7958194177468026\n",
            "[0m 41s] Epoch 163 [2560/8158] loss=0.0019930686219595374\n",
            "[0m 41s] Epoch 163 [5120/8158] loss=0.001959693257231265\n",
            "[0m 41s] Epoch 163 [7680/8158] loss=0.0019388143555261195\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7959498615653131\n",
            "[0m 41s] Epoch 164 [2560/8158] loss=0.001915295550134033\n",
            "[0m 41s] Epoch 164 [5120/8158] loss=0.0019148915365803988\n",
            "[0m 41s] Epoch 164 [7680/8158] loss=0.0019410274573601783\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7957976244696179\n",
            "[0m 41s] Epoch 165 [2560/8158] loss=0.001967597322072834\n",
            "[0m 41s] Epoch 165 [5120/8158] loss=0.001960128720384091\n",
            "[0m 41s] Epoch 165 [7680/8158] loss=0.0019417912854502598\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7960395614308264\n",
            "[0m 41s] Epoch 166 [2560/8158] loss=0.0019596651778556406\n",
            "[0m 42s] Epoch 166 [5120/8158] loss=0.0019422546669375152\n",
            "[0m 42s] Epoch 166 [7680/8158] loss=0.001940018602181226\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7960332445388891\n",
            "[0m 42s] Epoch 167 [2560/8158] loss=0.001916692149825394\n",
            "[0m 42s] Epoch 167 [5120/8158] loss=0.0019275906146503986\n",
            "[0m 42s] Epoch 167 [7680/8158] loss=0.0019442981186633308\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7957388773745986\n",
            "[0m 42s] Epoch 168 [2560/8158] loss=0.001926894357893616\n",
            "[0m 42s] Epoch 168 [5120/8158] loss=0.001939751545432955\n",
            "[0m 42s] Epoch 168 [7680/8158] loss=0.0019424062804318965\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7958266821725307\n",
            "[0m 42s] Epoch 169 [2560/8158] loss=0.0019348473171703518\n",
            "[0m 42s] Epoch 169 [5120/8158] loss=0.0019256543251685797\n",
            "[0m 42s] Epoch 169 [7680/8158] loss=0.0019416036006684105\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7961128373773023\n",
            "[0m 42s] Epoch 170 [2560/8158] loss=0.002019753574859351\n",
            "[0m 43s] Epoch 170 [5120/8158] loss=0.0019469342543743551\n",
            "[0m 43s] Epoch 170 [7680/8158] loss=0.0019459764395530026\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7958165751454307\n",
            "[0m 43s] Epoch 171 [2560/8158] loss=0.0019406125647947192\n",
            "[0m 43s] Epoch 171 [5120/8158] loss=0.0019313351658638567\n",
            "[0m 43s] Epoch 171 [7680/8158] loss=0.0019389820129921038\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7959062750109439\n",
            "[0m 43s] Epoch 172 [2560/8158] loss=0.0019481812720187008\n",
            "[0m 43s] Epoch 172 [5120/8158] loss=0.0019548109150491653\n",
            "[0m 43s] Epoch 172 [7680/8158] loss=0.0019291136685448387\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7960351396064703\n",
            "[0m 43s] Epoch 173 [2560/8158] loss=0.0020272506517358124\n",
            "[0m 43s] Epoch 173 [5120/8158] loss=0.001936741074314341\n",
            "[0m 43s] Epoch 173 [7680/8158] loss=0.0019414139717506866\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7958696370377062\n",
            "[0m 44s] Epoch 174 [2560/8158] loss=0.0019470612169243395\n",
            "[0m 44s] Epoch 174 [5120/8158] loss=0.001916593435453251\n",
            "[0m 44s] Epoch 174 [7680/8158] loss=0.001927434435735146\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7961336831206962\n",
            "[0m 44s] Epoch 175 [2560/8158] loss=0.00193721994291991\n",
            "[0m 44s] Epoch 175 [5120/8158] loss=0.0019487901241518556\n",
            "[0m 44s] Epoch 175 [7680/8158] loss=0.0019449727105287215\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7959846044709697\n",
            "[0m 44s] Epoch 176 [2560/8158] loss=0.0019804535433650015\n",
            "[0m 44s] Epoch 176 [5120/8158] loss=0.0019559323380235584\n",
            "[0m 44s] Epoch 176 [7680/8158] loss=0.0019458234348955254\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7960945183906833\n",
            "[0m 44s] Epoch 177 [2560/8158] loss=0.0019472661660984159\n",
            "[0m 44s] Epoch 177 [5120/8158] loss=0.0019326615438330919\n",
            "[0m 45s] Epoch 177 [7680/8158] loss=0.001929275846729676\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7961823231886155\n",
            "[0m 45s] Epoch 178 [2560/8158] loss=0.0019360957201570272\n",
            "[0m 45s] Epoch 178 [5120/8158] loss=0.001960482163121924\n",
            "[0m 45s] Epoch 178 [7680/8158] loss=0.0019425492345665893\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7959915530521011\n",
            "[0m 45s] Epoch 179 [2560/8158] loss=0.0019246898940764368\n",
            "[0m 45s] Epoch 179 [5120/8158] loss=0.001966965227620676\n",
            "[0m 45s] Epoch 179 [7680/8158] loss=0.0019371326159064969\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7961229444044026\n",
            "[0m 45s] Epoch 180 [2560/8158] loss=0.0019392776885069907\n",
            "[0m 45s] Epoch 180 [5120/8158] loss=0.0019481101306155324\n",
            "[0m 45s] Epoch 180 [7680/8158] loss=0.0019488880333180228\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7960155572414638\n",
            "[0m 45s] Epoch 181 [2560/8158] loss=0.0018818344920873641\n",
            "[0m 46s] Epoch 181 [5120/8158] loss=0.001914891047636047\n",
            "[0m 46s] Epoch 181 [7680/8158] loss=0.0019259256776422262\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7961513704181215\n",
            "[0m 46s] Epoch 182 [2560/8158] loss=0.0019774432643316687\n",
            "[0m 46s] Epoch 182 [5120/8158] loss=0.001944693858968094\n",
            "[0m 46s] Epoch 182 [7680/8158] loss=0.0019360283622518182\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7961684260263528\n",
            "[0m 46s] Epoch 183 [2560/8158] loss=0.0019238999229855835\n",
            "[0m 46s] Epoch 183 [5120/8158] loss=0.0019386612111702561\n",
            "[0m 46s] Epoch 183 [7680/8158] loss=0.0019390816295829913\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.795965653795157\n",
            "[0m 46s] Epoch 184 [2560/8158] loss=0.001947349973488599\n",
            "[0m 46s] Epoch 184 [5120/8158] loss=0.0019332605006638914\n",
            "[0m 46s] Epoch 184 [7680/8158] loss=0.001935912804522862\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7959820777141947\n",
            "[0m 46s] Epoch 185 [2560/8158] loss=0.0019213235122151674\n",
            "[0m 47s] Epoch 185 [5120/8158] loss=0.0019568023504689335\n",
            "[0m 47s] Epoch 185 [7680/8158] loss=0.0019401262514293193\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.796079989539227\n",
            "[0m 47s] Epoch 186 [2560/8158] loss=0.0019016538513824343\n",
            "[0m 47s] Epoch 186 [5120/8158] loss=0.0019275487051345409\n",
            "[0m 47s] Epoch 186 [7680/8158] loss=0.0019425816213091214\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7961381049450525\n",
            "[0m 47s] Epoch 187 [2560/8158] loss=0.0019506186479702591\n",
            "[0m 47s] Epoch 187 [5120/8158] loss=0.0019492129678837955\n",
            "[0m 47s] Epoch 187 [7680/8158] loss=0.0019361853560743232\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7961564239316714\n",
            "[0m 47s] Epoch 188 [2560/8158] loss=0.001959221495781094\n",
            "[0m 47s] Epoch 188 [5120/8158] loss=0.001937370334053412\n",
            "[0m 47s] Epoch 188 [7680/8158] loss=0.0019299522469130655\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7961804281210341\n",
            "[0m 48s] Epoch 189 [2560/8158] loss=0.0019039006321690977\n",
            "[0m 48s] Epoch 189 [5120/8158] loss=0.0019284264359157532\n",
            "[0m 48s] Epoch 189 [7680/8158] loss=0.0019331314989055196\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7961298929855336\n",
            "[0m 48s] Epoch 190 [2560/8158] loss=0.0019186868099495768\n",
            "[0m 48s] Epoch 190 [5120/8158] loss=0.0019309284223709255\n",
            "[0m 48s] Epoch 190 [7680/8158] loss=0.0019359747762791812\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.796064197309383\n",
            "[0m 48s] Epoch 191 [2560/8158] loss=0.0018482726300135255\n",
            "[0m 48s] Epoch 191 [5120/8158] loss=0.0018996494240127505\n",
            "[0m 48s] Epoch 191 [7680/8158] loss=0.0019388477046353122\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7961917985265218\n",
            "[0m 48s] Epoch 192 [2560/8158] loss=0.001943611039314419\n",
            "[0m 48s] Epoch 192 [5120/8158] loss=0.0019536586652975528\n",
            "[0m 48s] Epoch 192 [7680/8158] loss=0.0019470960871937374\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7962120125807219\n",
            "[0m 49s] Epoch 193 [2560/8158] loss=0.0019360825303010643\n",
            "[0m 49s] Epoch 193 [5120/8158] loss=0.0019206309225410223\n",
            "[0m 49s] Epoch 193 [7680/8158] loss=0.001940681366249919\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7962208562294346\n",
            "[0m 49s] Epoch 194 [2560/8158] loss=0.0019612145377323033\n",
            "[0m 49s] Epoch 194 [5120/8158] loss=0.0019491562794428318\n",
            "[0m 49s] Epoch 194 [7680/8158] loss=0.0019417627132497727\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7962435970404098\n",
            "[0m 49s] Epoch 195 [2560/8158] loss=0.0019172898842953145\n",
            "[0m 49s] Epoch 195 [5120/8158] loss=0.0019167653052136302\n",
            "[0m 49s] Epoch 195 [7680/8158] loss=0.0019289143422308067\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7960856747419708\n",
            "[0m 49s] Epoch 196 [2560/8158] loss=0.0019592653145082293\n",
            "[0m 49s] Epoch 196 [5120/8158] loss=0.0019256495870649815\n",
            "[0m 49s] Epoch 196 [7680/8158] loss=0.001944072952028364\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7961428426140057\n",
            "[0m 50s] Epoch 197 [2560/8158] loss=0.0019099620636552571\n",
            "[0m 50s] Epoch 197 [5120/8158] loss=0.001943658891832456\n",
            "[0m 50s] Epoch 197 [7680/8158] loss=0.0019414610772704084\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7960471417011515\n",
            "[0m 50s] Epoch 198 [2560/8158] loss=0.0018565436359494925\n",
            "[0m 50s] Epoch 198 [5120/8158] loss=0.0019088948029093444\n",
            "[0m 50s] Epoch 198 [7680/8158] loss=0.001931782947697987\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7961191542692398\n",
            "[0m 50s] Epoch 199 [2560/8158] loss=0.001990025816485286\n",
            "[0m 50s] Epoch 199 [5120/8158] loss=0.001950356806628406\n",
            "[0m 50s] Epoch 199 [7680/8158] loss=0.001938616558133314\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.796155160553284\n",
            "[0m 50s] Epoch 200 [2560/8158] loss=0.0019366435124538838\n",
            "[0m 50s] Epoch 200 [5120/8158] loss=0.0019203692383598537\n",
            "[0m 50s] Epoch 200 [7680/8158] loss=0.0019391381492217382\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7961027303502022\n",
            "[0m 51s] Epoch 201 [2560/8158] loss=0.0019468954298645258\n",
            "[0m 51s] Epoch 201 [5120/8158] loss=0.0019413914298638702\n",
            "[0m 51s] Epoch 201 [7680/8158] loss=0.00193917015955473\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7960237692009826\n",
            "[0m 51s] Epoch 202 [2560/8158] loss=0.0019695788505487144\n",
            "[0m 51s] Epoch 202 [5120/8158] loss=0.001928204030264169\n",
            "[0m 51s] Epoch 202 [7680/8158] loss=0.0019331997260451316\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7959618636599946\n",
            "[0m 51s] Epoch 203 [2560/8158] loss=0.0019314170349389315\n",
            "[0m 51s] Epoch 203 [5120/8158] loss=0.0019310582021716983\n",
            "[0m 51s] Epoch 203 [7680/8158] loss=0.0019320290147637328\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.796040824809214\n",
            "[0m 51s] Epoch 204 [2560/8158] loss=0.0019121662830002607\n",
            "[0m 52s] Epoch 204 [5120/8158] loss=0.0019503239134792238\n",
            "[0m 52s] Epoch 204 [7680/8158] loss=0.0019330650800839066\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7961324197423088\n",
            "[0m 52s] Epoch 205 [2560/8158] loss=0.0019594126264564694\n",
            "[0m 52s] Epoch 205 [5120/8158] loss=0.001965399750042707\n",
            "[0m 52s] Epoch 205 [7680/8158] loss=0.0019389448454603553\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.79614442183699\n",
            "[0m 52s] Epoch 206 [2560/8158] loss=0.0019357661250978709\n",
            "[0m 52s] Epoch 206 [5120/8158] loss=0.0019711005443241447\n",
            "[0m 52s] Epoch 206 [7680/8158] loss=0.0019471750090209146\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7959883946061324\n",
            "[0m 52s] Epoch 207 [2560/8158] loss=0.0019438644871115685\n",
            "[0m 52s] Epoch 207 [5120/8158] loss=0.0019440778181888164\n",
            "[0m 52s] Epoch 207 [7680/8158] loss=0.0019351129187271\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7959795509574198\n",
            "[0m 53s] Epoch 208 [2560/8158] loss=0.0019105174811556934\n",
            "[0m 53s] Epoch 208 [5120/8158] loss=0.0019445741723757236\n",
            "[0m 53s] Epoch 208 [7680/8158] loss=0.001942350040189922\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7960743043364832\n",
            "[0m 53s] Epoch 209 [2560/8158] loss=0.001975214690901339\n",
            "[0m 53s] Epoch 209 [5120/8158] loss=0.0019447305356152355\n",
            "[0m 53s] Epoch 209 [7680/8158] loss=0.0019323923625051975\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.796077462782452\n",
            "[0m 53s] Epoch 210 [2560/8158] loss=0.0019720287062227728\n",
            "[0m 53s] Epoch 210 [5120/8158] loss=0.0019502915325574576\n",
            "[0m 53s] Epoch 210 [7680/8158] loss=0.0019403247356725235\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7960379822078422\n",
            "[0m 53s] Epoch 211 [2560/8158] loss=0.001935348310507834\n",
            "[0m 53s] Epoch 211 [5120/8158] loss=0.001946307357866317\n",
            "[0m 54s] Epoch 211 [7680/8158] loss=0.0019290184563336274\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.795903748254169\n",
            "[0m 54s] Epoch 212 [2560/8158] loss=0.0019522455288097263\n",
            "[0m 54s] Epoch 212 [5120/8158] loss=0.0019492916471790522\n",
            "[0m 54s] Epoch 212 [7680/8158] loss=0.0019345250174713632\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.795986815383148\n",
            "[0m 54s] Epoch 213 [2560/8158] loss=0.0018764053005725146\n",
            "[0m 54s] Epoch 213 [5120/8158] loss=0.0019383774371817708\n",
            "[0m 54s] Epoch 213 [7680/8158] loss=0.0019435060598577063\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7959871312277447\n",
            "[0m 54s] Epoch 214 [2560/8158] loss=0.001923553750384599\n",
            "[0m 54s] Epoch 214 [5120/8158] loss=0.0019316149991936982\n",
            "[0m 54s] Epoch 214 [7680/8158] loss=0.0019349146246289213\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7958980630514253\n",
            "[0m 54s] Epoch 215 [2560/8158] loss=0.001967821060679853\n",
            "[0m 54s] Epoch 215 [5120/8158] loss=0.0019622675783466546\n",
            "[0m 55s] Epoch 215 [7680/8158] loss=0.0019372967071831226\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7959552309234601\n",
            "[0m 55s] Epoch 216 [2560/8158] loss=0.001967144210357219\n",
            "[0m 55s] Epoch 216 [5120/8158] loss=0.001949714304646477\n",
            "[0m 55s] Epoch 216 [7680/8158] loss=0.001931456154367576\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7960168206198512\n",
            "[0m 55s] Epoch 217 [2560/8158] loss=0.0019348974456079304\n",
            "[0m 55s] Epoch 217 [5120/8158] loss=0.001926011563045904\n",
            "[0m 55s] Epoch 217 [7680/8158] loss=0.0019402488755683104\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7960698825121268\n",
            "[0m 55s] Epoch 218 [2560/8158] loss=0.001920222968328744\n",
            "[0m 55s] Epoch 218 [5120/8158] loss=0.0019445643469225616\n",
            "[0m 55s] Epoch 218 [7680/8158] loss=0.0019411581995276113\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7958563715646373\n",
            "[0m 56s] Epoch 219 [2560/8158] loss=0.0020134780672378836\n",
            "[0m 56s] Epoch 219 [5120/8158] loss=0.001969122147420421\n",
            "[0m 56s] Epoch 219 [7680/8158] loss=0.0019355125919294854\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7958810074431938\n",
            "[0m 56s] Epoch 220 [2560/8158] loss=0.0019152804859913885\n",
            "[0m 56s] Epoch 220 [5120/8158] loss=0.0019380934012588114\n",
            "[0m 56s] Epoch 220 [7680/8158] loss=0.001932927364638696\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dev set: AUC  0.7957761470370303\n",
            "[0m 56s] Epoch 221 [2560/8158] loss=0.0019910816685296596\n",
            "[0m 56s] Epoch 221 [5120/8158] loss=0.0019218810193706304\n",
            "[0m 56s] Epoch 221 [7680/8158] loss=0.0019338241623093684\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7958266821725308\n",
            "[0m 57s] Epoch 222 [2560/8158] loss=0.0019496056833304466\n",
            "[0m 57s] Epoch 222 [5120/8158] loss=0.0019233906175941228\n",
            "[0m 57s] Epoch 222 [7680/8158] loss=0.0019369429326616227\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7960086086603325\n",
            "[0m 57s] Epoch 223 [2560/8158] loss=0.0019184428383596241\n",
            "[0m 57s] Epoch 223 [5120/8158] loss=0.0019423940568231047\n",
            "[0m 57s] Epoch 223 [7680/8158] loss=0.001932736001132677\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7958450011591498\n",
            "[0m 57s] Epoch 224 [2560/8158] loss=0.0018992576515302062\n",
            "[0m 57s] Epoch 224 [5120/8158] loss=0.0019152800436131657\n",
            "[0m 57s] Epoch 224 [7680/8158] loss=0.0019315392011776567\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7958645835241561\n",
            "[0m 57s] Epoch 225 [2560/8158] loss=0.00191662727156654\n",
            "[0m 57s] Epoch 225 [5120/8158] loss=0.0018880283983889966\n",
            "[0m 58s] Epoch 225 [7680/8158] loss=0.0019303900616553923\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.795701607712167\n",
            "[0m 58s] Epoch 226 [2560/8158] loss=0.0019669792847707866\n",
            "[0m 58s] Epoch 226 [5120/8158] loss=0.0019490870996378361\n",
            "[0m 58s] Epoch 226 [7680/8158] loss=0.0019319874273302654\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7959239623083693\n",
            "[0m 58s] Epoch 227 [2560/8158] loss=0.001922199851833284\n",
            "[0m 58s] Epoch 227 [5120/8158] loss=0.0019315310462843626\n",
            "[0m 58s] Epoch 227 [7680/8158] loss=0.001946462229049454\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.795740140752986\n",
            "[0m 58s] Epoch 228 [2560/8158] loss=0.001900467067025602\n",
            "[0m 58s] Epoch 228 [5120/8158] loss=0.001921132445568219\n",
            "[0m 58s] Epoch 228 [7680/8158] loss=0.0019299432596502205\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7958020462939742\n",
            "[0m 58s] Epoch 229 [2560/8158] loss=0.001884762616828084\n",
            "[0m 59s] Epoch 229 [5120/8158] loss=0.00190791129716672\n",
            "[0m 59s] Epoch 229 [7680/8158] loss=0.0019379216983603934\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7958898510919065\n",
            "[0m 59s] Epoch 230 [2560/8158] loss=0.0019314959296025335\n",
            "[0m 59s] Epoch 230 [5120/8158] loss=0.0019574098871089516\n",
            "[0m 59s] Epoch 230 [7680/8158] loss=0.0019395290136647722\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7956258050089163\n",
            "[0m 59s] Epoch 231 [2560/8158] loss=0.0019046336296014489\n",
            "[0m 59s] Epoch 231 [5120/8158] loss=0.001946658716769889\n",
            "[0m 59s] Epoch 231 [7680/8158] loss=0.001938162068836391\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7958191019022057\n",
            "[0m 59s] Epoch 232 [2560/8158] loss=0.0018943600007332861\n",
            "[0m 59s] Epoch 232 [5120/8158] loss=0.001953674020478502\n",
            "[0m 59s] Epoch 232 [7680/8158] loss=0.0019390782690607011\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7958525814294747\n",
            "[1m 0s] Epoch 233 [2560/8158] loss=0.0019174539833329619\n",
            "[1m 0s] Epoch 233 [5120/8158] loss=0.0019607332884334027\n",
            "[1m 0s] Epoch 233 [7680/8158] loss=0.0019384417062004407\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7958753222404499\n",
            "[1m 0s] Epoch 234 [2560/8158] loss=0.0019327783840708435\n",
            "[1m 0s] Epoch 234 [5120/8158] loss=0.0019452863314654678\n",
            "[1m 0s] Epoch 234 [7680/8158] loss=0.0019325769040733575\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7955879036572909\n",
            "[1m 0s] Epoch 235 [2560/8158] loss=0.001997715060133487\n",
            "[1m 0s] Epoch 235 [5120/8158] loss=0.0019477196678053589\n",
            "[1m 0s] Epoch 235 [7680/8158] loss=0.0019318408682011067\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7957432991989549\n",
            "[1m 0s] Epoch 236 [2560/8158] loss=0.001869750360492617\n",
            "[1m 1s] Epoch 236 [5120/8158] loss=0.0019057587604038417\n",
            "[1m 1s] Epoch 236 [7680/8158] loss=0.0019291950040496886\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2009/2658 75.58%\n",
            "Dev set: AUC  0.7958033096723618\n",
            "[1m 1s] Epoch 237 [2560/8158] loss=0.0019588358816690745\n",
            "[1m 1s] Epoch 237 [5120/8158] loss=0.0019256153085734696\n",
            "[1m 1s] Epoch 237 [7680/8158] loss=0.0019336316579331954\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7956535993334416\n",
            "[1m 1s] Epoch 238 [2560/8158] loss=0.0019467792939394713\n",
            "[1m 1s] Epoch 238 [5120/8158] loss=0.0019468540034722538\n",
            "[1m 1s] Epoch 238 [7680/8158] loss=0.0019360562165578207\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7956674964957042\n",
            "[1m 1s] Epoch 239 [2560/8158] loss=0.0019098216784186662\n",
            "[1m 1s] Epoch 239 [5120/8158] loss=0.0019363645813427865\n",
            "[1m 1s] Epoch 239 [7680/8158] loss=0.0019331646384671331\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7955512656840532\n",
            "[1m 2s] Epoch 240 [2560/8158] loss=0.0019437636015936731\n",
            "[1m 2s] Epoch 240 [5120/8158] loss=0.0019248372642323374\n",
            "[1m 2s] Epoch 240 [7680/8158] loss=0.0019343578955158592\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7956599162253791\n",
            "[1m 2s] Epoch 241 [2560/8158] loss=0.0018887316924519836\n",
            "[1m 2s] Epoch 241 [5120/8158] loss=0.0019266833318397404\n",
            "[1m 2s] Epoch 241 [7680/8158] loss=0.0019384660486442347\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7955304199406591\n",
            "[1m 2s] Epoch 242 [2560/8158] loss=0.0019354087417013943\n",
            "[1m 2s] Epoch 242 [5120/8158] loss=0.001934133074246347\n",
            "[1m 2s] Epoch 242 [7680/8158] loss=0.001936073589604348\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7956453873739228\n",
            "[1m 2s] Epoch 243 [2560/8158] loss=0.0019604676752351226\n",
            "[1m 3s] Epoch 243 [5120/8158] loss=0.0019554689060896635\n",
            "[1m 3s] Epoch 243 [7680/8158] loss=0.001938780890001605\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2010/2658 75.62%\n",
            "Dev set: AUC  0.7955323150082404\n",
            "[1m 3s] Epoch 244 [2560/8158] loss=0.0019260392524302005\n",
            "[1m 3s] Epoch 244 [5120/8158] loss=0.0019399259064812214\n",
            "[1m 3s] Epoch 244 [7680/8158] loss=0.0019317148330931862\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7955714797382534\n",
            "[1m 3s] Epoch 245 [2560/8158] loss=0.0019291391945444047\n",
            "[1m 3s] Epoch 245 [5120/8158] loss=0.0019359260448254644\n",
            "[1m 3s] Epoch 245 [7680/8158] loss=0.0019348534444967906\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2011/2658 75.66%\n",
            "Dev set: AUC  0.7956093810898787\n",
            "[1m 3s] Epoch 246 [2560/8158] loss=0.0019521106034517289\n",
            "[1m 3s] Epoch 246 [5120/8158] loss=0.0019442900724243374\n",
            "[1m 3s] Epoch 246 [7680/8158] loss=0.0019314755064745745\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7956542310226353\n",
            "[1m 4s] Epoch 247 [2560/8158] loss=0.0019248065771535039\n",
            "[1m 4s] Epoch 247 [5120/8158] loss=0.001932348485570401\n",
            "[1m 4s] Epoch 247 [7680/8158] loss=0.001929989061318338\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7955992740627785\n",
            "[1m 4s] Epoch 248 [2560/8158] loss=0.0019030007650144398\n",
            "[1m 4s] Epoch 248 [5120/8158] loss=0.0019195633591152727\n",
            "[1m 4s] Epoch 248 [7680/8158] loss=0.0019291210686787964\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dev set: AUC  0.795416715885783\n",
            "[1m 4s] Epoch 249 [2560/8158] loss=0.0019398514996282756\n",
            "[1m 4s] Epoch 249 [5120/8158] loss=0.0019248007505666465\n",
            "[1m 4s] Epoch 249 [7680/8158] loss=0.0019357521358566979\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7955361051434029\n",
            "[1m 4s] Epoch 250 [2560/8158] loss=0.0019603680935688316\n",
            "[1m 4s] Epoch 250 [5120/8158] loss=0.0019516199361532926\n",
            "[1m 4s] Epoch 250 [7680/8158] loss=0.001941408608884861\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7954880967646775\n",
            "[1m 5s] Epoch 251 [2560/8158] loss=0.001943121722433716\n",
            "[1m 5s] Epoch 251 [5120/8158] loss=0.0019287105591502041\n",
            "[1m 5s] Epoch 251 [7680/8158] loss=0.0019365924992598594\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7953516518988263\n",
            "[1m 5s] Epoch 252 [2560/8158] loss=0.001916562346741557\n",
            "[1m 5s] Epoch 252 [5120/8158] loss=0.0019301761116366834\n",
            "[1m 5s] Epoch 252 [7680/8158] loss=0.0019193514754685262\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7954966245687932\n",
            "[1m 5s] Epoch 253 [2560/8158] loss=0.0019602205604314802\n",
            "[1m 5s] Epoch 253 [5120/8158] loss=0.0019576897379010917\n",
            "[1m 5s] Epoch 253 [7680/8158] loss=0.0019393678715762992\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7955051523729089\n",
            "[1m 5s] Epoch 254 [2560/8158] loss=0.0019682989688590167\n",
            "[1m 5s] Epoch 254 [5120/8158] loss=0.0019258478831034153\n",
            "[1m 6s] Epoch 254 [7680/8158] loss=0.0019229894193510215\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7954432468319208\n",
            "[1m 6s] Epoch 255 [2560/8158] loss=0.0018919185269623995\n",
            "[1m 6s] Epoch 255 [5120/8158] loss=0.0019155804417096078\n",
            "[1m 6s] Epoch 255 [7680/8158] loss=0.0019314020949726303\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7955923254816472\n",
            "[1m 6s] Epoch 256 [2560/8158] loss=0.0019391058129258453\n",
            "[1m 6s] Epoch 256 [5120/8158] loss=0.0019396622374188155\n",
            "[1m 6s] Epoch 256 [7680/8158] loss=0.0019326546462252736\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7954982037917775\n",
            "[1m 6s] Epoch 257 [2560/8158] loss=0.0019428155617788434\n",
            "[1m 6s] Epoch 257 [5120/8158] loss=0.0019563359848689286\n",
            "[1m 6s] Epoch 257 [7680/8158] loss=0.0019384198589250446\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7953737610206076\n",
            "[1m 6s] Epoch 258 [2560/8158] loss=0.0019407502491958438\n",
            "[1m 7s] Epoch 258 [5120/8158] loss=0.0019224383344408126\n",
            "[1m 7s] Epoch 258 [7680/8158] loss=0.0019224508238645892\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.795341544871726\n",
            "[1m 7s] Epoch 259 [2560/8158] loss=0.0018812656286172568\n",
            "[1m 7s] Epoch 259 [5120/8158] loss=0.0019191664818208666\n",
            "[1m 7s] Epoch 259 [7680/8158] loss=0.0019316230434924364\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7954672510212836\n",
            "[1m 7s] Epoch 260 [2560/8158] loss=0.0019137849449180067\n",
            "[1m 7s] Epoch 260 [5120/8158] loss=0.0019218509027268738\n",
            "[1m 7s] Epoch 260 [7680/8158] loss=0.0019325697561725975\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2009/2658 75.58%\n",
            "Dev set: AUC  0.7955193653797684\n",
            "[1m 7s] Epoch 261 [2560/8158] loss=0.0019455926958471537\n",
            "[1m 7s] Epoch 261 [5120/8158] loss=0.0019285949703771621\n",
            "[1m 7s] Epoch 261 [7680/8158] loss=0.0019286629588653643\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7952417379791127\n",
            "[1m 7s] Epoch 262 [2560/8158] loss=0.0019729741499759255\n",
            "[1m 8s] Epoch 262 [5120/8158] loss=0.0019345537410117685\n",
            "[1m 8s] Epoch 262 [7680/8158] loss=0.0019246144918724894\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7952309992628187\n",
            "[1m 8s] Epoch 263 [2560/8158] loss=0.0019546792842447757\n",
            "[1m 8s] Epoch 263 [5120/8158] loss=0.0019589242234360426\n",
            "[1m 8s] Epoch 263 [7680/8158] loss=0.0019386281686214109\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dev set: AUC  0.7954457735886958\n",
            "[1m 8s] Epoch 264 [2560/8158] loss=0.0019490378443151712\n",
            "[1m 8s] Epoch 264 [5120/8158] loss=0.0019559790438506753\n",
            "[1m 8s] Epoch 264 [7680/8158] loss=0.001925970536346237\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7953219625067197\n",
            "[1m 8s] Epoch 265 [2560/8158] loss=0.0019817422376945614\n",
            "[1m 8s] Epoch 265 [5120/8158] loss=0.0019451930711511523\n",
            "[1m 8s] Epoch 265 [7680/8158] loss=0.0019368154151986042\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7954375616291769\n",
            "[1m 8s] Epoch 266 [2560/8158] loss=0.0019362690509296954\n",
            "[1m 9s] Epoch 266 [5120/8158] loss=0.0019339262566063554\n",
            "[1m 9s] Epoch 266 [7680/8158] loss=0.0019275920620808998\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7953327012230135\n",
            "[1m 9s] Epoch 267 [2560/8158] loss=0.0019319583429023624\n",
            "[1m 9s] Epoch 267 [5120/8158] loss=0.0019258585874922574\n",
            "[1m 9s] Epoch 267 [7680/8158] loss=0.0019259698262127738\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7956036958871349\n",
            "[1m 9s] Epoch 268 [2560/8158] loss=0.001985489181242883\n",
            "[1m 9s] Epoch 268 [5120/8158] loss=0.0019377919961698353\n",
            "[1m 9s] Epoch 268 [7680/8158] loss=0.0019268293826219937\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.795240474600725\n",
            "[1m 9s] Epoch 269 [2560/8158] loss=0.0019335713237524032\n",
            "[1m 9s] Epoch 269 [5120/8158] loss=0.0019133728463202714\n",
            "[1m 9s] Epoch 269 [7680/8158] loss=0.0019254336055989066\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7953712342638325\n",
            "[1m 10s] Epoch 270 [2560/8158] loss=0.0019429205683991313\n",
            "[1m 10s] Epoch 270 [5120/8158] loss=0.0019397016789298505\n",
            "[1m 10s] Epoch 270 [7680/8158] loss=0.001929688643819342\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7954748312916086\n",
            "[1m 10s] Epoch 271 [2560/8158] loss=0.0018970995908603073\n",
            "[1m 10s] Epoch 271 [5120/8158] loss=0.0019017739454284311\n",
            "[1m 10s] Epoch 271 [7680/8158] loss=0.0019113694472859303\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7954312447372395\n",
            "[1m 10s] Epoch 272 [2560/8158] loss=0.001942074194084853\n",
            "[1m 10s] Epoch 272 [5120/8158] loss=0.0019410656532272696\n",
            "[1m 10s] Epoch 272 [7680/8158] loss=0.0019278545863926412\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7955626360895406\n",
            "[1m 10s] Epoch 273 [2560/8158] loss=0.0019812437472864985\n",
            "[1m 10s] Epoch 273 [5120/8158] loss=0.001939278730424121\n",
            "[1m 10s] Epoch 273 [7680/8158] loss=0.001922640254876266\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7951495113568241\n",
            "[1m 11s] Epoch 274 [2560/8158] loss=0.0019311115378513933\n",
            "[1m 11s] Epoch 274 [5120/8158] loss=0.0019398400268983095\n",
            "[1m 11s] Epoch 274 [7680/8158] loss=0.0019315943975622455\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7955752698734158\n",
            "[1m 11s] Epoch 275 [2560/8158] loss=0.0019657544093206526\n",
            "[1m 11s] Epoch 275 [5120/8158] loss=0.0019410428823903203\n",
            "[1m 11s] Epoch 275 [7680/8158] loss=0.0019275627098977565\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7952970107835663\n",
            "[1m 11s] Epoch 276 [2560/8158] loss=0.0018989090691320597\n",
            "[1m 11s] Epoch 276 [5120/8158] loss=0.001912865327904001\n",
            "[1m 11s] Epoch 276 [7680/8158] loss=0.0019287653500214218\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7954836749403212\n",
            "[1m 11s] Epoch 277 [2560/8158] loss=0.001969543215818703\n",
            "[1m 11s] Epoch 277 [5120/8158] loss=0.001954817143268883\n",
            "[1m 11s] Epoch 277 [7680/8158] loss=0.0019353395017484823\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7953093287228445\n",
            "[1m 12s] Epoch 278 [2560/8158] loss=0.0018933632178232073\n",
            "[1m 12s] Epoch 278 [5120/8158] loss=0.0019011841097380966\n",
            "[1m 12s] Epoch 278 [7680/8158] loss=0.0019359068634609382\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7955323150082403\n",
            "[1m 12s] Epoch 279 [2560/8158] loss=0.0019204964162781834\n",
            "[1m 12s] Epoch 279 [5120/8158] loss=0.0019000359694473445\n",
            "[1m 12s] Epoch 279 [7680/8158] loss=0.0019317304172242681\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7954767263591899\n",
            "[1m 12s] Epoch 280 [2560/8158] loss=0.0019453782355412842\n",
            "[1m 12s] Epoch 280 [5120/8158] loss=0.0019109524379018693\n",
            "[1m 12s] Epoch 280 [7680/8158] loss=0.0019220970997897288\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7952044683166808\n",
            "[1m 12s] Epoch 281 [2560/8158] loss=0.0019146863487549126\n",
            "[1m 12s] Epoch 281 [5120/8158] loss=0.001930760219693184\n",
            "[1m 13s] Epoch 281 [7680/8158] loss=0.0019393922411836684\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7953819729801264\n",
            "[1m 13s] Epoch 282 [2560/8158] loss=0.0019226184114813805\n",
            "[1m 13s] Epoch 282 [5120/8158] loss=0.0019104652747046202\n",
            "[1m 13s] Epoch 282 [7680/8158] loss=0.001939108055861046\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7953497568312448\n",
            "[1m 13s] Epoch 283 [2560/8158] loss=0.002002971572801471\n",
            "[1m 13s] Epoch 283 [5120/8158] loss=0.0019220733316615223\n",
            "[1m 13s] Epoch 283 [7680/8158] loss=0.0019228728798528513\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7952764808847692\n",
            "[1m 13s] Epoch 284 [2560/8158] loss=0.001974527013953775\n",
            "[1m 13s] Epoch 284 [5120/8158] loss=0.0019470566825475545\n",
            "[1m 13s] Epoch 284 [7680/8158] loss=0.0019168366173592706\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7951893077760309\n",
            "[1m 14s] Epoch 285 [2560/8158] loss=0.0019501181202940642\n",
            "[1m 14s] Epoch 285 [5120/8158] loss=0.0019358523888513446\n",
            "[1m 14s] Epoch 285 [7680/8158] loss=0.0019260872892724972\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7956813936579668\n",
            "[1m 14s] Epoch 286 [2560/8158] loss=0.0019110357854515315\n",
            "[1m 14s] Epoch 286 [5120/8158] loss=0.0019255738705396652\n",
            "[1m 14s] Epoch 286 [7680/8158] loss=0.0019263507410263022\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7954767263591899\n",
            "[1m 14s] Epoch 287 [2560/8158] loss=0.0019292568555101751\n",
            "[1m 14s] Epoch 287 [5120/8158] loss=0.001952597248600796\n",
            "[1m 14s] Epoch 287 [7680/8158] loss=0.0019315709903215369\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7955064157512963\n",
            "[1m 14s] Epoch 288 [2560/8158] loss=0.001953793130815029\n",
            "[1m 15s] Epoch 288 [5120/8158] loss=0.0019423867051955312\n",
            "[1m 15s] Epoch 288 [7680/8158] loss=0.0019368453960244855\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7955487389272781\n",
            "[1m 15s] Epoch 289 [2560/8158] loss=0.0019427009392529726\n",
            "[1m 15s] Epoch 289 [5120/8158] loss=0.0019131576002109796\n",
            "[1m 15s] Epoch 289 [7680/8158] loss=0.001926035841461271\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7954798848051586\n",
            "[1m 15s] Epoch 290 [2560/8158] loss=0.0018872925429604948\n",
            "[1m 15s] Epoch 290 [5120/8158] loss=0.0019203669275157154\n",
            "[1m 15s] Epoch 290 [7680/8158] loss=0.0019301558573109408\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7951981514247434\n",
            "[1m 15s] Epoch 291 [2560/8158] loss=0.0019289530813694001\n",
            "[1m 15s] Epoch 291 [5120/8158] loss=0.0019445890793576837\n",
            "[1m 15s] Epoch 291 [7680/8158] loss=0.0019290165898079674\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7952265774384625\n",
            "[1m 15s] Epoch 292 [2560/8158] loss=0.0019277830026112497\n",
            "[1m 16s] Epoch 292 [5120/8158] loss=0.0019251776451710612\n",
            "[1m 16s] Epoch 292 [7680/8158] loss=0.0019318907211224238\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7953633381489107\n",
            "[1m 16s] Epoch 293 [2560/8158] loss=0.0019162789452821017\n",
            "[1m 16s] Epoch 293 [5120/8158] loss=0.0019222742004785687\n",
            "[1m 16s] Epoch 293 [7680/8158] loss=0.0019300999003462494\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7951438261540803\n",
            "[1m 16s] Epoch 294 [2560/8158] loss=0.0019032424781471491\n",
            "[1m 16s] Epoch 294 [5120/8158] loss=0.001897139853099361\n",
            "[1m 16s] Epoch 294 [7680/8158] loss=0.0019287951601048312\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7952827977767067\n",
            "[1m 16s] Epoch 295 [2560/8158] loss=0.0018919619033113122\n",
            "[1m 16s] Epoch 295 [5120/8158] loss=0.0019156984461005777\n",
            "[1m 16s] Epoch 295 [7680/8158] loss=0.0019201928633265197\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7954672510212835\n",
            "[1m 17s] Epoch 296 [2560/8158] loss=0.00192025953438133\n",
            "[1m 17s] Epoch 296 [5120/8158] loss=0.0019010513322427868\n",
            "[1m 17s] Epoch 296 [7680/8158] loss=0.0019250084723656377\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7951621451406992\n",
            "[1m 17s] Epoch 297 [2560/8158] loss=0.0018592699663713575\n",
            "[1m 17s] Epoch 297 [5120/8158] loss=0.0019066234002821148\n",
            "[1m 17s] Epoch 297 [7680/8158] loss=0.0019275456705751517\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dev set: AUC  0.795364917371895\n",
            "[1m 17s] Epoch 298 [2560/8158] loss=0.0019274274702183903\n",
            "[1m 17s] Epoch 298 [5120/8158] loss=0.001932082831626758\n",
            "[1m 17s] Epoch 298 [7680/8158] loss=0.001921197244276603\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7952019415599059\n",
            "[1m 17s] Epoch 299 [2560/8158] loss=0.0019175863824784755\n",
            "[1m 17s] Epoch 299 [5120/8158] loss=0.0019321964879054575\n",
            "[1m 18s] Epoch 299 [7680/8158] loss=0.001922591996844858\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.79509329101858\n",
            "[1m 18s] Epoch 300 [2560/8158] loss=0.0019525179755873977\n",
            "[1m 18s] Epoch 300 [5120/8158] loss=0.00195547608891502\n",
            "[1m 18s] Epoch 300 [7680/8158] loss=0.0019309121610907216\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7950724452751858\n",
            "[1m 18s] Epoch 301 [2560/8158] loss=0.001958322792779654\n",
            "[1m 18s] Epoch 301 [5120/8158] loss=0.001941212225938216\n",
            "[1m 18s] Epoch 301 [7680/8158] loss=0.0019298235420137644\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7951735155461869\n",
            "[1m 18s] Epoch 302 [2560/8158] loss=0.0018616499612107873\n",
            "[1m 18s] Epoch 302 [5120/8158] loss=0.001913238997804001\n",
            "[1m 18s] Epoch 302 [7680/8158] loss=0.0019193158096944293\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.795268900614444\n",
            "[1m 18s] Epoch 303 [2560/8158] loss=0.001919970626477152\n",
            "[1m 18s] Epoch 303 [5120/8158] loss=0.0019310825155116618\n",
            "[1m 19s] Epoch 303 [7680/8158] loss=0.0019324307873224218\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7953440716285011\n",
            "[1m 19s] Epoch 304 [2560/8158] loss=0.0019287058268673718\n",
            "[1m 19s] Epoch 304 [5120/8158] loss=0.0019344579661265016\n",
            "[1m 19s] Epoch 304 [7680/8158] loss=0.0019276374097292623\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7950977128429361\n",
            "[1m 19s] Epoch 305 [2560/8158] loss=0.001870247023180127\n",
            "[1m 19s] Epoch 305 [5120/8158] loss=0.0019078495330177247\n",
            "[1m 19s] Epoch 305 [7680/8158] loss=0.0019209873590928814\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7953636539935076\n",
            "[1m 19s] Epoch 306 [2560/8158] loss=0.0019136920454911888\n",
            "[1m 19s] Epoch 306 [5120/8158] loss=0.0019330257666297257\n",
            "[1m 19s] Epoch 306 [7680/8158] loss=0.0019251172780059278\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7953150139255882\n",
            "[1m 19s] Epoch 307 [2560/8158] loss=0.0019019164144992828\n",
            "[1m 19s] Epoch 307 [5120/8158] loss=0.0018975406885147095\n",
            "[1m 20s] Epoch 307 [7680/8158] loss=0.0019249724689871072\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7951141367619737\n",
            "[1m 20s] Epoch 308 [2560/8158] loss=0.0018866716884076596\n",
            "[1m 20s] Epoch 308 [5120/8158] loss=0.0019111771835014224\n",
            "[1m 20s] Epoch 308 [7680/8158] loss=0.0019165193972488245\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7952543717629876\n",
            "[1m 20s] Epoch 309 [2560/8158] loss=0.001929669629316777\n",
            "[1m 20s] Epoch 309 [5120/8158] loss=0.0019207503530196846\n",
            "[1m 20s] Epoch 309 [7680/8158] loss=0.0019347262025500337\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7952587935873439\n",
            "[1m 20s] Epoch 310 [2560/8158] loss=0.0019421614473685623\n",
            "[1m 20s] Epoch 310 [5120/8158] loss=0.0019495291053317487\n",
            "[1m 20s] Epoch 310 [7680/8158] loss=0.0019241590790140132\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.795315645614782\n",
            "[1m 20s] Epoch 311 [2560/8158] loss=0.0018900497001595796\n",
            "[1m 20s] Epoch 311 [5120/8158] loss=0.0019496265274938196\n",
            "[1m 21s] Epoch 311 [7680/8158] loss=0.0019311366874414186\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.794880411760284\n",
            "[1m 21s] Epoch 312 [2560/8158] loss=0.0019349141628481447\n",
            "[1m 21s] Epoch 312 [5120/8158] loss=0.0019152339024003594\n",
            "[1m 21s] Epoch 312 [7680/8158] loss=0.001924692210741341\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7952430013575\n",
            "[1m 21s] Epoch 313 [2560/8158] loss=0.0019430208718404173\n",
            "[1m 21s] Epoch 313 [5120/8158] loss=0.0019440294941887259\n",
            "[1m 21s] Epoch 313 [7680/8158] loss=0.0019264823019814988\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.79497895527451\n",
            "[1m 21s] Epoch 314 [2560/8158] loss=0.001952710119076073\n",
            "[1m 21s] Epoch 314 [5120/8158] loss=0.0019071549235377462\n",
            "[1m 21s] Epoch 314 [7680/8158] loss=0.0019249841027582685\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7951637243636835\n",
            "[1m 21s] Epoch 315 [2560/8158] loss=0.0019261269248090685\n",
            "[1m 22s] Epoch 315 [5120/8158] loss=0.001938739395700395\n",
            "[1m 22s] Epoch 315 [7680/8158] loss=0.0019226936545843878\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7949353687201409\n",
            "[1m 22s] Epoch 316 [2560/8158] loss=0.0018901946139521896\n",
            "[1m 22s] Epoch 316 [5120/8158] loss=0.001899449760094285\n",
            "[1m 22s] Epoch 316 [7680/8158] loss=0.0019162806333042682\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7954306130480456\n",
            "[1m 22s] Epoch 317 [2560/8158] loss=0.001889313547872007\n",
            "[1m 22s] Epoch 317 [5120/8158] loss=0.0019164777244441212\n",
            "[1m 22s] Epoch 317 [7680/8158] loss=0.0019192646684435507\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7951084515592299\n",
            "[1m 22s] Epoch 318 [2560/8158] loss=0.0019313654629513621\n",
            "[1m 22s] Epoch 318 [5120/8158] loss=0.0019150622945744544\n",
            "[1m 22s] Epoch 318 [7680/8158] loss=0.0019195189389089743\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7951463529108554\n",
            "[1m 22s] Epoch 319 [2560/8158] loss=0.001954677794128656\n",
            "[1m 23s] Epoch 319 [5120/8158] loss=0.0019596137513872235\n",
            "[1m 23s] Epoch 319 [7680/8158] loss=0.0019259879326758286\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.794872831489959\n",
            "[1m 23s] Epoch 320 [2560/8158] loss=0.0019129698281176387\n",
            "[1m 23s] Epoch 320 [5120/8158] loss=0.0019088034809101374\n",
            "[1m 23s] Epoch 320 [7680/8158] loss=0.001922343981762727\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.794603100204225\n",
            "[1m 23s] Epoch 321 [2560/8158] loss=0.00192501584533602\n",
            "[1m 23s] Epoch 321 [5120/8158] loss=0.001887470780638978\n",
            "[1m 23s] Epoch 321 [7680/8158] loss=0.0019187210282931725\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7951943612895809\n",
            "[1m 23s] Epoch 322 [2560/8158] loss=0.0019394498085603118\n",
            "[1m 23s] Epoch 322 [5120/8158] loss=0.0019300046958960592\n",
            "[1m 23s] Epoch 322 [7680/8158] loss=0.0019317512555668752\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7949347370309471\n",
            "[1m 23s] Epoch 323 [2560/8158] loss=0.0019629180314950645\n",
            "[1m 24s] Epoch 323 [5120/8158] loss=0.0019550814933609216\n",
            "[1m 24s] Epoch 323 [7680/8158] loss=0.001935300650075078\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7947793414892831\n",
            "[1m 24s] Epoch 324 [2560/8158] loss=0.0018882141332142055\n",
            "[1m 24s] Epoch 324 [5120/8158] loss=0.001910461240913719\n",
            "[1m 24s] Epoch 324 [7680/8158] loss=0.0019189439287098746\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7949972742611289\n",
            "[1m 24s] Epoch 325 [2560/8158] loss=0.0019201325485482812\n",
            "[1m 24s] Epoch 325 [5120/8158] loss=0.0019081670965533704\n",
            "[1m 24s] Epoch 325 [7680/8158] loss=0.0019336816195088128\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7950996079105175\n",
            "[1m 24s] Epoch 326 [2560/8158] loss=0.0019082170561887323\n",
            "[1m 24s] Epoch 326 [5120/8158] loss=0.0019248821656219662\n",
            "[1m 24s] Epoch 326 [7680/8158] loss=0.0019230311969295143\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7952707956820253\n",
            "[1m 25s] Epoch 327 [2560/8158] loss=0.001991241401992738\n",
            "[1m 25s] Epoch 327 [5120/8158] loss=0.0019485972705297172\n",
            "[1m 25s] Epoch 327 [7680/8158] loss=0.001927234026758621\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7949448440580472\n",
            "[1m 25s] Epoch 328 [2560/8158] loss=0.0019920422462746503\n",
            "[1m 25s] Epoch 328 [5120/8158] loss=0.001959897519554943\n",
            "[1m 25s] Epoch 328 [7680/8158] loss=0.0019322046544402837\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7948816751386716\n",
            "[1m 25s] Epoch 329 [2560/8158] loss=0.0018832466332241893\n",
            "[1m 25s] Epoch 329 [5120/8158] loss=0.0019095872528851032\n",
            "[1m 25s] Epoch 329 [7680/8158] loss=0.001917428313754499\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7947824999352519\n",
            "[1m 25s] Epoch 330 [2560/8158] loss=0.0018980221007950603\n",
            "[1m 26s] Epoch 330 [5120/8158] loss=0.0019445821933913976\n",
            "[1m 26s] Epoch 330 [7680/8158] loss=0.0019354611130741736\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7949461074364347\n",
            "[1m 26s] Epoch 331 [2560/8158] loss=0.0019194218679331244\n",
            "[1m 26s] Epoch 331 [5120/8158] loss=0.0018948369252029806\n",
            "[1m 26s] Epoch 331 [7680/8158] loss=0.0019284196314401925\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7951274022350427\n",
            "[1m 26s] Epoch 332 [2560/8158] loss=0.0019456042209640145\n",
            "[1m 26s] Epoch 332 [5120/8158] loss=0.0019712697714567183\n",
            "[1m 26s] Epoch 332 [7680/8158] loss=0.0019371606215524177\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7945531967579184\n",
            "[1m 26s] Epoch 333 [2560/8158] loss=0.001929718384053558\n",
            "[1m 26s] Epoch 333 [5120/8158] loss=0.0019383688864763825\n",
            "[1m 26s] Epoch 333 [7680/8158] loss=0.0019319291963862876\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7949322102741719\n",
            "[1m 27s] Epoch 334 [2560/8158] loss=0.0019007045309990644\n",
            "[1m 27s] Epoch 334 [5120/8158] loss=0.0019164837256539613\n",
            "[1m 27s] Epoch 334 [7680/8158] loss=0.0019328094747227927\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7949833770988662\n",
            "[1m 27s] Epoch 335 [2560/8158] loss=0.0019255852093920112\n",
            "[1m 27s] Epoch 335 [5120/8158] loss=0.001936475624097511\n",
            "[1m 27s] Epoch 335 [7680/8158] loss=0.0019243928642633061\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7951255071674614\n",
            "[1m 27s] Epoch 336 [2560/8158] loss=0.0018941442016512156\n",
            "[1m 27s] Epoch 336 [5120/8158] loss=0.001934389246162027\n",
            "[1m 27s] Epoch 336 [7680/8158] loss=0.001920693530701101\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7945026616224179\n",
            "[1m 27s] Epoch 337 [2560/8158] loss=0.0019500982132740318\n",
            "[1m 27s] Epoch 337 [5120/8158] loss=0.0019580926978960632\n",
            "[1m 27s] Epoch 337 [7680/8158] loss=0.0019245207852994402\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7947307014213639\n",
            "[1m 28s] Epoch 338 [2560/8158] loss=0.0019202081486582756\n",
            "[1m 28s] Epoch 338 [5120/8158] loss=0.0019310851697809995\n",
            "[1m 28s] Epoch 338 [7680/8158] loss=0.0019174196951401731\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7946814296642509\n",
            "[1m 28s] Epoch 339 [2560/8158] loss=0.0019544188166037203\n",
            "[1m 28s] Epoch 339 [5120/8158] loss=0.0019305347756016999\n",
            "[1m 28s] Epoch 339 [7680/8158] loss=0.0019249675679020584\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7951286656134304\n",
            "[1m 28s] Epoch 340 [2560/8158] loss=0.0019461783347651362\n",
            "[1m 28s] Epoch 340 [5120/8158] loss=0.001912248961161822\n",
            "[1m 28s] Epoch 340 [7680/8158] loss=0.0019150417104053\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7946896416237698\n",
            "[1m 28s] Epoch 341 [2560/8158] loss=0.0019044305197894573\n",
            "[1m 28s] Epoch 341 [5120/8158] loss=0.0019306299393065274\n",
            "[1m 28s] Epoch 341 [7680/8158] loss=0.0019192438727865617\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7947471253404017\n",
            "[1m 29s] Epoch 342 [2560/8158] loss=0.0018788035260513425\n",
            "[1m 29s] Epoch 342 [5120/8158] loss=0.0019260152883362025\n",
            "[1m 29s] Epoch 342 [7680/8158] loss=0.0019234712546070418\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7948399836518836\n",
            "[1m 29s] Epoch 343 [2560/8158] loss=0.0019481689785607159\n",
            "[1m 29s] Epoch 343 [5120/8158] loss=0.0019387098320294172\n",
            "[1m 29s] Epoch 343 [7680/8158] loss=0.0019222096540033816\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7949498975715972\n",
            "[1m 29s] Epoch 344 [2560/8158] loss=0.0018822061130777002\n",
            "[1m 29s] Epoch 344 [5120/8158] loss=0.0019185751618351788\n",
            "[1m 29s] Epoch 344 [7680/8158] loss=0.001927459748306622\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.794766707705408\n",
            "[1m 29s] Epoch 345 [2560/8158] loss=0.0019308152375742793\n",
            "[1m 29s] Epoch 345 [5120/8158] loss=0.001912973413709551\n",
            "[1m 30s] Epoch 345 [7680/8158] loss=0.001926908785632501\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7947635492594392\n",
            "[1m 30s] Epoch 346 [2560/8158] loss=0.001944917580112815\n",
            "[1m 30s] Epoch 346 [5120/8158] loss=0.0019331114483065904\n",
            "[1m 30s] Epoch 346 [7680/8158] loss=0.0019250493380241096\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7944053814865795\n",
            "[1m 30s] Epoch 347 [2560/8158] loss=0.0018627520301379263\n",
            "[1m 30s] Epoch 347 [5120/8158] loss=0.0018903506104834377\n",
            "[1m 30s] Epoch 347 [7680/8158] loss=0.0019142998460059365\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7946833247318321\n",
            "[1m 30s] Epoch 348 [2560/8158] loss=0.0018826530198566616\n",
            "[1m 30s] Epoch 348 [5120/8158] loss=0.0019086939690168947\n",
            "[1m 30s] Epoch 348 [7680/8158] loss=0.0019220934521096448\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.794903784260453\n",
            "[1m 30s] Epoch 349 [2560/8158] loss=0.0018865425372496247\n",
            "[1m 30s] Epoch 349 [5120/8158] loss=0.0019165640114806592\n",
            "[1m 31s] Epoch 349 [7680/8158] loss=0.001913925811337928\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7945734108121186\n",
            "[1m 31s] Epoch 350 [2560/8158] loss=0.0019412821042351426\n",
            "[1m 31s] Epoch 350 [5120/8158] loss=0.001924015360418707\n",
            "[1m 31s] Epoch 350 [7680/8158] loss=0.0019238526583649218\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7948481956114024\n",
            "[1m 31s] Epoch 351 [2560/8158] loss=0.0019213663646951318\n",
            "[1m 31s] Epoch 351 [5120/8158] loss=0.0019222751492634416\n",
            "[1m 31s] Epoch 351 [7680/8158] loss=0.0019194235443137585\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7947054338536136\n",
            "[1m 31s] Epoch 352 [2560/8158] loss=0.0019342750427313149\n",
            "[1m 31s] Epoch 352 [5120/8158] loss=0.0019410597509704531\n",
            "[1m 31s] Epoch 352 [7680/8158] loss=0.0019246369833126665\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7951166635187489\n",
            "[1m 31s] Epoch 353 [2560/8158] loss=0.0018939338740892708\n",
            "[1m 31s] Epoch 353 [5120/8158] loss=0.0018875667767133564\n",
            "[1m 32s] Epoch 353 [7680/8158] loss=0.0019181708921678364\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7949656898014412\n",
            "[1m 32s] Epoch 354 [2560/8158] loss=0.0019241286325268447\n",
            "[1m 32s] Epoch 354 [5120/8158] loss=0.0019089899782557041\n",
            "[1m 32s] Epoch 354 [7680/8158] loss=0.0019298889596636097\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7945917297987376\n",
            "[1m 32s] Epoch 355 [2560/8158] loss=0.0019346808549016714\n",
            "[1m 32s] Epoch 355 [5120/8158] loss=0.0018952755897771567\n",
            "[1m 32s] Epoch 355 [7680/8158] loss=0.0019137490773573518\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.794359899864629\n",
            "[1m 32s] Epoch 356 [2560/8158] loss=0.0019225407275371253\n",
            "[1m 32s] Epoch 356 [5120/8158] loss=0.0019038159982301295\n",
            "[1m 32s] Epoch 356 [7680/8158] loss=0.0019253762749334177\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.794587939663575\n",
            "[1m 32s] Epoch 357 [2560/8158] loss=0.0019374660216271878\n",
            "[1m 33s] Epoch 357 [5120/8158] loss=0.001949737052200362\n",
            "[1m 33s] Epoch 357 [7680/8158] loss=0.001929312851279974\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7940295264162947\n",
            "[1m 33s] Epoch 358 [2560/8158] loss=0.001879929390270263\n",
            "[1m 33s] Epoch 358 [5120/8158] loss=0.0019363856874406338\n",
            "[1m 33s] Epoch 358 [7680/8158] loss=0.0019231256213970483\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7944312807435234\n",
            "[1m 33s] Epoch 359 [2560/8158] loss=0.0018879085779190063\n",
            "[1m 33s] Epoch 359 [5120/8158] loss=0.0019024759472813456\n",
            "[1m 33s] Epoch 359 [7680/8158] loss=0.0019151408147687714\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7947269112862014\n",
            "[1m 33s] Epoch 360 [2560/8158] loss=0.0018700367538258434\n",
            "[1m 33s] Epoch 360 [5120/8158] loss=0.0018878638045862317\n",
            "[1m 33s] Epoch 360 [7680/8158] loss=0.001908378469912956\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7944489680409487\n",
            "[1m 34s] Epoch 361 [2560/8158] loss=0.001947349461261183\n",
            "[1m 34s] Epoch 361 [5120/8158] loss=0.0019464214681647718\n",
            "[1m 34s] Epoch 361 [7680/8158] loss=0.0019258735002949834\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7945689889877623\n",
            "[1m 34s] Epoch 362 [2560/8158] loss=0.0019531854311935605\n",
            "[1m 34s] Epoch 362 [5120/8158] loss=0.0019109778921119868\n",
            "[1m 34s] Epoch 362 [7680/8158] loss=0.0019158519494036834\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7943719019593104\n",
            "[1m 34s] Epoch 363 [2560/8158] loss=0.0018978667329065501\n",
            "[1m 34s] Epoch 363 [5120/8158] loss=0.0019234151521231979\n",
            "[1m 34s] Epoch 363 [7680/8158] loss=0.0019232067706373831\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.794805240746227\n",
            "[1m 34s] Epoch 364 [2560/8158] loss=0.0019128773477859796\n",
            "[1m 34s] Epoch 364 [5120/8158] loss=0.0019161513191647828\n",
            "[1m 34s] Epoch 364 [7680/8158] loss=0.0019278013108608623\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7947787098000892\n",
            "[1m 35s] Epoch 365 [2560/8158] loss=0.0019114382099360227\n",
            "[1m 35s] Epoch 365 [5120/8158] loss=0.001920654100831598\n",
            "[1m 35s] Epoch 365 [7680/8158] loss=0.0019270399546561141\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7947313331105577\n",
            "[1m 35s] Epoch 366 [2560/8158] loss=0.0019296084996312858\n",
            "[1m 35s] Epoch 366 [5120/8158] loss=0.001929676637519151\n",
            "[1m 35s] Epoch 366 [7680/8158] loss=0.0019319422465438644\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.794649845204563\n",
            "[1m 35s] Epoch 367 [2560/8158] loss=0.0019691769382916393\n",
            "[1m 35s] Epoch 367 [5120/8158] loss=0.001953779446193948\n",
            "[1m 35s] Epoch 367 [7680/8158] loss=0.0019270931913827857\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7944486521963517\n",
            "[1m 36s] Epoch 368 [2560/8158] loss=0.0019400080549530686\n",
            "[1m 36s] Epoch 368 [5120/8158] loss=0.0019545888353604823\n",
            "[1m 36s] Epoch 368 [7680/8158] loss=0.0019353285315446556\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7946372114206879\n",
            "[1m 36s] Epoch 369 [2560/8158] loss=0.0019282942404970527\n",
            "[1m 36s] Epoch 369 [5120/8158] loss=0.0019091677502728998\n",
            "[1m 36s] Epoch 369 [7680/8158] loss=0.0019182985299266875\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7943586364862414\n",
            "[1m 36s] Epoch 370 [2560/8158] loss=0.001931460911873728\n",
            "[1m 36s] Epoch 370 [5120/8158] loss=0.0019156030553858726\n",
            "[1m 36s] Epoch 370 [7680/8158] loss=0.0019232436626528701\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7947300697321702\n",
            "[1m 36s] Epoch 371 [2560/8158] loss=0.0019302717875689269\n",
            "[1m 36s] Epoch 371 [5120/8158] loss=0.0019406540784984827\n",
            "[1m 37s] Epoch 371 [7680/8158] loss=0.0019305985149306556\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7944773940546677\n",
            "[1m 37s] Epoch 372 [2560/8158] loss=0.0019676112802699207\n",
            "[1m 37s] Epoch 372 [5120/8158] loss=0.0019335908757057041\n",
            "[1m 37s] Epoch 372 [7680/8158] loss=0.0019354085127512614\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7942853605397657\n",
            "[1m 37s] Epoch 373 [2560/8158] loss=0.0018570680753327907\n",
            "[1m 37s] Epoch 373 [5120/8158] loss=0.0019208397308830172\n",
            "[1m 37s] Epoch 373 [7680/8158] loss=0.001931168445541213\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.794745861962014\n",
            "[1m 37s] Epoch 374 [2560/8158] loss=0.001957391609903425\n",
            "[1m 37s] Epoch 374 [5120/8158] loss=0.0019441178883425891\n",
            "[1m 37s] Epoch 374 [7680/8158] loss=0.0019277206001182398\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7944129617569046\n",
            "[1m 37s] Epoch 375 [2560/8158] loss=0.001918534387368709\n",
            "[1m 38s] Epoch 375 [5120/8158] loss=0.0019242555077653377\n",
            "[1m 38s] Epoch 375 [7680/8158] loss=0.0019189487948703269\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7941144886128546\n",
            "[1m 38s] Epoch 376 [2560/8158] loss=0.001922218524850905\n",
            "[1m 38s] Epoch 376 [5120/8158] loss=0.0019182294723577797\n",
            "[1m 38s] Epoch 376 [7680/8158] loss=0.0019308117994417747\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7946454233802069\n",
            "[1m 38s] Epoch 377 [2560/8158] loss=0.001940146554261446\n",
            "[1m 38s] Epoch 377 [5120/8158] loss=0.0019276118197012692\n",
            "[1m 38s] Epoch 377 [7680/8158] loss=0.0019210490436914066\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7943472660807538\n",
            "[1m 38s] Epoch 378 [2560/8158] loss=0.00192431554896757\n",
            "[1m 38s] Epoch 378 [5120/8158] loss=0.0019416863564401866\n",
            "[1m 38s] Epoch 378 [7680/8158] loss=0.001922180864494294\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7943984329054481\n",
            "[1m 38s] Epoch 379 [2560/8158] loss=0.001922208140604198\n",
            "[1m 39s] Epoch 379 [5120/8158] loss=0.0019023118249606342\n",
            "[1m 39s] Epoch 379 [7680/8158] loss=0.001916773555179437\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7943125231750974\n",
            "[1m 39s] Epoch 380 [2560/8158] loss=0.0019385266583412885\n",
            "[1m 39s] Epoch 380 [5120/8158] loss=0.0019289752875920386\n",
            "[1m 39s] Epoch 380 [7680/8158] loss=0.001927904326779147\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7944325441219111\n",
            "[1m 39s] Epoch 381 [2560/8158] loss=0.0019295971491374076\n",
            "[1m 39s] Epoch 381 [5120/8158] loss=0.0019384352606721222\n",
            "[1m 39s] Epoch 381 [7680/8158] loss=0.001918475228982667\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7944856060141865\n",
            "[1m 39s] Epoch 382 [2560/8158] loss=0.0019437110051512719\n",
            "[1m 39s] Epoch 382 [5120/8158] loss=0.0019246021809522062\n",
            "[1m 39s] Epoch 382 [7680/8158] loss=0.001907852734439075\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7942506176341091\n",
            "[1m 39s] Epoch 383 [2560/8158] loss=0.0019204603624530137\n",
            "[1m 40s] Epoch 383 [5120/8158] loss=0.0019067562418058515\n",
            "[1m 40s] Epoch 383 [7680/8158] loss=0.0019208786427043379\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7943482136145446\n",
            "[1m 40s] Epoch 384 [2560/8158] loss=0.0019192242645658553\n",
            "[1m 40s] Epoch 384 [5120/8158] loss=0.0019365359912626445\n",
            "[1m 40s] Epoch 384 [7680/8158] loss=0.001918734156060964\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7943248411143756\n",
            "[1m 40s] Epoch 385 [2560/8158] loss=0.0019160480471327901\n",
            "[1m 40s] Epoch 385 [5120/8158] loss=0.001910417212639004\n",
            "[1m 40s] Epoch 385 [7680/8158] loss=0.001923667829638968\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7945576185822746\n",
            "[1m 40s] Epoch 386 [2560/8158] loss=0.0019191791885532438\n",
            "[1m 40s] Epoch 386 [5120/8158] loss=0.0019035974983125925\n",
            "[1m 40s] Epoch 386 [7680/8158] loss=0.0019107019257110854\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7941299649981017\n",
            "[1m 41s] Epoch 387 [2560/8158] loss=0.0019012184930033982\n",
            "[1m 41s] Epoch 387 [5120/8158] loss=0.0019254583748988807\n",
            "[1m 41s] Epoch 387 [7680/8158] loss=0.0019231394049711525\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7940269996595195\n",
            "[1m 41s] Epoch 388 [2560/8158] loss=0.001873853849247098\n",
            "[1m 41s] Epoch 388 [5120/8158] loss=0.0019160632276907564\n",
            "[1m 41s] Epoch 388 [7680/8158] loss=0.0019184040604159236\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7942537760800781\n",
            "[1m 41s] Epoch 389 [2560/8158] loss=0.0018830920336768032\n",
            "[1m 41s] Epoch 389 [5120/8158] loss=0.0019174327608197927\n",
            "[1m 41s] Epoch 389 [7680/8158] loss=0.0019336055614985526\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7942537760800781\n",
            "[1m 41s] Epoch 390 [2560/8158] loss=0.0019405087223276496\n",
            "[1m 41s] Epoch 390 [5120/8158] loss=0.001938270911341533\n",
            "[1m 41s] Epoch 390 [7680/8158] loss=0.0019190724318226178\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7936467227648782\n",
            "[1m 42s] Epoch 391 [2560/8158] loss=0.0019342236104421318\n",
            "[1m 42s] Epoch 391 [5120/8158] loss=0.0019251380290370435\n",
            "[1m 42s] Epoch 391 [7680/8158] loss=0.0019231613492593168\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7942664098639531\n",
            "[1m 42s] Epoch 392 [2560/8158] loss=0.0019322508480399847\n",
            "[1m 42s] Epoch 392 [5120/8158] loss=0.0018860005948226899\n",
            "[1m 42s] Epoch 392 [7680/8158] loss=0.0019180815666913985\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7943630583105978\n",
            "[1m 42s] Epoch 393 [2560/8158] loss=0.001900927198585123\n",
            "[1m 42s] Epoch 393 [5120/8158] loss=0.0019148532650433482\n",
            "[1m 42s] Epoch 393 [7680/8158] loss=0.0019190253224223852\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7941432304711707\n",
            "[1m 42s] Epoch 394 [2560/8158] loss=0.001979495049454272\n",
            "[1m 42s] Epoch 394 [5120/8158] loss=0.0019493484171107412\n",
            "[1m 42s] Epoch 394 [7680/8158] loss=0.0019263890688307583\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7939878349295066\n",
            "[1m 43s] Epoch 395 [2560/8158] loss=0.0019348007044754922\n",
            "[1m 43s] Epoch 395 [5120/8158] loss=0.001925327250501141\n",
            "[1m 43s] Epoch 395 [7680/8158] loss=0.001909574633464217\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7940832199977638\n",
            "[1m 43s] Epoch 396 [2560/8158] loss=0.001962941384408623\n",
            "[1m 43s] Epoch 396 [5120/8158] loss=0.0019387970096431673\n",
            "[1m 43s] Epoch 396 [7680/8158] loss=0.0019164211970443526\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7938103302660612\n",
            "[1m 43s] Epoch 397 [2560/8158] loss=0.0019205603050068021\n",
            "[1m 43s] Epoch 397 [5120/8158] loss=0.0019352530129253865\n",
            "[1m 43s] Epoch 397 [7680/8158] loss=0.0019186073681339622\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7942373521610403\n",
            "[1m 43s] Epoch 398 [2560/8158] loss=0.0019344938220456243\n",
            "[1m 43s] Epoch 398 [5120/8158] loss=0.0019250637560617179\n",
            "[1m 43s] Epoch 398 [7680/8158] loss=0.0019245676385859648\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7941691297281147\n",
            "[1m 44s] Epoch 399 [2560/8158] loss=0.001940267370082438\n",
            "[1m 44s] Epoch 399 [5120/8158] loss=0.00192302938667126\n",
            "[1m 44s] Epoch 399 [7680/8158] loss=0.001915457600262016\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7938217006715489\n",
            "[1m 44s] Epoch 400 [2560/8158] loss=0.0018759403727017343\n",
            "[1m 44s] Epoch 400 [5120/8158] loss=0.0019225479161832481\n",
            "[1m 44s] Epoch 400 [7680/8158] loss=0.001920158703190585\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.794256302836853\n",
            "[1m 44s] Epoch 401 [2560/8158] loss=0.0019416177528910339\n",
            "[1m 44s] Epoch 401 [5120/8158] loss=0.0019418478768784553\n",
            "[1m 44s] Epoch 401 [7680/8158] loss=0.0019204171646075945\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7943106281075161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-851792dad8db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_EPOCHS2\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mN_EPOCHS1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Train cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevModel_auc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0macc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-87c05ac6d24e>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0melem_size\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Lcv7YwCjiTCm",
        "outputId": "095634a3-ec78-42c0-b1c1-5156c148ca2a"
      },
      "source": [
        "epoch = np.arange(1, len(auc_list) + 1, 1)\r\n",
        "plt.plot(epoch[:10], auc_list[:10], label = 'lr = 0.001')\r\n",
        "plt.plot(epoch[10:], auc_list[10:], label = 'lr = 0.0001')\r\n",
        " \r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylabel('AUC')\r\n",
        "plt.title('combined LR')\r\n",
        "plt.legend()\r\n",
        "plt.grid()\r\n",
        "plt.show()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxV9Z3/8deHJBAh7EtEwipUpZWiIGqtI2itYK06M4zCtC4dLVOntlP7sD+ZR2es47Qz2ml1xqWdh9YFO9XoWKu0xbqVuLRiFYvKIosIkojIDmFP7uf3x/fc5Obm5t5ckpNc4P18PM4j93zPks85gfO53+/3nPM1d0dERKS1unR2ACIicmhR4hARkbwocYiISF6UOEREJC9KHCIikhclDhERyYsSh0g7MbMrzeyVLMufNrMrYvi9I8zMzay4vfctkokSh0gHcfdp7j6no3+vma0xs89lKJ9sZgkzqzWznWa23My+0tHxyaFHiUPkyPahu5cBvYDrgHvN7LhOjkkKnBKHHBHMbKiZPWFmG81ss5ndFZV3MbN/NrO1ZvaxmT1kZr2jZckmoK+Y2Toz22pmXzOzU8zsbTPbltxP019ld5nZdjN718zOSVlQZWZXR5+vNLNXzOxH0X7fN7NpKev2NrP7zGy9mdWY2ffNrChaVhRtt8nMVgNfaOv58WAesAUY19b9yeFNiUMOe9EF9zfAWmAEMASojBZfGU1TgFFAGZCeDE4FxgCXAv8FfBf4HPBJ4BIzOytt3feAAcD3gCfMrF8LoZ0KLI/W/SFwn5lZtOxBoA4YDZwEfB64Olr2VeCCqHwiMD33WcguSqAXRrGsauv+5PCmxCFHgknAMcB33H2Xu+9192Qn9peA29x9tbvXAv8EzEjraP63aJtngV3AI+7+sbvXAC8TLuBJHwP/5e4H3P1RQmJoqUaw1t3vdfd6YA4wGCg3s3LgfOBbUbwfA7cDM6LtLol+xzp33wL8RxvOzTFmtg3YA/wK+La7/7kN+5MjgO7CkCPBUMJFui7DsmMINZGktYT/F+UpZRtSPu/JMF+WMl/jTd8cujb6HZl8lPzg7rujykYZ0A8oAdY3VkDoAqxLiXld426axJ+vD929wsy6AbcAZxNqVSItUo1DjgTrgGEt3K76ITA8ZX4YoYloQ4Z1W2NISnNTcn8f5rmPdcA+YIC794mmXu7+yWj5ekIyTP0dbeLu+4AbgBPN7OK27k8Ob0occiT4E+Fie4uZ9TCzUjM7I1r2CHCdmY00szLg34FHW6idtMYg4JtmVmJmfwOcAMzLZwfuvh54FvixmfWK+h+OTelLeSz6HRVm1heY3YrdlkTHnZyaJVF33w/8GLgxn3jlyKPEIYe9qA/hi4SO5g+AakJHN8D9wM+Bl4D3gb3AN9rw614jdKRvAn4ATHf3zQexn8uBrsBSYCvwOKEPBOBe4BngLeBN4IlW7G8eoVktOd3Uwnr3E2pnXzyImOUIYRrISURE8qEah4iI5EWJQ0RE8qLEISIieVHiEBGRvBwRDwAOGDDAR4wYkdc2u3btokePHvEE1E4KPUbF1zaFHh8UfoyKr20WLly4yd0HNlvg7of9NGHCBM/X/Pnz896moxV6jIqvbQo9PvfCj1HxtQ3whme4pqqpSkRE8qLEISIieVHiEBGRvChxiIhIXpQ4REQkL0ocIiKSFyUOERHJixKHiEiqvTvgT/fCnq0tr7NvJ6S/WfwIetP4EfHkuIi0Qd1+2LsdSntDcdfG8kQ9gzZUwbzfwqlfgz7DYP1bUDYIeg6G/bugpDvs2QILH4QDu+H4C2DIxLD9qudh6ZOw8V0483o4bhpsWgGrX4S1r0DNm1C3F47qB8NOg77DYesa6N4fjj4Rjj0bjurbEAu7N0PdPtjxIWx9H7a8zwnLXoU1P4LyT0G/kfDhn8OxFJfCsNOh3yjYuw16DYFBx0NRV3j4EvjgVfjDHfD5m6HvCKivg307QjJ5+zFY+Qz0PCZsX7shTNYF/vpnMOZc2PlRSC71ByBRB6W9oEf0AHaiDvbvhrV/YPCHC6HudFjzCsz/97Dtp2eGY006sDecu+LSEGvJUY3HvWcrvP9SON9DJ4U46/dD1+5huTs0GZCyfShxiByK6vbD7k2wayPs2hQumrs2hovSmPOg59Hh4rl3BxzYA/trYcv74cJbVBy22bgcPAE9BsDwM8IFZvMq2L0F9mwLF6U9W+HArvA7u/WGUWeFi1ftx7BnK2OTF8yFc6Brj5AkmokuXF2K4Q//DWVHQ7eesHllSEZH9YPKmdB9QDgmgN7DoOIUOKpPSASLn4D9O8M6e7eF47QiKP9kuGhWv57hdxu9uw2AkmHwxv1Qvw96DIKy8rCPJRnGv+paFi7SZ/8zvPlz+L8rm6/TvT985huwvQZ2rofysTD6HFjzB3j4Uhh4PHy8pFV/xuMAfvI72LoWuveDqv8IU98RMOLMcJ7+/L8haSV1KYYxnw9/8+rXw98Qwt9w04qQGE+4ELw+JKRr/hiSeTtS4hDpSIkE1CyEklIYeEK4iO/cAKurwrfMgceF+bV/4ISlv4LVt4QLf9eycAHfvSVczFMvJOmeyzLya8/B4dt5aS8YdEL4hr29OlzQIXwr7zEQ+gyFweOgtE/4dlvaK9Qm1rwcLr4DxkBJd5bUDeOTU6+Cqn8PcR43LVy4dm0KieTA7nDMJ04PCWrFs7DsqbD8rBvgkxeHb8Wv3B4SyYjPwsizQhyp6g+E2ke3niFprn8LVjwNHy6CbR/AJ86DY06G4m4hafYbBX2GseCVV5k8eXL4hr9nS6hZJL+Bb1kNtRtD8tq+DjYsgU0r4bipcMIX4bSvw4bFIUkWdQ3noFuvEFvJUc3P7b6d8Ot/hG3r4Nybw7nuUhymvdtDUrQuIeEVdYUhJ/POgt9z4gcPwujPhdrK7s2w4plQi1g2N+xz7MWhNnFgT0ikm98LibRneaipHXt2qCEtfCDUosoGhVpR1zI49pzwN2hnsSYOM5sK/DdQBPzM3W9JW347MCWa7Q4Mcvc+0bJbgS9Ey/7N3R+Nyh8EzgK2R8uudPdFcR6HSFb7dkL1G+Fbfdce4Rvnu78NF4DibqEs2cxQ82a4SEF0MeodLhbJb40pencbAN1PCBeCfbXhm27/MeGbafcB0KN/9HNguCj3GBCaat79TWgm6ndsuNCUdA9T7wroVpb5GPbvgi4lTZuiWmFjVRX0GgwX3tm6Dcb9TZjSTb4h+3ZFJWGCEOPQU8LUWl27NzbfJPUbFSYIzVRjzm2+zdBJrf8d3XrC9Ptbvz6weUAt/NX10CXqbi7tBad9LUyJ+vB3Ke3VfMPzftB0fvjpcOa3G+en/WdIkDE0U0GMicPMioC7gXMJYzy/bmZz3X1pch13vy5l/W8AJ0WfvwCcDIwHugFVZva0uye/Zn3H3R+PK3YR3MO3xERd+OZW3C00S7z3e1g6NzSfHNgFGGxbG9ZLN+AT4WK8v7bx2+LRJ8I5N4btNrwTfkfZ0eFb7vaa0LxUVg6DP82CxR8yecqU5vvN5ZSr89+ma+G+ofWw16WFe5S6FGVOGm3ZZzuJs8YxCVjl7qsBzKwSuAhY2sL6M4HvRZ/HAi+5ex1QZ2ZvA1OBx2KMV45kiQRsXBZqCSueCc0W+3c2Li8uDU0lENqfB54QvpG6w9gLQ/NKjwGh9rBhCRxzUmg2yPqNL+2b9zEnNZ239e1xZCLtzjymW8jMbDow1d2vjuYvA05192szrDscWABUuHu9mX2ekETOJTRh/Qm4291/HDVVnQ7sA14AZrv7vgz7nAXMAigvL59QWVmZV/y1tbWUlbVQrS8QhR5jZ8VXVLcb8wT1RaV4l2K671pHvy0L6bZvC133b6PkwHbciqirr6fU91Bct4tu+7ZQXB/agneWHcv23sezt7Qct2KK6ndRcqCWPUeVs7PnJ9jZc3RsTQCpCv3vC4Ufo+JrmylTpix094np5YXSOT4DeNzd6wHc/VkzOwX4I7AReBWoj9b9J+AjoCtwD3ADcHP6Dt39nmg5EydO9MmTJ+cVUFVVFflu09EKPcZY49teHe5EKS6FHdVhvm4frJ4fOhYhdET2HAw7asJ8SffG/oBEPTt37qTnwIrQ+dtjQLhNdOSZ9OxdQc94os5Lof99ofBjVHzxiDNx1ABDU+YrorJMZgBfTy1w9x8APwAws4eBFVF5sv6+z8weAK5vx5ilUCTqQz/Ctg/CXUjrXmu89bBrWegLyKTPcPiL/xeSwZ4t4fbTo0+EcTPCXSgpFh6i/2lFOlucieN1YIyZjSQkjBnA36avZGbHA30JtYpkWRHQx903m9k4YBzwbLRssLuvNzMDLgYWx3gMEqdtH8C6P4V+Aq8PNYhta8OzBB+90/Q2wn6jYNSU0Em9ZwucclW4h79uf7j9ss+wsKyke4c0I4kcyWJLHO5eZ2bXAs8Qbse9392XmNnNhOEI50arzgAqvWlnSwnwcsgN7AC+HHWUA/zCzAYSnipaBHwtrmOQNHX7whOxe7aGC33Z0eFW0TWvhKd/k0/JJuqg/gDHL38DVv5bWLdLcbh/vUtRWL51LdR+1Px39BwMfUfCyVeEWyR7VYTnCdr5ASYROXix9nG4+zxgXlrZjWnzN2XYbi/hzqpM+zy7HUOUdJvfCw8TfbwMPl4aHmbqMzTcTlr9BiQONF2/S3HmW1GBfiV9oGIcFHULySMRTcXdwgNPR38qPO1aXBpqCb2HhgfjRKSgFUrnuHSEA3ujV1RE0+b3wi2oG5eHJ3n372qsBRSXhqeYBx4XHlizovBQ0oBPhAfRrCiUb10Tbjsddnp4MKtLcXh2oUsRf3zxRfUhiByGlDgON3u3h5rBtrWhDyE5bV0Luz5uvn73/uGZhGPGh5rBkJPDMwn9jw3NSiIiaZQ4DmGle9bDy7eFzuRkF9HSpxpfStelODT/9BkW3uXTZ1h4KrnHwDD1GxluQxURyYMSx6HCPbzyYseHof9h9XxOXfwrIBE6qbsUhxffjb0IPj0j1Bh6DlatQUTanRJHoUokwtgBa18JfRBrXgnNT0lH9aW64gKGXvLD8JI5EZEOosRRSOr2h9dWv/tbWD4v1DAgNCtVnAKn/UN4T1K/UdB/NO+99BJDlTREpIMpcXS27TXw4q1hfIHN74UX65V0DwPDHH9BuG1V/RAiUkCUODrark3w3nxY81J4mG7NH8JzECM+CxUTQ6IYNTnzQDEiIgVAiSNue3fA2j+GF++9/2IYUQzCyGp9h4eRxqb8U2iCEhE5BChxxKR2Xx1l82+E1/4nPDVd1A2GnQpn/0t459Ix43XHk4gckpQ4YrDkw+387b2v8chpxzL2s9fBqLOgYpJepyEihwUljna2a18d33j4z5SWdKH8jC9DWbfODklEpF0pcbSzm3+9lDWbd/HwV0+jv5KGiByG4h3R/AhTs20P/7dwHV85YySnjerf2eGIiMRCiaMd/e+C8GT33312ZCdHIiISHyWOdrL3QD2P/OkDPj/2aIb00TMYInL4ijVxmNlUM1tuZqvMbHaG5beb2aJoWmFm21KW3Wpmi6Pp0pTykWb2WrTPR82sa5zH0Fo/e3k123Yf4MozRnR2KCIisYotcUTjht8NTCOM5jfTzJqM6ufu17n7eHcfD9wJPBFt+wXgZGA8cCpwvZn1ija7Fbjd3UcDW4Gr4jqG1vrlwmp+9OwKvjBuMKeO7NfZ4YiIxCrOGsckYJW7r3b3/UAlcFGW9WcCj0SfxwIvuXudu+8C3gamWhiE/Gzg8Wi9OcDFsUTfSh9u28PsJ97mM8f257ZLPk00TrqIyGHLPDkAUHvv2Gw6MNXdr47mLwNOdfdrM6w7HFgAVLh7vZl9HvgecC7QHfgTofYyB1gQ1TYws6HA0+7+qQz7nAXMAigvL59QWVmZV/y1tbWUlZU1zB9IONv3OQOOapprH1u+n6ffP8AP/+IoBnbv2C6j9BgLjeJrm0KPDwo/RsXXNlOmTFno7hObLXD3WCZgOvCzlPnLgLtaWPcG4M60su8Ci4DngF8A3wIGEGoxyXWGAotzxTJhwgTP1/z585vM3/7ccv/Ed+f5xzv2urv72k27vHbvAR930zN+zf++kff+20N6jIVG8bVNocfnXvgxKr62Ad7wDNfUOB8ArIku7EkVUVkmM4Cvpxa4+w+AHwCY2cPACmAz0MfMit29Lsc+29VzSzewry7Bk3+uYUjfo/iHX7xJ3+4lbN9zgL87Q7ffisiRI87E8TowxsxGEi7uM4C/TV/JzI4H+gKvppQVAX3cfbOZjQPGAc+6u5vZfEJtphK4AngqxmMA4OMde1ny4Q4AHntjHQl3hvfvTr8eXTm5e1cmDO8bdwgiIgUjtsTh7nVmdi3wDFAE3O/uS8zsZkL1Z2606gygMqoWJZUAL0cdzTuAL0c1DAjNWpVm9n3gz8B9cR1DUtWKjQBccfpw5rwaHvL76ZdOZtqJGn1PRI48sb6ryt3nAfPSym5Mm78pw3Z7CXdWZdrnasIdWx2mavnHlPfqxvXnHcdjb1QzYkAPzvvk0R0ZgohIwdBLDnOoq0/w8spNfOHEwfQsLeGBr5zCoJ7d6NJFt92KyJFJiSOHTbX72bm3jk8N6Q2glxeKyBFP76rKIRF1vZQUqYYhIgJKHDnVJ0Li0BPhIiKBEkcOyRpHkRKHiAigxJFTVOGgi86UiAigxJFTsqmqi2ocIiKAEkdOyecSlThERAIljhzqk30cem5DRARQ4sgpkQg/VeMQEQmUOHJINDRVdXIgIiIFQokjh2TnuJqqREQCJY4cEuocFxFpQokjh4bEoRqHiAigxJFTwwOAyhsiIoASR04NfRxqqhIRAWJOHGY21cyWm9kqM5udYfntZrYomlaY2baUZT80syVmtszM7rDoLYNmVhXtM7ndoDiPIZFQU5WISKrYxuOIxg2/GzgXqAZeN7O57r40uY67X5ey/jeAk6LPnwHOIIw1DvAKcBZQFc1/yd3fiCv2VI1NVUocIiIQb41jErDK3Ve7+36gErgoy/ozgUeizw6UAl2BboQxyDfEGGuLGp8c74zfLiJSeCz5LqZ237HZdGCqu18dzV8GnOru12ZYdziwAKhw9/qo7EfA1YABd7n7d6PyKqA/UA/8Evi+ZzgIM5sFzAIoLy+fUFlZmVf8tbW1lJWV8fbGOm5buI9/Pq2U0X2K8tpH3JIxFirF1zaFHh8UfoyKr22mTJmy0N0nNlvg7rFMwHTgZynzlxESQKZ1bwDuTJkfDfwWKIumV4Ezo2VDop89gWeBy3PFMmHCBM/X/Pnz3d39+aUf+fAbfuOLPtia9z7iloyxUCm+tin0+NwLP0bF1zbAG57hmhpnA0wNMDRlviIqy2QGjc1UAH8JLHD3WnevBZ4GTgdw95ro507gYUKTWGySfRx6clxEJIgzcbwOjDGzkWbWlZAc5qavZGbHA30JtYqkD4CzzKzYzEoIHePLovkB0XYlwAXA4hiPIWXo2Dh/i4jIoSO2xOHudcC1wDPAMuAxd19iZjeb2YUpq84AKqNqUdLjwHvAO8BbwFvu/mtCR/kzZvY2sIhQg7k3rmOAlKFjVeMQEQFivB0XwN3nAfPSym5Mm78pw3b1wN9nKN8FTGjfKLPTu6pERJrSTaY5aOhYEZGmlDhycL2rSkSkCSWOHDQeh4hIU0ocOaiPQ0SkKSWOHDQeh4hIU0ocOdQnwk+9Vl1EJFDiyKGxqaqTAxERKRBKHDmoqUpEpCkljhwSGgFQRKQJJY4c6jWQk4hIE0ocOXhDU1UnByIiUiB0OcxBrxwREWlKiSOHer0dV0SkCSWOHJLvqlKFQ0QkUOLIoV53VYmINKHEkYMGchIRaUqJI4dEw9CxShwiIhBz4jCzqWa23MxWmdnsDMtvN7NF0bTCzLalLPuhmS0xs2VmdodFV24zm2Bm70T7bCiPS727ahsiIiliSxxmVgTcDUwDxgIzzWxs6jrufp27j3f38cCdwBPRtp8BzgDGAZ8CTgHOijb7KfBVYEw0TY3rGAASrvdUiYikirPGMQlY5e6r3X0/UAlclGX9mcAj0WcHSoGuQDegBNhgZoOBXu6+wMOTeQ8BF8d1ABCaqvQMh4hIo+IY9z0EWJcyXw2cmmlFMxsOjAR+D+Dur5rZfGA9YMBd7r7MzCZG+0nd55AW9jkLmAVQXl5OVVVVXsHX1tZSVVXF2g/24Z7Ie/uOkIyxUCm+tin0+KDwY1R88YgzceRjBvC4u9cDmNlo4ASgIlr+nJmdCexp7Q7d/R7gHoCJEyf65MmT8wqoqqqKyZMn89LOpXRdv458t+8IyRgLleJrm0KPDwo/RsUXjzibqmqAoSnzFVFZJjNobKYC+EtggbvXunst8DRwerR9Rcp62fbZLhLu6uMQEUkRZ+J4HRhjZiPNrCshOcxNX8nMjgf6Aq+mFH8AnGVmxWZWQugYX+bu64EdZnZadDfV5cBTMR5DSBzKHCIiDWJLHO5eB1wLPAMsAx5z9yVmdrOZXZiy6gyg0pOvoQ0eB94D3gHeAt5y919Hy/4B+BmwKlrn6biOAcKT43pqXESkUax9HO4+D5iXVnZj2vxNGbarB/6+hX2+QbhFt0MkXA//iYik0pPjOSQSTpHOkohIA10Sc0i4mqpERFIpceRQ766mKhGRFC0mDjM7z8ymZyifbmbnxhtW4XDXm3FFRFJlq3HcCLyYobwKuDmWaApQfULPcYiIpMqWOLq5+8b0QnffBPSIL6TCUq/nOEREmsiWOHqZWbPbdaMH8o6KL6TC4q6XHIqIpMqWOJ4A7jWzhtqFmZUB/xMtOyLoAUARkaayJY5/BjYAa81soZm9CbwPbIyWHRESjpqqRERStPjkePTKkNlm9q/A6Kh4lbu3+g21h4OEOsdFRJpoMXGY2V+lFTnQx8wWufvOeMMqHBo6VkSkqWzvqvpihrJ+wDgzu8rdfx9TTAVF76oSEWkqW1PVVzKVR6P1PUYLo/kdbhIJp0h5Q0SkQd6vHHH3tYQxwI8ICd2OKyLSRN6JIxp4aV8MsRSk+oQeABQRSZWtc/zXhA7xVP2AwcCX4wyqkOhdVSIiTWXrHP9R2rwDWwjJ48s0Heo1IzObCvw3UAT8zN1vSVt+OzAlmu0ODHL3PmY2Bbg9ZdXjgRnu/qSZPUgYSnZ7tOxKd1+UK5aDVe9OiRKHiEiDbJ3jDS84NLOTgL8F/obwEOAvc+3YzIqAu4FzgWrgdTOb6+5LU37HdSnrfwM4KSqfD4yPyvsRhol9NmX333H3x1txfG0WXnKoxCEikpStqeoTwMxo2gQ8Cpi7T2lpmzSTCA8Mro72VwlcBCxtYf2ZwPcylE8Hnnb33a38ve1K76oSEWnK3NO7MaIFZgngZeAqd18Vla1291Gt2nEYy2Oqu18dzV8GnOru12ZYdziwAKiIxhtPXfZ74DZ3/000/yBwOqGD/gVgtrs366w3s1nALIDy8vIJlZWVrQm7QW1tLWVlZXzvj3vo0824bkJpXtt3hGSMhUrxtU2hxweFH6Pia5spU6YsdPeJzRa4e8YJuBioBNYB9wLnAO+3tH6G7acT+jWS85cBd7Ww7g3AnRnKBxPejVWSVmZAN2AOcGOuWCZMmOD5mj9/vru7T/uvl/yqB1/Pe/uOkIyxUCm+tin0+NwLP0bF1zbAG57hmtri7bju/qS7zyB0TM8HvgUMMrOfmtnnW5GsaoChKfMVUVkmM4BHMpRfAvzK3Q+kxLU+OqZ9wAOEJrHYhOc44vwNIiKHlpzPcbj7Lnd/2N2/SLj4/5lQQ8jldWCMmY00s66E5DA3faXouZC+ZL5LayZpCcXMBkc/jVArWtyKWA5aQu+qEhFpItvtuM24+1bgnmjKtW6dmV0LPEO4Hfd+d19iZjcTqj/JJDIDqIyqRQ3MbAShxpI+fO0vzGwgoblqEfC1fI4hX7qrSkSkqbwSR77cfR4wL63sxrT5m1rYdg0wJEP52e0XYW4aj0NEpKm8XzlypFEfh4hIU0ocOWjoWBGRppQ4cnA1VYmINKHEkUO9ho4VEWlCiSMHDR0rItKUEkcO7q6hY0VEUihx5KDOcRGRppQ4ckhoICcRkSaUOHJIJBxVOEREGilx5JBwNVWJiKRS4sih3l3PcYiIpFDiyCGRQC85FBFJocSRg95VJSLSlBJHDnoAUESkKSWOLMIwiWqqEhFJpcSRRSIaWkqJQ0SkUayJw8ymmtlyM1tlZrMzLL/dzBZF0woz2xaVT0kpX2Rme83s4mjZSDN7Ldrno9GwtLFIRIMSFim9iog0iO2SaGZFwN3ANGAsMNPMxqau4+7Xuft4dx8P3Ak8EZXPTyk/G9gNPBttditwu7uPBrYCV8V1DPVRlUPvqhIRaRTnd+lJwCp3X+3u+4FK4KIs688EHslQPh142t13W7iCnw08Hi2bA1zcjjE30VjjUOIQEUmKc8zxIcC6lPlq4NRMK5rZcGAk8PsMi2cAt0Wf+wPb3L0uZZ/NxiWP9jkLmAVQXl5OVVVVXsHX1tby4ksvA/D+6tVU+bocW3S82travI+rIym+tin0+KDwY1R88YgzceRjBvC4u9enFprZYOBE4Jl8d+ju9wD3AEycONEnT56c1/ZVVVWcdOoZ8PyzjBl9LJPPHJVvCLGrqqoi3+PqSIqvbQo9Pij8GBVfPOJsqqoBhqbMV0RlmcwgczPVJcCv3P1ANL8Z6GNmyYSXbZ9t5mqqEhFpJs7E8TowJroLqishOcxNX8nMjgf6Aq9m2EeTfg8PV/L5hH4PgCuAp9o57gbJznHdjisi0ii2xBH1Q1xLaGZaBjzm7kvM7GYzuzBl1RlApSe/3kfMbAShxvJi2q5vAL5tZqsIfR73xXME4alxQC85FBFJEWsfh7vPA+alld2YNn9TC9uuIUPHt7uvJtyxFTtveACwI36biMihQY+2ZZFsqtJ4HCIijZQ4skioqUpEpBkljiwSifBTneMiIo2UOLLQu6pERJrTJTGLhruqVOMQEWmgxJFFQs9xiIg0o8SRhcbjEBFpTokji4bbcXWWREQa6JKYRUJ9HCIizShxZKHEISLSnBJHFsk+Dr0dV0SkkRJHFqEO7OMAABDGSURBVI1Dx3ZyICIiBUSJIwsNHSsi0pwSRxZ6jkNEpDkljiz05LiISHNKHFm4OsdFRJqJNXGY2VQzW25mq8xsdoblt5vZomhaYWbbUpYNM7NnzWyZmS2NRgTEzB40s/dTthsfV/yNQ8fG9RtERA49sY0AaGZFwN3AuUA18LqZzXX3pcl13P26lPW/AZyUsouHgB+4+3NmVgYkUpZ9x90fjyv2JA0dKyLSXJw1jknAKndf7e77gUrgoizrzwQeATCzsUCxuz8H4O617r47xlgzcvVxiIg0Y8mLY7vv2Gw6MNXdr47mLwNOdfdrM6w7HFgAVLh7vZldDFwN7AdGAs8Ds6NlDwKnA/uAF6LyfRn2OQuYBVBeXj6hsrIyr/hra2tZubuU/35zHzedXsqI3kV5bd8RamtrKSsr6+wwWqT42qbQ44PCj1Hxtc2UKVMWuvvEZgvcPZYJmA78LGX+MuCuFta9AbgzbdvtwChCc9ovgauiZYMBA7oBc4Abc8UyYcIEz9f8+fP9d4vX+/AbfuOLa7blvX1HmD9/fmeHkJXia5tCj8+98GNUfG0DvOEZrqlxNlXVAENT5iuiskxmEDVTRaqBRR6aueqAJ4GTAdx9fXRM+4AHCE1isdBzHCIizcWZOF4HxpjZSDPrSkgOc9NXMrPjgb7Aq2nb9jGzgdH82cDSaP3B0U8DLgYWx3UAeleViEhzsd1V5e51ZnYt8AxQBNzv7kvM7GZC9SeZRGYAlVG1KLltvZldD7wQJYiFwL3R4l9ECcWARcDX4jqGxgcA4/oNIiKHntgSB4C7zwPmpZXdmDZ/UwvbPgeMy1B+djuGmJWaqkREmtOT41loPA4RkeaUOLJoHDpWiUNEJEmJI4tkr4ueHBcRaaTEkYU6x0VEmlPiyKJhICf1cYiINFDiyCLRMHSsEoeISJISRxbqHBcRaS7W5zgOdQ1PjqvGIdKhDhw4QHV1NXv37m3Tfnr37s2yZcvaKar2VyjxlZaWUlFRQUlJSavWV+LIItnHYaqXiXSo6upqevbsyYgRI9rUVLxz50569uzZjpG1r0KIz93ZvHkz1dXVjBw5slXb6JKYhTrHRTrH3r176d+/v/oXO4CZ0b9//7xqd0ocWdRHYw7qyXGRjqek0XHyPddKHFk0vHJEZ0lEpIEuiVnoJYciR644RuZzd775zW8yevRoxo0bx6JFizKut3DhQk488URGjx7NN7/5zYZhrLds2cK5557LmDFjOPfcc9m6dSsA7777LqeffjrdunXjRz/6UbvHnU6JI4t69XGISIq6uro2bf/000+zcuVKVq5cyT333MN1112Xcb1rrrmGe++9t2Hd3/3udwDccsstnHPOOaxcuZJzzjmHW265BYB+/fpxxx13cP3117cpvtbSXVVZJPSuKpFO96+/XsLSD3cc1Lb19fUUFRU1Kx97TC++98VPtmofVVVV/Mu//At9+/bl3XffZcWKFQcVC8BTTz3F5Zdfjplx2mmnsX37dtavX8/gwYMb1lm/fj07duzgtNNOA+Dyyy/nySefZNq0aTz11FNUVVUBcMUVVzB58mRuvfVWBg0axKBBg/jtb3970LHlQ4kji0TC9Z4qEeHNN99k8eLFGW9XvfTSS1m+fHmz8m9/+9tcfvnlTcpqamoYOrRxRO0hQ4ZQU1PTJHHU1NRQUVHRMF9RUUFNTRh1e8OGDQ3rHn300WzYsKFtB3aQlDiySLjrqXGRTtbamkEm7fWcxKRJk1p8xuHRRx9t8/4Phpl12p1nsfZxmNlUM1tuZqvMbHaG5beb2aJoWmFm21KWDTOzZ81smZktNbMRUflIM3st2uej0Xjmsah31y2BIkKPHj1aXHbppZcyfvz4ZtNDDz3UbN0hQ4awbt26hvmamhqGDBnSbJ3q6uqG+erq6oZ1ysvLWb9+PRCatAYNGtSm4zpYsSUOMysC7gamAWOBmWY2NnUdd7/O3ce7+3jgTuCJlMUPAf/p7icAk4CPo/JbgdvdfTSwFbgqrmNIJFwd4yKS1aOPPsqiRYuaTenNVAAXXnghDz30EO7OggUL6NWrV5NmKoDBgwfTq1cvFixYgLvz0EMPcdFFFzVsP2fOHADmzJnTUN7R4qxxTAJWuftqd98PVALZjnIm8AhAlGCKo3HHcfdad99t4ev/2cDj0TZzgIvjOoCEaywOEWk/559/PqNGjWL06NF89atf5bbbbmtYNn78+IbPP/nJT7j66qsZPXo0xx57LNOmTQNg9uzZPPfcc4wZM4bnn3+e2bNDQ85HH31ERUUFt912G9///vepqKhgx46Du6GgNSx5f3C779hsOjDV3a+O5i8DTnX3azOsOxxYAFS4e72ZXQxcDewHRgLPA7OBvsCCqLaBmQ0Fnnb3T2XY5yxgFkB5efmEysrKvOKvra3lqXUlvFJTx08/13I1tTPV1tbGcq95e1F8bVPo8UF8Mfbu3ZvRo0e3eT8t3VVVKAopvlWrVrF9+/YmZVOmTFno7hPT1y2UzvEZwOPuXh/NFwNnAicBHwCPAlcCT7V2h+5+D3APwMSJE33y5Ml5BVRVVcUxxwyg28YPyXfbjlJVVVWwsYHia6tCjw/ii3HZsmXt0qldCC8RzKaQ4istLeWkk05q1bpxNlXVAENT5iuiskxmEDVTRaqBRVEzVx3wJHAysBnoY2bJhJdtn21W766nxkVE0sSZOF4HxkR3QXUlJIe56SuZ2fGEJqhX07btY2YDo/mzgaUe2tXmA9Oj8ivIoxaSr9DHocQhIpIqtsQR1RSuBZ4BlgGPufsSM7vZzC5MWXUGUOkpnS1Rk9X1wAtm9g5gwL3R4huAb5vZKqA/cF9cx6AHAEVEmou1j8Pd5wHz0spuTJu/qYVtnwPGZShfTbhjK3b1CT0AKCKSTi85zEJNVSIizSlxZJFw11gcIkeoQ+m16un7ffPNNxv2NXXqVPr06cMFF1zQbsehy2IWCdeT4yLSqFBfq56+32uuuaZhX9/5znf4+c9/3qa40xXKcxwFqT6h23FFOt3Ts+Gjdw5q06Pq66Aow2Xu6BNh2i2t2seh8Fr19P1u27atYb/nnHNOwzbtRYkjC3eNxSEihf9a9fT9JrdJfw9We1HiyKJet+OKdL5W1gwy2aPXqsdCiSMLPTkuIpD7teqtrXG012vVBw8e3OS16un7Td0mDuocz8I1kJOI5FAIr1VP32/v3r1ja6YC1TiyUue4iLSn888/n3nz5jF69Gi6d+/OXXfd1bBs/PjxDbfn/uQnP+HKK69kz549TJs2rclr1S+55BLuu+8+hg8fzmOPPZZxvw888EDDfs8880zeffddamtrqaio4L777uO8885r03EocWQxcUQ/ave17fY7ETk01dbWAjB58uR2ewOwmXH33Xc3zO/cubPhc+ozHRMnTmTx4sXNtu/fvz8vvPBCzv2mevnll9sSckZKHFl8fUrbxwMQETncqI9DRETyosQhIgUprtFJpbl8z7USh4gUnNLSUjZv3qzk0QHcnc2bN1NaWtrqbdTHISIFp6KigurqajZu3Nim/ezduzevC2JHK5T4SktLmzytnosSh4gUnJKSkhaf1M5HVVVVq8fR7gyFHl9L1FQlIiJ5UeIQEZG8KHGIiEhe7Ei4a8HMNgJr89xsALAphnDaU6HHqPjaptDjg8KPUfG1zXB3H5heeEQkjoNhZm+4+8TOjiObQo9R8bVNoccHhR+j4ouHmqpERCQvShwiIpIXJY6W3dPZAbRCoceo+Nqm0OODwo9R8cVAfRwiIpIX1ThERCQvShwiIpIXJY4MzGyqmS03s1VmNrsA4hlqZvPNbKmZLTGzf4zKbzKzGjNbFE3nd2KMa8zsnSiON6Kyfmb2nJmtjH727cT4jks5T4vMbIeZfaszz6GZ3W9mH5vZ4pSyjOfMgjuif5Nvm9nJnRTff5rZu1EMvzKzPlH5CDPbk3Ie/yfu+LLE2OLf1Mz+KTqHy82sbeOnHnx8j6bEtsbMFkXlnXIOD4q7a0qZgCLgPWAU0BV4CxjbyTENBk6OPvcEVgBjgZuA6zv7nEVxrQEGpJX9EJgdfZ4N3NrZcab8jT8ChnfmOQT+AjgZWJzrnAHnA08DBpwGvNZJ8X0eKI4+35oS34jU9Tr5HGb8m0b/Z94CugEjo//nRR0dX9ryHwM3duY5PJhJNY7mJgGr3H21u+8HKoGLOjMgd1/v7m9Gn3cCy4AhnRlTK10EzIk+zwEu7sRYUp0DvOfu+b5NoF25+0vAlrTils7ZRcBDHiwA+pjZ4I6Oz92fdfe6aHYB0Pp3cceghXPYkouASnff5+7vA6sI/99jky0+MzPgEuCROGOIgxJHc0OAdSnz1RTQRdrMRgAnAa9FRddGzQb3d2ZTEODAs2a20MxmRWXl7r4++vwRUN45oTUzg6b/WQvlHELL56wQ/13+HaEWlDTSzP5sZi+a2ZmdFVQk09+00M7hmcAGd1+ZUlZI57BFShyHEDMrA34JfMvddwA/BY4FxgPrCdXezvJZdz8ZmAZ83cz+InWhh7p4p9/7bWZdgQuB/4uKCukcNlEo5ywTM/suUAf8IipaDwxz95OAbwMPm1mvTgqvYP+maWbS9AtMIZ3DrJQ4mqsBhqbMV0RlncrMSghJ4xfu/gSAu29w93p3TwD3EnO1Oxt3r4l+fgz8KoplQ7I5Jfr5cWfFl2Ia8Ka7b4DCOoeRls5Zwfy7NLMrgQuAL0XJjaj5Z3P0eSGh/+ATnRFflr9pIZ3DYuCvgEeTZYV0DnNR4mjudWCMmY2Mvp3OAOZ2ZkBRW+h9wDJ3vy2lPLWN+y+BxenbdgQz62FmPZOfCR2oiwnn7YpotSuApzojvjRNvuUVyjlM0dI5mwtcHt1ddRqwPaVJq8OY2VTg/wEXuvvulPKBZlYUfR4FjAFWd3R80e9v6W86F5hhZt3MbCQhxj91dHyRzwHvunt1sqCQzmFOnd07X4gT4Q6WFYSM/90CiOezhCaLt4FF0XQ+8HPgnah8LjC4k+IbRbhb5S1gSfKcAf2BF4CVwPNAv04+jz2AzUDvlLJOO4eEBLYeOEBob7+qpXNGuJvq7ujf5DvAxE6KbxWhnyD57/B/onX/OvrbLwLeBL7Yieewxb8p8N3oHC4HpnVGfFH5g8DX0tbtlHN4MJNeOSIiInlRU5WIiORFiUNERPKixCEiInlR4hARkbwocYiISF6UOETagZnVW9O377bbW5Wjt6Z29vMlIg2KOzsAkcPEHncf39lBiHQE1ThEYhSNt/BDC2OV/MnMRkflI8zs99GL+F4ws2FReXk0zsVb0fSZaFdFZnavhfFYnjWzozrtoOSIp8Qh0j6OSmuqujRl2XZ3PxG4C/ivqOxOYI67jyO8KPCOqPwO4EV3/zRhHIclUfkY4G53/ySwjfCUsUin0JPjIu3AzGrdvSxD+RrgbHdfHb2o8iN3729mmwivwjgQla939wFmthGocPd9KfsYATzn7mOi+RuAEnf/fvxHJtKcahwi8fMWPudjX8rnetQ/KZ1IiUMkfpem/Hw1+vxHwpuXAb4EvBx9fgG4BsDMisysd0cFKdJa+tYi0j6OMrNFKfO/c/fkLbl9zextQq1hZlT2DeABM/sOsBH4SlT+j8A9ZnYVoWZxDeHtqiIFQ30cIjGK+jgmuvumzo5FpL2oqUpERPKiGoeIiORFNQ4REcmLEoeIiORFiUNERPKixCEiInlR4hARkbz8f9XdjBsF/8GFAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppxHtN5jii5t",
        "outputId": "b5f8e523-98ad-4efb-83c3-85214e802b9b"
      },
      "source": [
        "#调参 learning rate\r\n",
        "\r\n",
        "HIDDEN_SIZE = 128\r\n",
        "N_LAYER = 3\r\n",
        "BATCH_SIZE = 256\r\n",
        "\r\n",
        "N_EPOCHS1  = 10\r\n",
        "LR1 = 0.001\r\n",
        "N_EPOCHS2  = 180\r\n",
        "LR2 = 0.0001\r\n",
        "\r\n",
        "\r\n",
        "trainset = DayFeatureDataset(is_train_set = True)\r\n",
        "trainloader = DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True)\r\n",
        "devset = DayFeatureDataset(is_train_set = False)\r\n",
        "devloader = DataLoader(devset, batch_size = BATCH_SIZE, shuffle = False)\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "  classifier = RNNClassifier(11, HIDDEN_SIZE, N_CLASS, N_LAYER, bidirectional = False)\r\n",
        "  if USE_GPU:\r\n",
        "    device = torch.device(\"cuda:0\")\r\n",
        "    classifier.to(device)\r\n",
        "\r\n",
        "  criterion = torch.nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "  optimizer = torch.optim.Adam(classifier.parameters(), lr=LR1)\r\n",
        "\r\n",
        "  start = time.time()\r\n",
        "  print(\"Training for %d epochs...\" % N_EPOCHS1 )\r\n",
        "  acc_list = []\r\n",
        "  auc_list = []\r\n",
        "  for epoch in range(1, N_EPOCHS1  + 1):\r\n",
        "    # Train cycle\r\n",
        "    trainModel()\r\n",
        "    acc, auc = devModel_auc()\r\n",
        "    acc_list.append(acc)\r\n",
        "    auc_list.append(auc)\r\n",
        "\r\n",
        "  optimizer = torch.optim.Adam(classifier.parameters(), lr=LR2)\r\n",
        "\r\n",
        "  start = time.time()\r\n",
        "  print(\"Training for another %d epochs...\" % N_EPOCHS2 )\r\n",
        "  for epoch in range(N_EPOCHS1, N_EPOCHS2  + N_EPOCHS1):\r\n",
        "    # Train cycle\r\n",
        "    trainModel()\r\n",
        "    acc, auc = devModel_auc()\r\n",
        "    acc_list.append(acc)\r\n",
        "    auc_list.append(auc)\r\n",
        "  accs.append(acc_list)\r\n",
        "  aucs.append(auc_list)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 10 epochs...\n",
            "[0m 0s] Epoch 1 [2560/8158] loss=0.002543482487089932\n",
            "[0m 0s] Epoch 1 [5120/8158] loss=0.0024882721481844783\n",
            "[0m 0s] Epoch 1 [7680/8158] loss=0.0024206011944139997\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1906/2658 71.71%\n",
            "Dev set: AUC  0.7569853770268538\n",
            "[0m 0s] Epoch 2 [2560/8158] loss=0.0021305083530023692\n",
            "[0m 0s] Epoch 2 [5120/8158] loss=0.00208756648353301\n",
            "[0m 0s] Epoch 2 [7680/8158] loss=0.0020813211915083228\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1946/2658 73.21%\n",
            "Dev set: AUC  0.7886260570529046\n",
            "[0m 0s] Epoch 3 [2560/8158] loss=0.0020043008727952836\n",
            "[0m 0s] Epoch 3 [5120/8158] loss=0.0020054576569236817\n",
            "[0m 0s] Epoch 3 [7680/8158] loss=0.0020112494899270435\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1963/2658 73.85%\n",
            "Dev set: AUC  0.7897359349663341\n",
            "[0m 0s] Epoch 4 [2560/8158] loss=0.0019758402951993046\n",
            "[0m 0s] Epoch 4 [5120/8158] loss=0.001987737836316228\n",
            "[0m 0s] Epoch 4 [7680/8158] loss=0.002003126731142402\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.7889033686089635\n",
            "[0m 1s] Epoch 5 [2560/8158] loss=0.0020415502018295228\n",
            "[0m 1s] Epoch 5 [5120/8158] loss=0.0020450675510801375\n",
            "[0m 1s] Epoch 5 [7680/8158] loss=0.002007117363003393\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1984/2658 74.64%\n",
            "Dev set: AUC  0.7895312676675572\n",
            "[0m 1s] Epoch 6 [2560/8158] loss=0.001967659848742187\n",
            "[0m 1s] Epoch 6 [5120/8158] loss=0.0019651833456009625\n",
            "[0m 1s] Epoch 6 [7680/8158] loss=0.0019828759987528125\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1982/2658 74.57%\n",
            "Dev set: AUC  0.7893948228017058\n",
            "[0m 1s] Epoch 7 [2560/8158] loss=0.00196471557719633\n",
            "[0m 1s] Epoch 7 [5120/8158] loss=0.0019734512665309013\n",
            "[0m 1s] Epoch 7 [7680/8158] loss=0.001980732920734833\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.790121897063719\n",
            "[0m 1s] Epoch 8 [2560/8158] loss=0.001969712402205914\n",
            "[0m 1s] Epoch 8 [5120/8158] loss=0.00196382679278031\n",
            "[0m 1s] Epoch 8 [7680/8158] loss=0.0019675833289511504\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7904731162554475\n",
            "[0m 2s] Epoch 9 [2560/8158] loss=0.0019454831955954433\n",
            "[0m 2s] Epoch 9 [5120/8158] loss=0.0019622327818069607\n",
            "[0m 2s] Epoch 9 [7680/8158] loss=0.0019656930584460498\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7912191411932734\n",
            "[0m 2s] Epoch 10 [2560/8158] loss=0.0019509191042743623\n",
            "[0m 2s] Epoch 10 [5120/8158] loss=0.0020003878453280777\n",
            "[0m 2s] Epoch 10 [7680/8158] loss=0.0019794048353408773\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7916499532234152\n",
            "Training for another 180 epochs...\n",
            "[0m 0s] Epoch 10 [2560/8158] loss=0.0020135692320764065\n",
            "[0m 0s] Epoch 10 [5120/8158] loss=0.0020092938968446104\n",
            "[0m 0s] Epoch 10 [7680/8158] loss=0.0019674471928738057\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7918483036302545\n",
            "[0m 0s] Epoch 11 [2560/8158] loss=0.0019743549870327114\n",
            "[0m 0s] Epoch 11 [5120/8158] loss=0.0019930295704398302\n",
            "[0m 0s] Epoch 11 [7680/8158] loss=0.0019603911282805105\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7919247380226991\n",
            "[0m 0s] Epoch 12 [2560/8158] loss=0.001938832935411483\n",
            "[0m 0s] Epoch 12 [5120/8158] loss=0.0019603913999162613\n",
            "[0m 0s] Epoch 12 [7680/8158] loss=0.0019697108150770267\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7920157012666\n",
            "[0m 0s] Epoch 13 [2560/8158] loss=0.0019281604676507414\n",
            "[0m 0s] Epoch 13 [5120/8158] loss=0.001963946147589013\n",
            "[0m 1s] Epoch 13 [7680/8158] loss=0.0019648135794947544\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7920959257942071\n",
            "[0m 1s] Epoch 14 [2560/8158] loss=0.00192620629677549\n",
            "[0m 1s] Epoch 14 [5120/8158] loss=0.001964328851317987\n",
            "[0m 1s] Epoch 14 [7680/8158] loss=0.001964831887744367\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.792186889038108\n",
            "[0m 1s] Epoch 15 [2560/8158] loss=0.001964388380292803\n",
            "[0m 1s] Epoch 15 [5120/8158] loss=0.0019662755541503428\n",
            "[0m 1s] Epoch 15 [7680/8158] loss=0.0019626925893438357\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.792212788295052\n",
            "[0m 1s] Epoch 16 [2560/8158] loss=0.0019159177551046013\n",
            "[0m 1s] Epoch 16 [5120/8158] loss=0.0019646314613055438\n",
            "[0m 1s] Epoch 16 [7680/8158] loss=0.00197108134549732\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7924003999855974\n",
            "[0m 1s] Epoch 17 [2560/8158] loss=0.0019752499647438526\n",
            "[0m 2s] Epoch 17 [5120/8158] loss=0.0019657495431602\n",
            "[0m 2s] Epoch 17 [7680/8158] loss=0.0019626294417927665\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1991/2658 74.91%\n",
            "Dev set: AUC  0.7924376696480291\n",
            "[0m 2s] Epoch 18 [2560/8158] loss=0.001929671794641763\n",
            "[0m 2s] Epoch 18 [5120/8158] loss=0.0019412113120779394\n",
            "[0m 2s] Epoch 18 [7680/8158] loss=0.001963859462800125\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7925437934325802\n",
            "[0m 2s] Epoch 19 [2560/8158] loss=0.001975158287677914\n",
            "[0m 2s] Epoch 19 [5120/8158] loss=0.0019643046660348775\n",
            "[0m 2s] Epoch 19 [7680/8158] loss=0.0019741071931396923\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7926328616088998\n",
            "[0m 2s] Epoch 20 [2560/8158] loss=0.0019413504051044583\n",
            "[0m 2s] Epoch 20 [5120/8158] loss=0.0019658072094898673\n",
            "[0m 2s] Epoch 20 [7680/8158] loss=0.0019576932264802355\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7927361427920789\n",
            "[0m 2s] Epoch 21 [2560/8158] loss=0.001949496567249298\n",
            "[0m 3s] Epoch 21 [5120/8158] loss=0.0019650832226034256\n",
            "[0m 3s] Epoch 21 [7680/8158] loss=0.001960014873960366\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7927869937721761\n",
            "[0m 3s] Epoch 22 [2560/8158] loss=0.0019505136529915034\n",
            "[0m 3s] Epoch 22 [5120/8158] loss=0.0019514518906362354\n",
            "[0m 3s] Epoch 22 [7680/8158] loss=0.00195788685620452\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7928747985701082\n",
            "[0m 3s] Epoch 23 [2560/8158] loss=0.001969567802734673\n",
            "[0m 3s] Epoch 23 [5120/8158] loss=0.001966493419604376\n",
            "[0m 3s] Epoch 23 [7680/8158] loss=0.001964768759595851\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7929070147189898\n",
            "[0m 3s] Epoch 24 [2560/8158] loss=0.0019909240771085026\n",
            "[0m 3s] Epoch 24 [5120/8158] loss=0.0019621760526206343\n",
            "[0m 3s] Epoch 24 [7680/8158] loss=0.0019662086813089746\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7929796589762717\n",
            "[0m 4s] Epoch 25 [2560/8158] loss=0.0018691510078497231\n",
            "[0m 4s] Epoch 25 [5120/8158] loss=0.0019207449455279856\n",
            "[0m 4s] Epoch 25 [7680/8158] loss=0.001950981126477321\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7931369495855171\n",
            "[0m 4s] Epoch 26 [2560/8158] loss=0.001901265198830515\n",
            "[0m 4s] Epoch 26 [5120/8158] loss=0.0019487003271933645\n",
            "[0m 4s] Epoch 26 [7680/8158] loss=0.0019585909283099073\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.79318622134263\n",
            "[0m 4s] Epoch 27 [2560/8158] loss=0.001958815206307918\n",
            "[0m 4s] Epoch 27 [5120/8158] loss=0.001957057014806196\n",
            "[0m 4s] Epoch 27 [7680/8158] loss=0.0019679564322965843\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7931549527275391\n",
            "[0m 4s] Epoch 28 [2560/8158] loss=0.0019557237159460784\n",
            "[0m 5s] Epoch 28 [5120/8158] loss=0.0019810164347290994\n",
            "[0m 5s] Epoch 28 [7680/8158] loss=0.00195692820319285\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7932936085055687\n",
            "[0m 5s] Epoch 29 [2560/8158] loss=0.0019777054549194872\n",
            "[0m 5s] Epoch 29 [5120/8158] loss=0.0019525797048117965\n",
            "[0m 5s] Epoch 29 [7680/8158] loss=0.001963362431464096\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dev set: AUC  0.7933390901275191\n",
            "[0m 5s] Epoch 30 [2560/8158] loss=0.001975653739646077\n",
            "[0m 5s] Epoch 30 [5120/8158] loss=0.0019506298005580903\n",
            "[0m 5s] Epoch 30 [7680/8158] loss=0.0019454567305122812\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.793390888641407\n",
            "[0m 5s] Epoch 31 [2560/8158] loss=0.001983009628020227\n",
            "[0m 5s] Epoch 31 [5120/8158] loss=0.0019409730564802885\n",
            "[0m 5s] Epoch 31 [7680/8158] loss=0.0019603153225034474\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7934388970201325\n",
            "[0m 6s] Epoch 32 [2560/8158] loss=0.0019660282297991214\n",
            "[0m 6s] Epoch 32 [5120/8158] loss=0.002003933535888791\n",
            "[0m 6s] Epoch 32 [7680/8158] loss=0.0019572968129068615\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dev set: AUC  0.7934546892499765\n",
            "[0m 6s] Epoch 33 [2560/8158] loss=0.0019407266518101096\n",
            "[0m 6s] Epoch 33 [5120/8158] loss=0.001949109073029831\n",
            "[0m 6s] Epoch 33 [7680/8158] loss=0.0019532757656027873\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7934774300609517\n",
            "[0m 6s] Epoch 34 [2560/8158] loss=0.001920932694338262\n",
            "[0m 6s] Epoch 34 [5120/8158] loss=0.001948943268507719\n",
            "[0m 6s] Epoch 34 [7680/8158] loss=0.001956805563531816\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7935096462098332\n",
            "[0m 6s] Epoch 35 [2560/8158] loss=0.002025705575942993\n",
            "[0m 6s] Epoch 35 [5120/8158] loss=0.001985023997258395\n",
            "[0m 6s] Epoch 35 [7680/8158] loss=0.0019571999708811443\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7935595496561401\n",
            "[0m 7s] Epoch 36 [2560/8158] loss=0.0019377365708351136\n",
            "[0m 7s] Epoch 36 [5120/8158] loss=0.001944897440262139\n",
            "[0m 7s] Epoch 36 [7680/8158] loss=0.0019594474773233137\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7936151383051905\n",
            "[0m 7s] Epoch 37 [2560/8158] loss=0.002053283981513232\n",
            "[0m 7s] Epoch 37 [5120/8158] loss=0.0019775214605033398\n",
            "[0m 7s] Epoch 37 [7680/8158] loss=0.001962607889436185\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7936195601295468\n",
            "[0m 7s] Epoch 38 [2560/8158] loss=0.0019938576268032195\n",
            "[0m 7s] Epoch 38 [5120/8158] loss=0.001992526836693287\n",
            "[0m 7s] Epoch 38 [7680/8158] loss=0.0019625298213213683\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7937092599950601\n",
            "[0m 7s] Epoch 39 [2560/8158] loss=0.0019887464819476008\n",
            "[0m 7s] Epoch 39 [5120/8158] loss=0.001952090917620808\n",
            "[0m 8s] Epoch 39 [7680/8158] loss=0.0019621008192189037\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7937547416170107\n",
            "[0m 8s] Epoch 40 [2560/8158] loss=0.0019145534024573863\n",
            "[0m 8s] Epoch 40 [5120/8158] loss=0.0019325744186062365\n",
            "[0m 8s] Epoch 40 [7680/8158] loss=0.0019537099016209442\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7938191739147737\n",
            "[0m 8s] Epoch 41 [2560/8158] loss=0.0019692769972607495\n",
            "[0m 8s] Epoch 41 [5120/8158] loss=0.0019768501399084924\n",
            "[0m 8s] Epoch 41 [7680/8158] loss=0.001955735532101244\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.793769270468467\n",
            "[0m 8s] Epoch 42 [2560/8158] loss=0.0019513810635544359\n",
            "[0m 8s] Epoch 42 [5120/8158] loss=0.001956347341183573\n",
            "[0m 8s] Epoch 42 [7680/8158] loss=0.0019520102262807388\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7938507583744615\n",
            "[0m 8s] Epoch 43 [2560/8158] loss=0.0019570444710552692\n",
            "[0m 9s] Epoch 43 [5120/8158] loss=0.001944873674074188\n",
            "[0m 9s] Epoch 43 [7680/8158] loss=0.001955320321333905\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7938829745233431\n",
            "[0m 9s] Epoch 44 [2560/8158] loss=0.0018977295141667128\n",
            "[0m 9s] Epoch 44 [5120/8158] loss=0.001935650350060314\n",
            "[0m 9s] Epoch 44 [7680/8158] loss=0.0019538138954279323\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.793885501280118\n",
            "[0m 9s] Epoch 45 [2560/8158] loss=0.001963897526729852\n",
            "[0m 9s] Epoch 45 [5120/8158] loss=0.0019370779453311115\n",
            "[0m 9s] Epoch 45 [7680/8158] loss=0.001952883896107475\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7938261224959051\n",
            "[0m 9s] Epoch 46 [2560/8158] loss=0.001935792458243668\n",
            "[0m 9s] Epoch 46 [5120/8158] loss=0.0019543079659342768\n",
            "[0m 10s] Epoch 46 [7680/8158] loss=0.0019580003378602364\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7939284561452936\n",
            "[0m 10s] Epoch 47 [2560/8158] loss=0.001982242439407855\n",
            "[0m 10s] Epoch 47 [5120/8158] loss=0.0019595789897721263\n",
            "[0m 10s] Epoch 47 [7680/8158] loss=0.0019503732328303158\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7939290878344875\n",
            "[0m 10s] Epoch 48 [2560/8158] loss=0.0019357326673343778\n",
            "[0m 10s] Epoch 48 [5120/8158] loss=0.0019738967646844687\n",
            "[0m 10s] Epoch 48 [7680/8158] loss=0.001954422778605173\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7939183491181935\n",
            "[0m 10s] Epoch 49 [2560/8158] loss=0.0019153472385369241\n",
            "[0m 10s] Epoch 49 [5120/8158] loss=0.00196148935938254\n",
            "[0m 10s] Epoch 49 [7680/8158] loss=0.001953289227094501\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7939448800643312\n",
            "[0m 11s] Epoch 50 [2560/8158] loss=0.0019496625289320945\n",
            "[0m 11s] Epoch 50 [5120/8158] loss=0.001934446208178997\n",
            "[0m 11s] Epoch 50 [7680/8158] loss=0.0019591372498931983\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7940029954701568\n",
            "[0m 11s] Epoch 51 [2560/8158] loss=0.0019702219986356797\n",
            "[0m 11s] Epoch 51 [5120/8158] loss=0.0019654358795378355\n",
            "[0m 11s] Epoch 51 [7680/8158] loss=0.0019620983395725487\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7939960468890255\n",
            "[0m 11s] Epoch 52 [2560/8158] loss=0.0019793402054347096\n",
            "[0m 11s] Epoch 52 [5120/8158] loss=0.001956625794991851\n",
            "[0m 11s] Epoch 52 [7680/8158] loss=0.0019610224252877137\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7940042588485443\n",
            "[0m 11s] Epoch 53 [2560/8158] loss=0.0019414472044445574\n",
            "[0m 11s] Epoch 53 [5120/8158] loss=0.0019426548911724239\n",
            "[0m 11s] Epoch 53 [7680/8158] loss=0.0019529677034976582\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7939903616862817\n",
            "[0m 12s] Epoch 54 [2560/8158] loss=0.0019454172463156283\n",
            "[0m 12s] Epoch 54 [5120/8158] loss=0.0019460512441582978\n",
            "[0m 12s] Epoch 54 [7680/8158] loss=0.0019565216537254554\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7940339482406508\n",
            "[0m 12s] Epoch 55 [2560/8158] loss=0.001956712000537664\n",
            "[0m 12s] Epoch 55 [5120/8158] loss=0.0019434023299254477\n",
            "[0m 12s] Epoch 55 [7680/8158] loss=0.001955011378352841\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7941179629034205\n",
            "[0m 12s] Epoch 56 [2560/8158] loss=0.002007948397658765\n",
            "[0m 12s] Epoch 56 [5120/8158] loss=0.001956680655712262\n",
            "[0m 12s] Epoch 56 [7680/8158] loss=0.0019568733832178017\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7941040657411578\n",
            "[0m 12s] Epoch 57 [2560/8158] loss=0.0019388429238460958\n",
            "[0m 12s] Epoch 57 [5120/8158] loss=0.0019558527623303235\n",
            "[0m 12s] Epoch 57 [7680/8158] loss=0.0019564580172300337\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7941407037143956\n",
            "[0m 13s] Epoch 58 [2560/8158] loss=0.001989660132676363\n",
            "[0m 13s] Epoch 58 [5120/8158] loss=0.0019494000938721002\n",
            "[0m 13s] Epoch 58 [7680/8158] loss=0.0019507127813994884\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7941880804039273\n",
            "[0m 13s] Epoch 59 [2560/8158] loss=0.001949801144655794\n",
            "[0m 13s] Epoch 59 [5120/8158] loss=0.0019345341192092747\n",
            "[0m 13s] Epoch 59 [7680/8158] loss=0.0019454520001697044\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7942525127016904\n",
            "[0m 13s] Epoch 60 [2560/8158] loss=0.0019852341851219536\n",
            "[0m 13s] Epoch 60 [5120/8158] loss=0.0019767667225096376\n",
            "[0m 13s] Epoch 60 [7680/8158] loss=0.0019480047941518328\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7942878872965407\n",
            "[0m 13s] Epoch 61 [2560/8158] loss=0.0019977417076006533\n",
            "[0m 14s] Epoch 61 [5120/8158] loss=0.0019633773306850346\n",
            "[0m 14s] Epoch 61 [7680/8158] loss=0.001956862618681043\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7943681118241477\n",
            "[0m 14s] Epoch 62 [2560/8158] loss=0.0019387084059417248\n",
            "[0m 14s] Epoch 62 [5120/8158] loss=0.0019734370755031703\n",
            "[0m 14s] Epoch 62 [7680/8158] loss=0.0019638752991644045\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7944331758111047\n",
            "[0m 14s] Epoch 63 [2560/8158] loss=0.0019682431709952654\n",
            "[0m 14s] Epoch 63 [5120/8158] loss=0.0019410746113862841\n",
            "[0m 14s] Epoch 63 [7680/8158] loss=0.00195316713458548\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7943965378378668\n",
            "[0m 14s] Epoch 64 [2560/8158] loss=0.0019454365829005837\n",
            "[0m 14s] Epoch 64 [5120/8158] loss=0.001955526531673968\n",
            "[0m 14s] Epoch 64 [7680/8158] loss=0.001957732792167614\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7944887644601553\n",
            "[0m 15s] Epoch 65 [2560/8158] loss=0.0019415756221860647\n",
            "[0m 15s] Epoch 65 [5120/8158] loss=0.0019638237135950476\n",
            "[0m 15s] Epoch 65 [7680/8158] loss=0.0019601818717395266\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7944754989870865\n",
            "[0m 15s] Epoch 66 [2560/8158] loss=0.0019609350943937897\n",
            "[0m 15s] Epoch 66 [5120/8158] loss=0.0019441907934378833\n",
            "[0m 15s] Epoch 66 [7680/8158] loss=0.001954360178206116\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7945254024333931\n",
            "[0m 15s] Epoch 67 [2560/8158] loss=0.001996293349657208\n",
            "[0m 15s] Epoch 67 [5120/8158] loss=0.001954779447987676\n",
            "[0m 15s] Epoch 67 [7680/8158] loss=0.0019553444969157376\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7945304559469432\n",
            "[0m 15s] Epoch 68 [2560/8158] loss=0.0019452522043138742\n",
            "[0m 15s] Epoch 68 [5120/8158] loss=0.0019347957917489112\n",
            "[0m 15s] Epoch 68 [7680/8158] loss=0.001951880248573919\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7946530036505319\n",
            "[0m 16s] Epoch 69 [2560/8158] loss=0.0019503064919263124\n",
            "[0m 16s] Epoch 69 [5120/8158] loss=0.0019485691271256655\n",
            "[0m 16s] Epoch 69 [7680/8158] loss=0.0019489746696005265\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7946454233802067\n",
            "[0m 16s] Epoch 70 [2560/8158] loss=0.0019251918885856867\n",
            "[0m 16s] Epoch 70 [5120/8158] loss=0.0019348122004885226\n",
            "[0m 16s] Epoch 70 [7680/8158] loss=0.001942808743721495\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7945989942244658\n",
            "[0m 16s] Epoch 71 [2560/8158] loss=0.00196828170446679\n",
            "[0m 16s] Epoch 71 [5120/8158] loss=0.0019378602504730225\n",
            "[0m 16s] Epoch 71 [7680/8158] loss=0.0019527329325986404\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7947237528402326\n",
            "[0m 16s] Epoch 72 [2560/8158] loss=0.0019390893052332104\n",
            "[0m 16s] Epoch 72 [5120/8158] loss=0.0019536875071935354\n",
            "[0m 17s] Epoch 72 [7680/8158] loss=0.0019491964407886068\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7947594432796797\n",
            "[0m 17s] Epoch 73 [2560/8158] loss=0.001981773599982262\n",
            "[0m 17s] Epoch 73 [5120/8158] loss=0.0019659538753330707\n",
            "[0m 17s] Epoch 73 [7680/8158] loss=0.001947396850058188\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.794781868246058\n",
            "[0m 17s] Epoch 74 [2560/8158] loss=0.0019662476261146367\n",
            "[0m 17s] Epoch 74 [5120/8158] loss=0.0019219763053115457\n",
            "[0m 17s] Epoch 74 [7680/8158] loss=0.001942407824875166\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7947704978405703\n",
            "[0m 17s] Epoch 75 [2560/8158] loss=0.001973335980437696\n",
            "[0m 17s] Epoch 75 [5120/8158] loss=0.0019668163207825274\n",
            "[0m 17s] Epoch 75 [7680/8158] loss=0.0019531462574377658\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7947970287867083\n",
            "[0m 18s] Epoch 76 [2560/8158] loss=0.001913899916689843\n",
            "[0m 18s] Epoch 76 [5120/8158] loss=0.0019477971422020347\n",
            "[0m 18s] Epoch 76 [7680/8158] loss=0.001950297070046266\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2009/2658 75.58%\n",
            "Dev set: AUC  0.7948140843949397\n",
            "[0m 18s] Epoch 77 [2560/8158] loss=0.001994853815995157\n",
            "[0m 18s] Epoch 77 [5120/8158] loss=0.0019643242994789034\n",
            "[0m 18s] Epoch 77 [7680/8158] loss=0.0019452643464319408\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7948898870981904\n",
            "[0m 18s] Epoch 78 [2560/8158] loss=0.0019623813219368457\n",
            "[0m 18s] Epoch 78 [5120/8158] loss=0.0019444858015049249\n",
            "[0m 18s] Epoch 78 [7680/8158] loss=0.0019513697790292403\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7949915890583852\n",
            "[0m 19s] Epoch 79 [2560/8158] loss=0.0020374679006636143\n",
            "[0m 19s] Epoch 79 [5120/8158] loss=0.001958668720908463\n",
            "[0m 19s] Epoch 79 [7680/8158] loss=0.0019482635039215286\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.79504370341687\n",
            "[0m 19s] Epoch 80 [2560/8158] loss=0.0019450607942417264\n",
            "[0m 19s] Epoch 80 [5120/8158] loss=0.0019448076607659459\n",
            "[0m 19s] Epoch 80 [7680/8158] loss=0.0019512142015931508\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7950490727750169\n",
            "[0m 19s] Epoch 81 [2560/8158] loss=0.00192401478998363\n",
            "[0m 19s] Epoch 81 [5120/8158] loss=0.0019442929769866168\n",
            "[0m 19s] Epoch 81 [7680/8158] loss=0.0019546762690879405\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dev set: AUC  0.7950168566261354\n",
            "[0m 19s] Epoch 82 [2560/8158] loss=0.0019196940120309591\n",
            "[0m 19s] Epoch 82 [5120/8158] loss=0.001937749411445111\n",
            "[0m 19s] Epoch 82 [7680/8158] loss=0.001944377226755023\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7950692868292172\n",
            "[0m 20s] Epoch 83 [2560/8158] loss=0.001897272130008787\n",
            "[0m 20s] Epoch 83 [5120/8158] loss=0.0019041570834815503\n",
            "[0m 20s] Epoch 83 [7680/8158] loss=0.001932294627962013\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7951823591948994\n",
            "[0m 20s] Epoch 84 [2560/8158] loss=0.00195671875262633\n",
            "[0m 20s] Epoch 84 [5120/8158] loss=0.0019471870502457023\n",
            "[0m 20s] Epoch 84 [7680/8158] loss=0.0019592931338896355\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7952013098707121\n",
            "[0m 20s] Epoch 85 [2560/8158] loss=0.0019592629396356642\n",
            "[0m 20s] Epoch 85 [5120/8158] loss=0.001960945938481018\n",
            "[0m 20s] Epoch 85 [7680/8158] loss=0.00194156418244044\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7951981514247434\n",
            "[0m 20s] Epoch 86 [2560/8158] loss=0.0019333622884005308\n",
            "[0m 20s] Epoch 86 [5120/8158] loss=0.0019599345803726466\n",
            "[0m 20s] Epoch 86 [7680/8158] loss=0.0019498859687397878\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7952872196010631\n",
            "[0m 21s] Epoch 87 [2560/8158] loss=0.0019996056100353597\n",
            "[0m 21s] Epoch 87 [5120/8158] loss=0.001963916781824082\n",
            "[0m 21s] Epoch 87 [7680/8158] loss=0.0019503624566520254\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7952954315605818\n",
            "[0m 21s] Epoch 88 [2560/8158] loss=0.001928724138997495\n",
            "[0m 21s] Epoch 88 [5120/8158] loss=0.0019464578595943748\n",
            "[0m 21s] Epoch 88 [7680/8158] loss=0.0019499967728431025\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7952291041952375\n",
            "[0m 21s] Epoch 89 [2560/8158] loss=0.0019761268980801106\n",
            "[0m 21s] Epoch 89 [5120/8158] loss=0.0019627242756541817\n",
            "[0m 21s] Epoch 89 [7680/8158] loss=0.001954402638754497\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2007/2658 75.51%\n",
            "Dev set: AUC  0.7953753402435919\n",
            "[0m 22s] Epoch 90 [2560/8158] loss=0.001918882131576538\n",
            "[0m 22s] Epoch 90 [5120/8158] loss=0.0019448783015832305\n",
            "[0m 22s] Epoch 90 [7680/8158] loss=0.001955692027695477\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7953522835880198\n",
            "[0m 22s] Epoch 91 [2560/8158] loss=0.001935173664242029\n",
            "[0m 22s] Epoch 91 [5120/8158] loss=0.0019363905012141913\n",
            "[0m 22s] Epoch 91 [7680/8158] loss=0.0019457407450924316\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7953453350068885\n",
            "[0m 22s] Epoch 92 [2560/8158] loss=0.0018946076394058763\n",
            "[0m 22s] Epoch 92 [5120/8158] loss=0.0019241884583607316\n",
            "[0m 22s] Epoch 92 [7680/8158] loss=0.0019454792879211407\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7953740768652044\n",
            "[0m 22s] Epoch 93 [2560/8158] loss=0.0019545116811059415\n",
            "[0m 22s] Epoch 93 [5120/8158] loss=0.0019336710625793784\n",
            "[0m 23s] Epoch 93 [7680/8158] loss=0.001946429714250068\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7954899918322587\n",
            "[0m 23s] Epoch 94 [2560/8158] loss=0.001941818161867559\n",
            "[0m 23s] Epoch 94 [5120/8158] loss=0.0019248628290370108\n",
            "[0m 23s] Epoch 94 [7680/8158] loss=0.0019411015518320104\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7954211377101393\n",
            "[0m 23s] Epoch 95 [2560/8158] loss=0.00196455845143646\n",
            "[0m 23s] Epoch 95 [5120/8158] loss=0.0019565458584111183\n",
            "[0m 23s] Epoch 95 [7680/8158] loss=0.0019530447898432612\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7954034504127141\n",
            "[0m 23s] Epoch 96 [2560/8158] loss=0.001919386617373675\n",
            "[0m 23s] Epoch 96 [5120/8158] loss=0.0019660193473100664\n",
            "[0m 23s] Epoch 96 [7680/8158] loss=0.0019455972864913444\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7955114692648464\n",
            "[0m 23s] Epoch 97 [2560/8158] loss=0.0019407752552069724\n",
            "[0m 23s] Epoch 97 [5120/8158] loss=0.0019476925663184375\n",
            "[0m 24s] Epoch 97 [7680/8158] loss=0.001945864292792976\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7954318764264332\n",
            "[0m 24s] Epoch 98 [2560/8158] loss=0.001952127180993557\n",
            "[0m 24s] Epoch 98 [5120/8158] loss=0.001933645294047892\n",
            "[0m 24s] Epoch 98 [7680/8158] loss=0.0019483226661880812\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7954653559537023\n",
            "[0m 24s] Epoch 99 [2560/8158] loss=0.0019769919686950742\n",
            "[0m 24s] Epoch 99 [5120/8158] loss=0.00193499096785672\n",
            "[0m 24s] Epoch 99 [7680/8158] loss=0.0019486248570804794\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7955575825759906\n",
            "[0m 24s] Epoch 100 [2560/8158] loss=0.0019262571004219353\n",
            "[0m 24s] Epoch 100 [5120/8158] loss=0.0019291211210656911\n",
            "[0m 24s] Epoch 100 [7680/8158] loss=0.0019447167093555133\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7955897987248721\n",
            "[0m 24s] Epoch 101 [2560/8158] loss=0.0019354552729055285\n",
            "[0m 24s] Epoch 101 [5120/8158] loss=0.001944616821128875\n",
            "[0m 25s] Epoch 101 [7680/8158] loss=0.0019396999229987463\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.795572111427447\n",
            "[0m 25s] Epoch 102 [2560/8158] loss=0.0019186040270142257\n",
            "[0m 25s] Epoch 102 [5120/8158] loss=0.001934785913908854\n",
            "[0m 25s] Epoch 102 [7680/8158] loss=0.0019521776858406762\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.795603064197941\n",
            "[0m 25s] Epoch 103 [2560/8158] loss=0.001908212318085134\n",
            "[0m 25s] Epoch 103 [5120/8158] loss=0.0019188821723219007\n",
            "[0m 25s] Epoch 103 [7680/8158] loss=0.0019376853170494238\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7956194881169787\n",
            "[0m 25s] Epoch 104 [2560/8158] loss=0.00196582549251616\n",
            "[0m 25s] Epoch 104 [5120/8158] loss=0.0019514779618475587\n",
            "[0m 25s] Epoch 104 [7680/8158] loss=0.0019519455265253783\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2005/2658 75.43%\n",
            "Dev set: AUC  0.7956428606171477\n",
            "[0m 25s] Epoch 105 [2560/8158] loss=0.0019461855641566216\n",
            "[0m 26s] Epoch 105 [5120/8158] loss=0.0019562708155717702\n",
            "[0m 26s] Epoch 105 [7680/8158] loss=0.0019481024821288884\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.795659916225379\n",
            "[0m 26s] Epoch 106 [2560/8158] loss=0.0019531219499185682\n",
            "[0m 26s] Epoch 106 [5120/8158] loss=0.001951432484202087\n",
            "[0m 26s] Epoch 106 [7680/8158] loss=0.001951596257276833\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7956725500092543\n",
            "[0m 26s] Epoch 107 [2560/8158] loss=0.001962973061017692\n",
            "[0m 26s] Epoch 107 [5120/8158] loss=0.001972850534366444\n",
            "[0m 26s] Epoch 107 [7680/8158] loss=0.0019462877806896964\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7957262435907235\n",
            "[0m 26s] Epoch 108 [2560/8158] loss=0.00194541837554425\n",
            "[0m 26s] Epoch 108 [5120/8158] loss=0.0019173876440618186\n",
            "[0m 26s] Epoch 108 [7680/8158] loss=0.0019378518569283187\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7956194881169788\n",
            "[0m 27s] Epoch 109 [2560/8158] loss=0.0019355906872078777\n",
            "[0m 27s] Epoch 109 [5120/8158] loss=0.0019337483216077089\n",
            "[0m 27s] Epoch 109 [7680/8158] loss=0.001940418166729311\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7956687598740917\n",
            "[0m 27s] Epoch 110 [2560/8158] loss=0.0018685812479816376\n",
            "[0m 27s] Epoch 110 [5120/8158] loss=0.0019673846371006222\n",
            "[0m 27s] Epoch 110 [7680/8158] loss=0.0019433526322245598\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7956769718336105\n",
            "[0m 27s] Epoch 111 [2560/8158] loss=0.0019772412488237023\n",
            "[0m 27s] Epoch 111 [5120/8158] loss=0.001974014937877655\n",
            "[0m 27s] Epoch 111 [7680/8158] loss=0.0019513115791293483\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7956946591310357\n",
            "[0m 27s] Epoch 112 [2560/8158] loss=0.00189430289901793\n",
            "[0m 27s] Epoch 112 [5120/8158] loss=0.0019121771678328515\n",
            "[0m 27s] Epoch 112 [7680/8158] loss=0.0019498099107295274\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.79575087946928\n",
            "[0m 28s] Epoch 113 [2560/8158] loss=0.0019073776784352958\n",
            "[0m 28s] Epoch 113 [5120/8158] loss=0.0019251913065090776\n",
            "[0m 28s] Epoch 113 [7680/8158] loss=0.0019499638234265148\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.795740140752986\n",
            "[0m 28s] Epoch 114 [2560/8158] loss=0.0019867650000378488\n",
            "[0m 28s] Epoch 114 [5120/8158] loss=0.0019307472801301628\n",
            "[0m 28s] Epoch 114 [7680/8158] loss=0.0019492228631861508\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7957988878480053\n",
            "[0m 28s] Epoch 115 [2560/8158] loss=0.0019473803229629993\n",
            "[0m 28s] Epoch 115 [5120/8158] loss=0.001930113259004429\n",
            "[0m 28s] Epoch 115 [7680/8158] loss=0.0019397828999596336\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7957900441992929\n",
            "[0m 28s] Epoch 116 [2560/8158] loss=0.0019430899294093252\n",
            "[0m 28s] Epoch 116 [5120/8158] loss=0.00195587943890132\n",
            "[0m 29s] Epoch 116 [7680/8158] loss=0.0019528978892291585\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7957047661581357\n",
            "[0m 29s] Epoch 117 [2560/8158] loss=0.0019555748905986546\n",
            "[0m 29s] Epoch 117 [5120/8158] loss=0.0019358753983397038\n",
            "[0m 29s] Epoch 117 [7680/8158] loss=0.0019507638333986202\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7957584597396052\n",
            "[0m 29s] Epoch 118 [2560/8158] loss=0.001907982339616865\n",
            "[0m 29s] Epoch 118 [5120/8158] loss=0.0019020205247215927\n",
            "[0m 29s] Epoch 118 [7680/8158] loss=0.0019386268958138922\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7957837273073554\n",
            "[0m 29s] Epoch 119 [2560/8158] loss=0.0019371126079931855\n",
            "[0m 29s] Epoch 119 [5120/8158] loss=0.0019630754890386016\n",
            "[0m 29s] Epoch 119 [7680/8158] loss=0.0019459592566515008\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7957875174425177\n",
            "[0m 30s] Epoch 120 [2560/8158] loss=0.0019967809319496157\n",
            "[0m 30s] Epoch 120 [5120/8158] loss=0.0019527221738826483\n",
            "[0m 30s] Epoch 120 [7680/8158] loss=0.0019397345914815864\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7958140483886557\n",
            "[0m 30s] Epoch 121 [2560/8158] loss=0.001968186558224261\n",
            "[0m 30s] Epoch 121 [5120/8158] loss=0.0019417222763877362\n",
            "[0m 30s] Epoch 121 [7680/8158] loss=0.0019464141068359217\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.795846264537537\n",
            "[0m 30s] Epoch 122 [2560/8158] loss=0.0019291490432806313\n",
            "[0m 30s] Epoch 122 [5120/8158] loss=0.0019485113152768463\n",
            "[0m 30s] Epoch 122 [7680/8158] loss=0.0019399549812078476\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7957420358205676\n",
            "[0m 30s] Epoch 123 [2560/8158] loss=0.001990482083056122\n",
            "[0m 31s] Epoch 123 [5120/8158] loss=0.0019532977894414216\n",
            "[0m 31s] Epoch 123 [7680/8158] loss=0.0019367358142820497\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2008/2658 75.55%\n",
            "Dev set: AUC  0.795787517442518\n",
            "[0m 31s] Epoch 124 [2560/8158] loss=0.0018739113933406771\n",
            "[0m 31s] Epoch 124 [5120/8158] loss=0.001939040416618809\n",
            "[0m 31s] Epoch 124 [7680/8158] loss=0.0019427253981120884\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.795796992780424\n",
            "[0m 31s] Epoch 125 [2560/8158] loss=0.0019483653479255736\n",
            "[0m 31s] Epoch 125 [5120/8158] loss=0.0019402390404138713\n",
            "[0m 31s] Epoch 125 [7680/8158] loss=0.0019451502865801254\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7959612319708008\n",
            "[0m 31s] Epoch 126 [2560/8158] loss=0.0019167483085766434\n",
            "[0m 31s] Epoch 126 [5120/8158] loss=0.001932411064626649\n",
            "[0m 31s] Epoch 126 [7680/8158] loss=0.001938993774820119\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7957647766315425\n",
            "[0m 31s] Epoch 127 [2560/8158] loss=0.0019158386974595487\n",
            "[0m 32s] Epoch 127 [5120/8158] loss=0.0019455520901829003\n",
            "[0m 32s] Epoch 127 [7680/8158] loss=0.001934759714640677\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2006/2658 75.47%\n",
            "Dev set: AUC  0.7958765856188375\n",
            "[0m 32s] Epoch 128 [2560/8158] loss=0.001928520598448813\n",
            "[0m 32s] Epoch 128 [5120/8158] loss=0.0019340488710440696\n",
            "[0m 32s] Epoch 128 [7680/8158] loss=0.0019449580926448108\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7958228920373681\n",
            "[0m 32s] Epoch 129 [2560/8158] loss=0.0019085663952864707\n",
            "[0m 32s] Epoch 129 [5120/8158] loss=0.001958053797716275\n",
            "[0m 32s] Epoch 129 [7680/8158] loss=0.001951734550918142\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7958386842672122\n",
            "[0m 32s] Epoch 130 [2560/8158] loss=0.0019747048616409303\n",
            "[0m 32s] Epoch 130 [5120/8158] loss=0.0019439342431724072\n",
            "[0m 32s] Epoch 130 [7680/8158] loss=0.0019530369900166988\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7958146800778494\n",
            "[0m 32s] Epoch 131 [2560/8158] loss=0.0019233364844694733\n",
            "[0m 33s] Epoch 131 [5120/8158] loss=0.0019250044540967792\n",
            "[0m 33s] Epoch 131 [7680/8158] loss=0.0019480078558747967\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7959555467680571\n",
            "[0m 33s] Epoch 132 [2560/8158] loss=0.001902089186478406\n",
            "[0m 33s] Epoch 132 [5120/8158] loss=0.0019526710093487054\n",
            "[0m 33s] Epoch 132 [7680/8158] loss=0.0019484458141960203\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7958544764970561\n",
            "[0m 33s] Epoch 133 [2560/8158] loss=0.0019222636590711772\n",
            "[0m 33s] Epoch 133 [5120/8158] loss=0.0019451623840723187\n",
            "[0m 33s] Epoch 133 [7680/8158] loss=0.0019427587666238347\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.79575087946928\n",
            "[0m 33s] Epoch 134 [2560/8158] loss=0.0019078045967034995\n",
            "[0m 33s] Epoch 134 [5120/8158] loss=0.0019075047108344733\n",
            "[0m 33s] Epoch 134 [7680/8158] loss=0.0019394657535788914\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7958993264298126\n",
            "[0m 34s] Epoch 135 [2560/8158] loss=0.001996246003545821\n",
            "[0m 34s] Epoch 135 [5120/8158] loss=0.0019499031419400125\n",
            "[0m 34s] Epoch 135 [7680/8158] loss=0.0019403829045283297\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.795997238254845\n",
            "[0m 34s] Epoch 136 [2560/8158] loss=0.0019663305720314384\n",
            "[0m 34s] Epoch 136 [5120/8158] loss=0.0019412546418607235\n",
            "[0m 34s] Epoch 136 [7680/8158] loss=0.001937292900402099\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7957742519694491\n",
            "[0m 34s] Epoch 137 [2560/8158] loss=0.001955423108302057\n",
            "[0m 34s] Epoch 137 [5120/8158] loss=0.00194767284556292\n",
            "[0m 34s] Epoch 137 [7680/8158] loss=0.0019466736276323596\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7958677419701249\n",
            "[0m 34s] Epoch 138 [2560/8158] loss=0.001990052592009306\n",
            "[0m 35s] Epoch 138 [5120/8158] loss=0.001941348990658298\n",
            "[0m 35s] Epoch 138 [7680/8158] loss=0.0019431362394243478\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7959429129841818\n",
            "[0m 35s] Epoch 139 [2560/8158] loss=0.0019612017669714986\n",
            "[0m 35s] Epoch 139 [5120/8158] loss=0.001953958999365568\n",
            "[0m 35s] Epoch 139 [7680/8158] loss=0.001949722416854153\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7960111354171076\n",
            "[0m 35s] Epoch 140 [2560/8158] loss=0.0019228114280849696\n",
            "[0m 35s] Epoch 140 [5120/8158] loss=0.0019229679717682302\n",
            "[0m 35s] Epoch 140 [7680/8158] loss=0.0019504602959689995\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.795991553052101\n",
            "[0m 35s] Epoch 141 [2560/8158] loss=0.0019215593114495278\n",
            "[0m 35s] Epoch 141 [5120/8158] loss=0.0019154949579387904\n",
            "[0m 35s] Epoch 141 [7680/8158] loss=0.0019459712784737349\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7959031165649753\n",
            "[0m 36s] Epoch 142 [2560/8158] loss=0.0019889911520294843\n",
            "[0m 36s] Epoch 142 [5120/8158] loss=0.0019620262319222093\n",
            "[0m 36s] Epoch 142 [7680/8158] loss=0.0019442677303838233\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7959511249437007\n",
            "[0m 36s] Epoch 143 [2560/8158] loss=0.0019257773761637508\n",
            "[0m 36s] Epoch 143 [5120/8158] loss=0.0019124574784655124\n",
            "[0m 36s] Epoch 143 [7680/8158] loss=0.0019272858432183664\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7958980630514252\n",
            "[0m 36s] Epoch 144 [2560/8158] loss=0.0019414578913711012\n",
            "[0m 36s] Epoch 144 [5120/8158] loss=0.0019438155519310384\n",
            "[0m 36s] Epoch 144 [7680/8158] loss=0.0019346939477448662\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7957995195371991\n",
            "[0m 36s] Epoch 145 [2560/8158] loss=0.0019444608595222234\n",
            "[0m 36s] Epoch 145 [5120/8158] loss=0.0019495336746331304\n",
            "[0m 36s] Epoch 145 [7680/8158] loss=0.0019419222565678258\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7957515111584736\n",
            "[0m 37s] Epoch 146 [2560/8158] loss=0.001917074283119291\n",
            "[0m 37s] Epoch 146 [5120/8158] loss=0.0019031692820135504\n",
            "[0m 37s] Epoch 146 [7680/8158] loss=0.0019309941291188201\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7958405793347935\n",
            "[0m 37s] Epoch 147 [2560/8158] loss=0.001856277952902019\n",
            "[0m 37s] Epoch 147 [5120/8158] loss=0.0019160034717060626\n",
            "[0m 37s] Epoch 147 [7680/8158] loss=0.0019410127735075852\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.795846896226731\n",
            "[0m 37s] Epoch 148 [2560/8158] loss=0.0019423542660661041\n",
            "[0m 37s] Epoch 148 [5120/8158] loss=0.0019414069189224391\n",
            "[0m 37s] Epoch 148 [7680/8158] loss=0.0019403006493424375\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7958955362946502\n",
            "[0m 37s] Epoch 149 [2560/8158] loss=0.0019402783596888184\n",
            "[0m 37s] Epoch 149 [5120/8158] loss=0.0019740931049454956\n",
            "[0m 37s] Epoch 149 [7680/8158] loss=0.0019507328125958642\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7959328059570818\n",
            "[0m 38s] Epoch 150 [2560/8158] loss=0.0019093850045464934\n",
            "[0m 38s] Epoch 150 [5120/8158] loss=0.0019741323369089512\n",
            "[0m 38s] Epoch 150 [7680/8158] loss=0.0019423984650832912\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7959789192682261\n",
            "[0m 38s] Epoch 151 [2560/8158] loss=0.0019187439815141261\n",
            "[0m 38s] Epoch 151 [5120/8158] loss=0.0019233508151955902\n",
            "[0m 38s] Epoch 151 [7680/8158] loss=0.0019454488996416331\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.795887956024325\n",
            "[0m 38s] Epoch 152 [2560/8158] loss=0.0019169725012034178\n",
            "[0m 38s] Epoch 152 [5120/8158] loss=0.001935406809207052\n",
            "[0m 38s] Epoch 152 [7680/8158] loss=0.0019404693079801897\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7959606002816071\n",
            "[0m 38s] Epoch 153 [2560/8158] loss=0.0019473466672934592\n",
            "[0m 38s] Epoch 153 [5120/8158] loss=0.0019506681652273982\n",
            "[0m 39s] Epoch 153 [7680/8158] loss=0.0019416436863442263\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7959593369032195\n",
            "[0m 39s] Epoch 154 [2560/8158] loss=0.0019728086306713523\n",
            "[0m 39s] Epoch 154 [5120/8158] loss=0.0019435142807196827\n",
            "[0m 39s] Epoch 154 [7680/8158] loss=0.0019414735220683117\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7958715321052873\n",
            "[0m 39s] Epoch 155 [2560/8158] loss=0.0019763002172112467\n",
            "[0m 39s] Epoch 155 [5120/8158] loss=0.0019532492209691555\n",
            "[0m 39s] Epoch 155 [7680/8158] loss=0.00193291639831538\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7958967996730377\n",
            "[0m 39s] Epoch 156 [2560/8158] loss=0.0019652571994811297\n",
            "[0m 39s] Epoch 156 [5120/8158] loss=0.0019491470709908753\n",
            "[0m 39s] Epoch 156 [7680/8158] loss=0.0019411510322242976\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7958342624428557\n",
            "[0m 39s] Epoch 157 [2560/8158] loss=0.0019206550787203014\n",
            "[0m 39s] Epoch 157 [5120/8158] loss=0.0019254405284300446\n",
            "[0m 40s] Epoch 157 [7680/8158] loss=0.0019405108333254853\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.795786885753324\n",
            "[0m 40s] Epoch 158 [2560/8158] loss=0.00196242390666157\n",
            "[0m 40s] Epoch 158 [5120/8158] loss=0.0019322638981975615\n",
            "[0m 40s] Epoch 158 [7680/8158] loss=0.0019365187967196108\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1989/2658 74.83%\n",
            "Dev set: AUC  0.7961223127152086\n",
            "[0m 40s] Epoch 159 [2560/8158] loss=0.0019383121631108224\n",
            "[0m 40s] Epoch 159 [5120/8158] loss=0.0019526563992258161\n",
            "[0m 40s] Epoch 159 [7680/8158] loss=0.001937660900875926\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7958873243351313\n",
            "[0m 40s] Epoch 160 [2560/8158] loss=0.0019639470148831604\n",
            "[0m 40s] Epoch 160 [5120/8158] loss=0.001923252019332722\n",
            "[0m 40s] Epoch 160 [7680/8158] loss=0.0019414455047808588\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7958570032538311\n",
            "[0m 40s] Epoch 161 [2560/8158] loss=0.0018844914971850812\n",
            "[0m 40s] Epoch 161 [5120/8158] loss=0.0019126374972984195\n",
            "[0m 41s] Epoch 161 [7680/8158] loss=0.00193359376086543\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7958765856188375\n",
            "[0m 41s] Epoch 162 [2560/8158] loss=0.0019556268001906575\n",
            "[0m 41s] Epoch 162 [5120/8158] loss=0.0019412222492974252\n",
            "[0m 41s] Epoch 162 [7680/8158] loss=0.0019421582343056797\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7958939570716658\n",
            "[0m 41s] Epoch 163 [2560/8158] loss=0.001932386017870158\n",
            "[0m 41s] Epoch 163 [5120/8158] loss=0.0019187987840268762\n",
            "[0m 41s] Epoch 163 [7680/8158] loss=0.0019348044918539623\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7960439832551829\n",
            "[0m 41s] Epoch 164 [2560/8158] loss=0.0019576113321818412\n",
            "[0m 41s] Epoch 164 [5120/8158] loss=0.0019317121594212949\n",
            "[0m 41s] Epoch 164 [7680/8158] loss=0.0019331155422454079\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7959631270383821\n",
            "[0m 42s] Epoch 165 [2560/8158] loss=0.0019244996597990393\n",
            "[0m 42s] Epoch 165 [5120/8158] loss=0.001920669386163354\n",
            "[0m 42s] Epoch 165 [7680/8158] loss=0.0019299224601127206\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7959846044709699\n",
            "[0m 42s] Epoch 166 [2560/8158] loss=0.0019461082061752676\n",
            "[0m 42s] Epoch 166 [5120/8158] loss=0.0019584579742513595\n",
            "[0m 42s] Epoch 166 [7680/8158] loss=0.0019512492037999133\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1988/2658 74.79%\n",
            "Dev set: AUC  0.7960262959577576\n",
            "[0m 42s] Epoch 167 [2560/8158] loss=0.0019293582416139543\n",
            "[0m 42s] Epoch 167 [5120/8158] loss=0.0019500072754453867\n",
            "[0m 42s] Epoch 167 [7680/8158] loss=0.0019472547496358554\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7957414041313737\n",
            "[0m 42s] Epoch 168 [2560/8158] loss=0.001941637322306633\n",
            "[0m 43s] Epoch 168 [5120/8158] loss=0.001917931518983096\n",
            "[0m 43s] Epoch 168 [7680/8158] loss=0.00193570515839383\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7958494229835058\n",
            "[0m 43s] Epoch 169 [2560/8158] loss=0.0019359036930836737\n",
            "[0m 43s] Epoch 169 [5120/8158] loss=0.001958142966032028\n",
            "[0m 43s] Epoch 169 [7680/8158] loss=0.0019436094638270636\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1990/2658 74.87%\n",
            "Dev set: AUC  0.7958576349430247\n",
            "[0m 43s] Epoch 170 [2560/8158] loss=0.0019318936858326197\n",
            "[0m 43s] Epoch 170 [5120/8158] loss=0.001949692500056699\n",
            "[0m 43s] Epoch 170 [7680/8158] loss=0.0019453504976506034\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1994/2658 75.02%\n",
            "Dev set: AUC  0.7958620567673811\n",
            "[0m 43s] Epoch 171 [2560/8158] loss=0.001947040669620037\n",
            "[0m 43s] Epoch 171 [5120/8158] loss=0.001923227804945782\n",
            "[0m 43s] Epoch 171 [7680/8158] loss=0.0019385742605663836\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7958292089293058\n",
            "[0m 44s] Epoch 172 [2560/8158] loss=0.0020000420860014855\n",
            "[0m 44s] Epoch 172 [5120/8158] loss=0.001958688802551478\n",
            "[0m 44s] Epoch 172 [7680/8158] loss=0.0019390968140214682\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1993/2658 74.98%\n",
            "Dev set: AUC  0.7958380525780184\n",
            "[0m 44s] Epoch 173 [2560/8158] loss=0.0018952745827846228\n",
            "[0m 44s] Epoch 173 [5120/8158] loss=0.0019361573387868703\n",
            "[0m 44s] Epoch 173 [7680/8158] loss=0.0019341898849233985\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7958380525780184\n",
            "[0m 44s] Epoch 174 [2560/8158] loss=0.0018971475074067712\n",
            "[0m 44s] Epoch 174 [5120/8158] loss=0.0019111199886538087\n",
            "[0m 44s] Epoch 174 [7680/8158] loss=0.0019332452211529016\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7959277524435318\n",
            "[0m 44s] Epoch 175 [2560/8158] loss=0.0019291850039735436\n",
            "[0m 45s] Epoch 175 [5120/8158] loss=0.0019270621880423278\n",
            "[0m 45s] Epoch 175 [7680/8158] loss=0.0019351067719981074\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1995/2658 75.06%\n",
            "Dev set: AUC  0.7957167682528172\n",
            "[0m 45s] Epoch 176 [2560/8158] loss=0.0019349059322848916\n",
            "[0m 45s] Epoch 176 [5120/8158] loss=0.0019253053062129766\n",
            "[0m 45s] Epoch 176 [7680/8158] loss=0.0019296559466359517\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1996/2658 75.09%\n",
            "Dev set: AUC  0.7959921847412949\n",
            "[0m 45s] Epoch 177 [2560/8158] loss=0.00193289783783257\n",
            "[0m 45s] Epoch 177 [5120/8158] loss=0.0019582779495976865\n",
            "[0m 45s] Epoch 177 [7680/8158] loss=0.0019321959271716574\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7958633201457687\n",
            "[0m 45s] Epoch 178 [2560/8158] loss=0.0019515947788022459\n",
            "[0m 45s] Epoch 178 [5120/8158] loss=0.0019384983635973186\n",
            "[0m 45s] Epoch 178 [7680/8158] loss=0.0019346174085512757\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7957710935234802\n",
            "[0m 46s] Epoch 179 [2560/8158] loss=0.0019764176453463734\n",
            "[0m 46s] Epoch 179 [5120/8158] loss=0.0019594739249441774\n",
            "[0m 46s] Epoch 179 [7680/8158] loss=0.0019413738162256777\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7957003443337796\n",
            "[0m 46s] Epoch 180 [2560/8158] loss=0.0019619480008259416\n",
            "[0m 46s] Epoch 180 [5120/8158] loss=0.0019339685037266462\n",
            "[0m 46s] Epoch 180 [7680/8158] loss=0.001945057053429385\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1992/2658 74.94%\n",
            "Dev set: AUC  0.7958443694699558\n",
            "[0m 46s] Epoch 181 [2560/8158] loss=0.0019474929431453347\n",
            "[0m 46s] Epoch 181 [5120/8158] loss=0.001948416739469394\n",
            "[0m 46s] Epoch 181 [7680/8158] loss=0.001948801214651515\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2000/2658 75.24%\n",
            "Dev set: AUC  0.7958273138617245\n",
            "[0m 46s] Epoch 182 [2560/8158] loss=0.0019712706562131643\n",
            "[0m 47s] Epoch 182 [5120/8158] loss=0.0019499197078403085\n",
            "[0m 47s] Epoch 182 [7680/8158] loss=0.0019326556745606165\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1999/2658 75.21%\n",
            "Dev set: AUC  0.7958544764970561\n",
            "[0m 47s] Epoch 183 [2560/8158] loss=0.0019362197373993695\n",
            "[0m 47s] Epoch 183 [5120/8158] loss=0.001941275189165026\n",
            "[0m 47s] Epoch 183 [7680/8158] loss=0.0019358667739046117\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1997/2658 75.13%\n",
            "Dev set: AUC  0.7957294020366924\n",
            "[0m 47s] Epoch 184 [2560/8158] loss=0.0019284479902125895\n",
            "[0m 47s] Epoch 184 [5120/8158] loss=0.001926566386828199\n",
            "[0m 47s] Epoch 184 [7680/8158] loss=0.0019350297555016974\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7958222603481744\n",
            "[0m 47s] Epoch 185 [2560/8158] loss=0.0019528323085978628\n",
            "[0m 47s] Epoch 185 [5120/8158] loss=0.0019348107045516372\n",
            "[0m 47s] Epoch 185 [7680/8158] loss=0.0019380609388463198\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2002/2658 75.32%\n",
            "Dev set: AUC  0.7959100651461066\n",
            "[0m 47s] Epoch 186 [2560/8158] loss=0.0019390247529372574\n",
            "[0m 48s] Epoch 186 [5120/8158] loss=0.0019188211183063686\n",
            "[0m 48s] Epoch 186 [7680/8158] loss=0.001929380022920668\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2003/2658 75.36%\n",
            "Dev set: AUC  0.7957331921718548\n",
            "[0m 48s] Epoch 187 [2560/8158] loss=0.001948908867780119\n",
            "[0m 48s] Epoch 187 [5120/8158] loss=0.0019398228323552758\n",
            "[0m 48s] Epoch 187 [7680/8158] loss=0.0019434744763808945\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 1998/2658 75.17%\n",
            "Dev set: AUC  0.7958475279159247\n",
            "[0m 48s] Epoch 188 [2560/8158] loss=0.0019070315756835043\n",
            "[0m 48s] Epoch 188 [5120/8158] loss=0.0019402433361392468\n",
            "[0m 48s] Epoch 188 [7680/8158] loss=0.0019366105164711673\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2004/2658 75.40%\n",
            "Dev set: AUC  0.7957161365636234\n",
            "[0m 48s] Epoch 189 [2560/8158] loss=0.0019276817562058567\n",
            "[0m 48s] Epoch 189 [5120/8158] loss=0.0019463315838947892\n",
            "[0m 48s] Epoch 189 [7680/8158] loss=0.0019393660554972787\n",
            "evaluating trained model ...\n",
            "Dev set: Accuracy 2001/2658 75.28%\n",
            "Dev set: AUC  0.7957976244696181\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhMITT7jmmlg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}